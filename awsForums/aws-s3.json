[
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247898&tstart=25", "Title": "I can't delete a bucket as it has no region", "crawled": true, "dateScraped": "2017-04-05-21-58", "Question": "Hello, There is a bucket that was running ok, that now has no region assigned. I need to delete it, but it is impossible. It was created in EU Germany region. Thanks in advance for your answer.", "Answers": {"usr-1": ["It has disappeared. Thanks."], "usr-2": ["Thank you for your comment. We are interested in finding more details about your case. When you open your case please add as much information as possible which relates to this issue. We would like to collect this information we can improve your experience. Please include description, bucket, calls and related and details how this occurred and we will be happy to investigate further. You can provide us more information by opening a case https://aws.amazon.com/contact-us/ and titling it, \"bucket region missing 1/27/2017\" so we may reference it to this post."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242239&tstart=25", "Title": "S3 access logs taking over my storage", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi all, I am looking for a way to prevent certain users being added to access logs. so far i have accessed s3 maybe 4 times myself and instead of getting 4 files with the information contained within it i have found that the internal amazon account (presumably the one that delivers logs and handles other things) has made around 1200 files of varying sizes (on average one every 150 seconds) that just contain utterly useless logs, i am interested when someone else accesses the bucket not when amazon does something - generally i don't believe i should have to pay for the software itself to log itself accessing itself. How do i go about removing certain accounts from being logged within the access logs? if i can't i imagine i will be sacking access logs off for that bucket!", "Answers": {"usr-1": ["I'm sorry to hear that you are getting access log records for object accesses you're not interested in logging. S3 Access Logging is either enabled or disabled for a given bucket, and when enabled, it records all accesses. You may use a prefix within a bucket or a separate bucket to distinguish your access logs from other objects. Then, using S3 Lifecycle policies, you can expire (delete) access logs and/or transition them to the lower cost Standard - Infrequent Access storage class as discussed here: http://docs.aws.amazon.com/AmazonS3/latest/dev/deleting-log-files-lifecycle.html ."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241824&tstart=25", "Title": "Multi-region routing in S3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello there! I'm building a mobile app (iOS / Android) which connects to an S3 bucket in order to download files. I would also like to replicate the bucket to another region, but I was wondering: how does the code in my app know which region to access? If two buckets with the same content were accessible, would the AWS sdk code connect to the fastest region? Should I be the one who specifies which region to use inside the app? If so, how? Please advice! Thanks in advance.", "Answers": {"usr-8": ["Appreciate that link - will mos def catch up on recent API Gateway changes. We abandoned API Gateway over the issue of custom SSL's via Certificate Manager not being able to work with API Gateway as they do with Cloudfront. Also, the complexity issue is (was?) steep."], "usr-9": ["If you're using CloudFront and have your custom cert there, just set up your API Gateway endpoint as an additional origin server in your CloudFront distribution, then create a cache behavior with a path pattern like /api* and route it to the new origin -- one stone, two birds -- you have your custom domain and SSL with API Gateway and you have eliminated some potential cross-origin issues. However... counting objects in a bucket isn't something you will want to be doing in a high performance/high traffic path in your application. It will get expensive and will not scale well. Storing a count somewhere else, whether RDS, DynamoDB, SimpleDB, or even as a JSON object in S3 is likely to be a better idea. S3 event notifications could be used to trigger code that maintains such a count."], "usr-1": ["colto wrote: If two buckets with the same content were accessible, would the AWS sdk code connect to the fastest region? Bucket names are globally unique meaning that these would have different names. The SDK uses the bucket name you specify. Should I be the one who specifies which region to use inside the app? If so, how? I can probably come up with a way or two to figure out the best region to use. But first, have you considered CloudFront? It's a content delivery network that will cache the content closer to the users. You would basically set up a single S3 bucket and point CloudFront there. When your app requests some content, the traffic will automatically go through the best CloudFront edge location. These are spread all over the world. You get an optimal connection to S3 and your content is cached at edge locations when it is being accessed, meaning that you often don't need to go the whole way back to S3."], "usr-2": ["I did not know about CloudFront, but it looks like it could work in my scenario. I'll try to use it and report back if I have issues. Edit: do I need more code to instruct the app to work with CloudFront or does AWS automatically understand that there's a CloudFront interface in between? Thanks for your help! Edited by: colto on Oct 26, 2016 12:39 AM Edited by: colto on Oct 26, 2016 1:15 AM"], "usr-3": ["colto wrote: do I need more code to instruct the app to work with CloudFront or does AWS automatically understand that there's a CloudFront interface in between? When you set up CloudFront, you get an address looking something like \"d123456.cloudfront.net\" (you can optionally connect your own custom domain). Let's say that you have a file in your S3 bucket called \"example.png\" and that CloudFront has been configured to get files from that bucket. Your app would then call http://d123456.cloudfront.net/example.png. The request is routed to the nearest CloudFront server location automatically and CloudFront then fetches example.png from S3 or the local cache at that location. The AWS SDK is not involved."], "usr-4": ["+1 for Cloudfront. Its the bomb. Also the reporting Cloudfront produces is super helpful."], "usr-5": ["Thank you so much guys, it worked! AWS is truly amazing. Just one last question: for app internal logic reasons, I need to know the number of items in the bucket when I open the app. Right now I use the AWS SDK which has a specific function to do so, but I would like to get rid of it if possibile and replace it with CloudFront links (thus, allowing me to remove the AWS SDK altogether). How would you proceed? I was thinking of a 'lambda function' made available using 'API gateway' that reads the number of items and reports back. Is that possible? What's your advice? I'm pretty new to AWS! Thanks in advance!"], "usr-6": ["The question was answered but I asked for further clarification on the same subject and did not want to open a new thread"], "usr-7": ["colto wrote: I was thinking of a 'lambda function' made available using 'API gateway' that reads the number of items and reports back. That sounds like a nice approach in my opinion. It's a relatively lightweight solution. I have found the API Gateway to be unnecessarily complicated to work with, but a recent announcement seem to have changed that. As described in the blog post API Gateway Update \u2013 New Features Simplify API Development you can now basically map everything to Lambda and have the function look at the request and act accordingly. It's mostly about your Lambda function - very little work on the API Gateway side."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242751&tstart=25", "Title": "Using squid proxy in front of s3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, we are trying to use squid in front of s3. Is there a way we can configure the sdk to use this proxy for all requests? Or will we have to just use an http client and sign the requests ourselves?", "Answers": {"usr-1": ["I'll use the presigned url"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243212&tstart=25", "Title": "using getSignedURL method within Lambda", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am trying to generated a signed S3 URL from within a Lambda function, and I can't get it to work. I have this code in Lambda: var aws = require( 'aws-sdk' ); var s3 = new aws.S3( { signatureVersion: 'v4' } ); var bucket = 'test-bucket' ; var params = { Bucket: bucket, Key: 'test-file' } ; s3.getSignedUrl( 'getObject' , params, function(err, url) { console.log( 'Signed URL: ' + url); } That returns a URL that looks like: https://test-bucket.s3.amazonaws.com/test-file?X-Amz-Algorithm=AWS4-HMAC-SHA256andX-Amz-Credential= <key>\\%2F20161116\\%2Fus-east-1\\%2Fs3\\%2Faws4_requestandX-Amz-Date=20161116T150249ZandX-Amz-Expires=1000andX-Amz-Security-Token=FQoDYXdzEPD\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2FwEaDAP7Q6dx5kOs53eXMyLuASsKNCdHGHJKGhjfGUFgufgH36Fhv05MYR5S\\%2B\\%2BHustV81iYG4c21yCJZnV2Z1tBFrqn26hzPfZGXllMS2SiMituXGmCw7sscA8F855aVyMbcDKWpjn4\\%2FsVzfZQ76Gdzk6dKctemxyt9r5WYMk2IgQtgFeoyiQk\\%2BliWFkIqCqs3M55fqEj5HMQk4kyPwdXznfo0fXzmtafHrUFSHHo6sgfCgi18i8XYfPF3ibj7vu\\%2FyyKCYnc4ABzyFPC\\%2B\\%2B3gylA3\\%2B0pS7cUPqRZrmOOu19zmj7mTKOvQOVkWDRjWfVHeic\\%2BlwPWVomnvsoDoIo\\%2FNmxwQU\\%3DandX-Amz-Signature=e53abc5b096hjkg7679tyfft78rt6yifkft7b6ab7f68479c3beandX-Amz-SignedHeaders=host I stick that URL in a web browser and I just get an 'Access Denied' message. I can successfully create a signed url for the same file using the Bucket Explorer application-- this generate a URL in the form: https://test-bucket.s3.amazonaws.com/test-file?AWSAccessKeyId=andltAccess Key RemovedandgtandExpires=1479396462andSignature=qv8tddF6hjkGHKGHKjghjkGHONgEM\\%3D I wanted to try to replicate this format to test my signature but is requires the expiration timestamp instead of a count..... So, what am I doing wrong in my request? Has anyone successfully used Lambda to generate signed URLs? Edited by: youarenotagadget on Nov 16, 2016 9:26 AM", "Answers": {"usr-1": ["When in doubt, double check the permissions your Lambda has!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243843&tstart=25", "Title": "object not visible despite having same acl as visible object in same bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi I have two objects at the same prefix in a bucket, both are visible from the cli but only one shows up on my s3 console. They both have the same owner and object acl's, were both uploaded using command 'aws deploy push' from cli. The object not showing in s3 console is test.zip please advise. Kind regards, Ethan", "Answers": {"usr-1": ["test.zip was originally uploaded using \"aws deploy push\" but with no object name specified. That caused strange behavior in the console and attached object ACL. I subsequently modified the object to be same as the correctly configured object but when that didn't help I asked this question, but by the time I finished typing the question the changes had taken effect and resolved the issue. I forgot that immediate read-after-write is only available for PUTS of new objects with eventually consistency being in effect thereafter for the object."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242197&tstart=25", "Title": "S3 and IAM, what am I doing wrong?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I just created a bucket called 'minio-test' and haven't changed any default settings. I then created a new user in IAM called 'miniotester' and attached the following policy to give that user full access to the bucket: { 'Version' : '2012-10-17' , 'Statement' : [ { 'Effect' : 'Allow' , 'Action' : [ 's3:-' ], 'Resource' : [ 'arn:aws:s3:::minio-test/-' ] } ] } I then wanted to test if that worked as expected so I entered 'miniotester''s credentials under 'default' in ~/.aws/credentials and tried the following command: $ aws s3 ls minio-test/\u00a0 \u00a0 An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I suppose my policy is incorrect? I recall using that same policy for other buckets in the past and it worked. Maybe it takes some time for the policy to take effect? Anyone knows what I'm doing wrong? Edited by: olalondee on Oct 30, 2016 5:53 PM Edited by: olalondee on Oct 30, 2016 5:53 PM", "Answers": {"usr-1": ["Ok, the following policy seemed to fix the problem: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::minio-test\", \"arn:aws:s3:::minio-test/*\" ] } ] } Edited by: olalondee on Oct 30, 2016 6:10 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243962&tstart=25", "Title": "Save button missing on logging section under aws s3 console", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am trying to enable logging on an S3 bucket and the 'Save' button is missing. I have tried on multiple browsers (firefox / safari) and multiple AWS accounts. The save button is not there on any of the s3 buckets that I have tried. Repro steps: 1. login to aws console via firefox/safari 2. navigate to S3 3. select an S3 bucket - this can be a bucket with logging enabled or not, same results 4. select properties 5. select logging The enable checkbox is there, the bucket text box is there, and the prefix textbox is also there. Anybody else see this issue or know how to resolve?", "Answers": {"usr-1": ["Thank you for bringing this to our attention. We've identified the issue and are working on a fix. --Carl"], "usr-2": ["The issue should be resolved. Thanks again for reporting the issue, and apologies for any inconvenience. --Carl"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244074&tstart=25", "Title": "S3 website: won't serve example.com domain", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I've set up my static S3 website, using a custom domain from Google Domains. In the Google Domains DNS settings, I am redirecting example.com to www.example.com, and I have this CNAME record in place: Name: 'www' Type: CNAME TTL: 5m Data: example.com.s3-website-us-east-1.amazonaws.com. Thus, I successfully load my website when I navigate to either www.example.com or example.com (which just redirects). BUT - the domain is still served from 'http://example.com.s3-website-us-east-1.amazonaws.com'! I've followed the walkthroughs closely thus far. I assume there must be a way to serve my website from the 'example.com' domain without switching over to Route 53, right? Do I not have my CNAME configured correctly? Anyone? Bueller?", "Answers": {"usr-1": ["This will happen if you for example redirect to the full S3 static website address instead of your own domain. That may not be the case here. If you are willing to share the domain name (PM is fine), I would be happy to take a quick look."], "usr-2": ["Thank you - I'll PM. But do you mean redirect in the DNS from www.example.com to my S3 bucket (e.g., example.com.s3-website-us-east-1.amazonaws.com)?"], "usr-3": ["I'm following up on your PM here. The redirection from example.com to www handled by Google works fine. However, the www.example.com S3 bucket is then redirecting to the full example.com.s3-website... static website address where the actual content is currently located. You may want to 1.) disable the redirection in the www S3 bucket static website settings, and 2.) move the content from the example.com bucket to the www bucket."], "usr-4": ["This worked! Thank you."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247998&tstart=0", "Title": "Lost access to S3 Bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I've applied below S3 bucket policy to our bucket : { 'Version': '2012-10-17', 'Id': 'Policy1415115909152', 'Statement': [ { 'Sid': 'Access-to-specific-VPCE-only', 'Principal': '-', 'Action': 's3:-', 'Effect': 'Deny', 'Resource': ['arn:aws:s3:::bucket-name', 'arn:aws:s3:::bucket-name/-'], 'Condition': { 'StringNotEquals': { 'aws:sourceVpce': 'vpce-1a2b3c4d' } } } ] } The problem is that sourceVpce does not exist, therefore I can't neither access the bucke nor change the policy no matter what IAM privileges I have. Is there a way to re-gain control over that bucket?", "Answers": {"usr-1": ["In order for it to be removed, we had to have the root user login and remove it by CLI. Edited by: akreveve on Jan 30, 2017 1:55 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242153&tstart=25", "Title": "Strategies for blue-green deployment for site on s3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, Are there any suggested strategies for blue-green deployment for a site that is hosted on s3? Ideally one that is behind cloud front. Thanks, Michal. Edited by: michalc18 on Oct 28, 2016 11:22 PM", "Answers": {"usr-1": ["I've come up with a way where you put the HTML of the site behind an API Gateway. Details at http://charemza.name/blog/posts/devops/aws/blue-green-deployment-static-site-s3/ ."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244159&tstart=25", "Title": "New s3 management console - metric inconsistencies", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi all, I opted in to the new s3 management console. Great and simple design, bold and clear, much better than the old one. However, when trying to add a new metric filter, I encountered the following: 1. A filter tag is added (I added an s3 object tag) and the filter is created, however when editing the newly added filter, the tag isn't there. Also, the relevant cloudwatch metric displays wrong values (to be exact, it shows all values, the filter doesn't work at all). I want to clarify that tag names with dashes cannot be added. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics-filter.html 2. When editing bucket metric options, 'Request metrics' and 'Data transfer metrics' are both selected or not and the drop downs cannot be pinpointed. Seems like a javascript issue. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html -> Step 7 3. slashes (/) cannot be used at prefix, for example adding the prefix '/Bucket01/BigData/SparkCluster' fails with message ''Prefix search does not support the character '/'.'. Does that mean that only root prefixes can be used or the javascript validator doesn't work as expected? Prefix without slash can be added, but that limits the whole filter concept significantly. Also, the s3 faq states that slashes can be included ( https://aws.amazon.com/s3/faqs/ -> storage management -> S3 CloudWatch Metrics -> Can I align storage metrics to my applications or business organizations?) Tried with latest firefox (50.0.2) and chrome (55.0.2883.75 (64-bit)) at macos sierra (10.12.1 (16B2555)) Regards, Thanos Edited by: thanos on Dec 2, 2016 11:20 AM", "Answers": {"usr-1": ["Update Filter including tags cannot be added! The response is 'Error. We encountered an internal error. Please try again.' Procedure described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics-filter.html, the only difference is that value 'music' is a tag and not a prefix. Tried on 2 separate aws accounts. Regards, Thanos"], "usr-2": ["Hi Thanos, Thank you for reporting these issues. thanos wrote: 1. A filter tag is added (I added an s3 object tag) and the filter is created, however when editing the newly added filter, the tag isn't there. Also, the relevant cloudwatch metric displays wrong values (to be exact, it shows all values, the filter doesn't work at all). What may have happened here is that you didn't completely finish adding the tag to the filter by hitting Enter after entering the tag value. The console has been updated to prevent this from happening; can you please try adding a tag-based filter again? I want to clarify that tag names with dashes cannot be added. Hyphens in tag names are supported; you can see the supported characters for tags in the developer guide: http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html. It should be possible to set up a metrics filter for any valid tag. The console currently does not allow some characters that it should, but we're working on a fix for that; in the meantime, as a workaround, you can use the AWS CLI to configure metrics: http://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-metrics-configuration.html. 2. When editing bucket metric options, 'Request metrics' and 'Data transfer metrics' are both selected or not and the drop downs cannot be pinpointed. Seems like a javascript issue. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html -> Step 7 The S3 API currently only supports enabling or disabling them together. I agree that the screenshot in the documentation is misleading; I've requested that it be updated. 3. slashes (/) cannot be used at prefix, for example adding the prefix \"/Bucket01/BigData/SparkCluster\" fails with message ''Prefix search does not support the character '/'.\". The issue that prevented using slashes in prefixes has been resolved; can you please try this again? Please note however that your keys probably do not start with a slash, so in your example, if the bucket name is \"Bucket01\", the prefix value that you will want to enter is \"BigData/SparkCluster\". We encountered an internal error. Please try again. This message can appear when configuring a tag filter where the value is an empty string. We are working on a fix for this issue and hope to have it deployed soon. Thanks, Miles"], "usr-3": ["Hi Miles, Thank you for your great reply. Everything now works as described. The javascript validators work as expected, tags and prefixes can be added. It was a bit obscure that tags should be in the form '<tag key> | <tag value>', neither was it documented somewhere, the new javascript helper made that crystal clear. Regarding point 2, I hope that bucket metric pinpointing would be available soon. Regards, Thanos"], "usr-4": ["milesaws wrote: This message can appear when configuring a tag filter where the value is an empty string. We are working on a fix for this issue and hope to have it deployed soon. Metric filters that include tags with empty values are now fully supported. -Miles"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236826&tstart=50", "Title": "Are there known issues when MySQL operates on EBS storage?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I'm testing an App together with MySQL on a free t2.Micro instance that comes with EBS. However, it ran out of space for the root directory, where it shows 100\\% as per below, and my App and MySQL went down. Filesystem Size Used Avail Use\\% Mounted on udev 492M 8.0K 492M 1\\% /dev tmpfs 100M 348K 99M 1\\% /run /dev/xvda1 7.8G 7.7G 0 100\\% / none 4.0K 0 4.0K 0\\% /sys/fs/cgroup none 5.0M 0 5.0M 0\\% /run/lock none 497M 0 497M 0\\% /run/shm none 100M 0 100M 0\\% /run/user Are there known issues on MySQL sitting on an instance supported by EBS or if we are running MySQL DB, it should sit in an instance with fixed storage space. Some urgent advice is much appreciated. Thanks.", "Answers": {"usr-1": ["Hello There are no issues running MySQL on an EBS backed EC2 instance. Please ensure that the EBS volume is large enough. For a managed MySQL instance, please try Amazon RDS for MySQL (https://aws.amazon.com/rds/mysql/). Regards Conrad"], "usr-2": ["Replies no longer required. Forgotten this is a free test instance, so EBS is capped at 8GB, unlike my other paid instances."], "usr-3": ["Hi, thanks for your confirmation on no known issues of MySQL running on EBS. Just realised the free instance comes with a cap at 8GB, unlike my paid instances. Thanks."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237633&tstart=50", "Title": "SQL Server backups corrupted after downloading them from S3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi All, I have a weird problem, we store SQL Server database backups in S3 and every once in a while we download them to perform restores but when I try this i get the following errors in SQL Server 2008 R2: 'The media family on device X is incorrectly formed' Which according to my brief online research normally points to corruption in the backup. Now this is where it gets really strange. When i download the backups via a browser like Firefox the backups restore perfectly fine but when I use a client like S3 Browser (www.s3browser.com) or Cloudberry Explorer the restore fails with the above error message. When i compare the file size i noticed it differs very slightly when I download it via a browser compared to the above mentioned S3 clients. So e.g. for Firefox the file size is 6,555,466KB and when i use S3 client the size is 6,379,796KB. Why this discrepancy? What is going on here??? Why does using these clients seem to corrupt the SQL backups when i download them? Please help! Edited by: Rabi Achrafi on Aug 19, 2016 7:26 AM", "Answers": {"usr-1": ["Has anyone else experienced this or at least provide suggestions on what could be causing this? Any help would be greatly appreciated."], "usr-2": ["Hi Rabi, Let's start with what software do you use to back up SQL databases?"], "usr-3": ["SQL Server 2008 R2, we don't use any special third party backup software and no encryption/compression is used."], "usr-4": ["I think I now know what is causing this, so basically i realised the backup software which uploads the SQL backups to S3 actually compresses it at the same time thus altering the file state ever so slightly. I have now disabled this feature so restoring backups should now work. Thanks All."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231276&tstart=75", "Title": "S3 Encryption and Cross Region Replication Issues?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Ok, little about what I'm attempting to do: 1) Script the archival of backup files and upload to S3 bucket using the aws CLI. 2) Encryption: Be able to have the backup file easily encrypted/decrypted server-side (preferable, as I'd rather not have to maintain keys and extra code). eg. --sse flags via aws CLI 3) Have the S3 bucket replicate to another region via Cross-Region Replication. A backup for my backup for my backup. Need to meet those options and additionally, this needs to remain simple as it needs to scale. I've managed these things individually but have encountered issues with the cross-region-replication due to the encryption. For encryption, I've first attempted creating and using a kms key, which is pretty standard and how I generally encrypt/decrypt my S3 files. This works great, my backup file gets encrypted server-side with simple flag '--sse aws:kms --sse-kms-key-id blahblahmy-encryption-key' Only problem is any files encrypted this way don't show up in my replicated S3 bucket.. The requirements for S3 Cross-Region-Replication state that client and server side encrypted content is ignored for cross-region-replication: 'Objects created with server-side encryption using either customer-provided (SSE-C) or AWS KMS-managed encryption (SSE-KMS) keys are not replicated. ' From here I'm not sure where to go. S3 Replication docs seem to suggest to encrypt using the 'S3 master key', for replication. However, I don't know for sure what that is or how to specify via the AWS CLI.. Nor do I know what control I have over that key and how secure or any management options for it etc. I've tried changing my aws s3 cp flag to '--sse aws:kms'. It successfully uploaded my backup file with details specifying 'Server Side Encryption:Using AWS KMS master key: aws/s3 (default)', which would seem to be correct? However, it was not replicated to my other region's bucket so I'm unsure. I'm sure there are solid reasons but seems silly to have limitations on Cross-Region-Replication like that. Any ideas on how to approach this? I just want a simple encrypted backup which I have, but my other caveat is I must have multiple backups and preferrably with zero additional / minimal effort..", "Answers": {"usr-1": ["Is it possible at all? Anyone, bueller?"], "usr-2": ["S3 cross region replication currently supports replication of SSE-S3 objects (described at the following link: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html) We suggest using SSE-S3 as the encryption method and cross region replication will replicate those objects. Cross Region Replication does not currently support replication for KMS encrypted S3 objects. We have recorded Cross Region Replication for KMS objects as a feature request."], "usr-3": ["Thanks abhilasha, I think I was having issues utilising SSE-S3 as I was trying to specify it via the s3 cli. If I'm not mistaken I needed to use \"s3api\" eg. aws s3api put-object --bucket xx --key xx --body xx --server-side-encryption AES256 Eitherway, my scripted backup process is now working nicely with cross-region-replication! Cross-Region-Replication support for SSE-KMS or SSE-C encrypted bucket objects would certainly get my +1 as a feature request though! Edited by: ninjada on May 25, 2016 6:31 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227272&tstart=100", "Title": "S3 Glacier Restore Status via PHP", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Is there a way to determine the Glacier Restore status via the PHP SDK? I don't see the metadata or details in the values returned via ListObject?", "Answers": {"usr-1": ["Use GetObject instead, and look at the Restore value of the result. And set the Range to \"bytes=0-0\" to skip retrieving the content of the file itself. And be sure to trap for exceptions -- if the object is in Glacier and not restored, AWS will throw an InvalidObjectStateError and the script will die if the error is not caught. Edited by: ds2jd on Mar 12, 2016 12:44 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237507&tstart=50", "Title": "images don't display", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "In S3, I have an index.html, a CSS file, and an images folder with 3 jpg in it. I cut and pasted the jpgs into the S3 folder. They show there. I set a display policy per the instructions to display to everyone. The page displays, but the images do not. I get the alt text and the tooltips, but no images. On the page, when I right click the Open Image in New Tab, I get an Access Denied message in XML. The page is - http://s3.amazonaws.com/pittsfordcommunitycenter/index.html Images display in my development PC. I've tried a lot of things in AWS, including what the documentation says. What do I need to change?", "Answers": {"usr-1": ["Hello MikeL, I am unable to reproduce this issue. Except for the favicon.ico I am able to load all objects. If you are still seeing 403 Access Denied for images I would suggest clearing browser cache and trying again. Regards, Dilip S"], "usr-2": ["The problem is now solved. I thought I made an error in AWS somewhere, but the error was in the image tags in my HTML. I changed ../image/ to ./image/ and everything works on my dev PC and in AWS. Thanks for looking at the situation."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=228684&tstart=100", "Title": "S3 uploads take longer using PHP SDK, occasionally VERY long", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We have a PHP-application that uploads files to S3 using the PHP SDK. We've noticed that sometimes the transfers take a very long time. We were unable to reproduce the problem with the AWS CLI, using 'aws s3 cp', so I created a simple test script. The script uploads the same 1GB file to the same S3 location using the AWS CLI, the PHP SDK using SSL, and the PHP SDK without SSL (the last due to some mentions on the net that said disabling SSL might speed things up). The uploads are then repeated. Here are the results from the test, after a total of 47 uploads (I ran 60 cases but lost the logs for the first ones): Average seconds SDK w/SSL: 36 SDK w/o SSL: 272 AWS CLI: 26 Median seconds SDK w/SSL: 34 SDK w/o SSL: 43 AWS CLI: 20 The CLI tool is consistently almost twice as fast as the SDK. But more importantly, sometimes the SDK just takes incredibly long. The dataset contains delays of 159 seconds, 105 seconds, and 3582 seconds. Any idea what's going on in here? Full data attached (as an image, since the forum doesn't allow pdfs).", "Answers": {"usr-1": ["Hi, I see that you've opened case 1706615101 with AWS Support. A Support engineer has already reached out to you via that case and we'll be working with you there to help resolve your issue. Best Regards, Omar S."], "usr-2": ["And for the benefit of anyone else with the same problem: Use the multipart upload. https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html My code was essentially like this: {code} $options = [ 'region' => 'eu-west-1', 'version' => 'latest', 'signature_version' => 'v4', ]; $s3 = new S3Client($options); // Upload a file. $result = $s3->putObject([ 'Bucket' => $bucket, 'Key' => $keyname, 'SourceFile' => $filepath, ]); {/code} And that doesn't use multipart upload."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227301&tstart=100", "Title": "S3 -> Glacier happening for only some files", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have an S3 bucket that contains several files that has a lifecyle rule to 'Archive to the Glacier Storage Class 0 days after the object's creation date.'. After three days one of the files in that bucket continues to show storage class Standard where all the other files show Glacier. The only thing that differentiates that file from the other is size, it's ~250 GB where the next largest file is ~30 GB. Is there a size limit for files to be archived to Glacier? Or does it just take longer for large files? marcos", "Answers": {"usr-1": ["After 3 days the large file did change to glacier class, so it appears that large files just take longer. marcos"], "usr-2": ["Hello marcoshw, I'm an engineer on the S3 team. First, there is no size limit for objects transitions to Glacier. Second, 3 days for a 250GB object sounds abnormally high. Could you PM me the bucket and object name that you had that issue with? We can investigate and understand why this object didn't transition faster. Because our billing system bills at the rate of the storage where the object should be, it is important to note that you were being billed at Glacier rate even when your object was still in S3 Standard. Thank you for helping us understand what happened here. Nicolas. Edited by: Nicolas@AWS on Mar 16, 2016 4:17 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227300&tstart=100", "Title": "Help me solve the bucket endpoint error!!!!!", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, Can someone help me with this S3 bucket error that i'm attaching. I'm also attaching the script i used in my ec2 instance to create a server. i'm using ec2 instance to upload the data in s3 and then make entries in DynamoDB. My s3 and ec2 bucket region is Singapore (ap-southeast-2). Thank You!! Regards, Ashutosh, Researching on AWS.", "Answers": {"usr-1": ["This question cannot be answered."], "usr-2": ["This question cannot be answered."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242047&tstart=25", "Title": "Ohio region not actually ready for production?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I created a new S3 bucket in the also new Ohio region. This announcement from AWS says you can use S3 in Ohio now. So.... ...I use a Gulp build script to compress and upload to S3. I have used this for years. I gave my code upload user IAM permissions to the new bucket and tried to upload some code. NOPE Just to make sure I wasn't doing something dumb I created another new S3 bucket, only this time marking the region as US Standard instead of Ohio. IAM permissions, tried upload. YUP! So the only difference is the region specified when creating the bucket. Two questions Q1) I don't know why S3 buckets are tagged by location in the first place because aren't they supposed to be global? Q2) Why isn't Ohio up and running like the announcement says?", "Answers": {"usr-1": ["I'm sorry you are running into trouble. I suspect that the reason you are having issues is that all new AWS regions require Sigv4 signing for S3 requests - http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html You will notice the note on this page: Amazon S3 supports Signature Version 4, a protocol for authenticating inbound API requests to AWS services, in all AWS regions. At this time, AWS regions created before January 30, 2014 will continue to support the previous protocol, Signature Version 2. Any new regions after January 30, 2014 will support only Signature Version 4 and therefore all requests to those regions must be made with Signature Version 4. For more information about AWS Signature Version 2, see Signing and Authenticating REST Requests in the Amazon Simple Storage Service Developer Guide. Also, while the bucket namespace is Global, AWS regions allow your data to be stored where you want it, close to your customers or close to your compute resources in AWS in order to provide the least latency for your applications. I hope this helps."], "usr-2": ["LeeK@AWS, Thanks for the heads up - I missed the nuance of how SigV4 application differed from region to region. Its confusing and not the ideal, but anyway, we are through it. Thank you. For others in a similar situation using Gulp to upload, the easy fix to this issue is to switch to a different Gulp module. This is the one we switched to and we now have no SigV4 issues. The other NPM S3 module referenced above is not being kept current."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227465&tstart=100", "Title": "S3 connection timeout and extremely slow access speed.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello team, We are facing trouble with s3 access from ec2 instances in us-east-1 region. Most of the time our applications getting files from s3 (boto) timeout/fail and when I try manually from command line (awscli and s3cmd), the speed is extremely slow. One of the server I am trying this is i-0f5c868b. This is not specific to this instance though, many users have complained the same problem on other servers. Please advice. Update: Below is the boto error we get, Traceback (most recent call last): File '<stdin>', line 1, in <module> File '/usr/local/lib/python2.7/dist-packages/boto/s3/key.py', line 1712, in get_contents_to_filename response_headers=response_headers) File '/usr/local/lib/python2.7/dist-packages/boto/s3/key.py', line 1650, in get_contents_to_file response_headers=response_headers) File '/usr/local/lib/python2.7/dist-packages/boto/s3/key.py', line 1482, in get_file query_args=None) File '/usr/local/lib/python2.7/dist-packages/boto/s3/key.py', line 1535, in _get_file_internal for bytes in self: File '/usr/local/lib/python2.7/dist-packages/boto/s3/key.py', line 386, in next data = self.resp.read(self.BufferSize) File '/usr/local/lib/python2.7/dist-packages/boto/connection.py', line 413, in read return http_client.HTTPResponse.read(self, amt) File '/usr/lib/python2.7/httplib.py', line 561, in read s = self.fp.read(amt) File '/usr/lib/python2.7/socket.py', line 380, in read data = self._sock.recv(left) File '/usr/lib/python2.7/ssl.py', line 241, in recv return self.read(buflen) File '/usr/lib/python2.7/ssl.py', line 160, in read return self._sslobj.read(len) ssl.SSLError: The read operation timed out Edited by: sabarishm on Mar 16, 2016 1:12 AM", "Answers": {"usr-1": ["Hello, It looks like this instance is routing all Internet facing traffic through a m1.small NAT instance, i-8e55cc5d. I recommend setting up a VPC Endpoint for S3. This will allow S3 traffic to be handled without going through your NAT instance, which should speed things up. See here: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html Richard"], "usr-2": ["Thank you Richard. Your advice makes sense, I will setup an service endpoint for s3 and route traffic through that."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227505&tstart=100", "Title": "HTML Upload Page Error HTTP POST", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am trying to create a webpage to allow anyone to upload to one of my s3 buckets. I have been following some guidelines but keep getting an error message: <Message> The request signature we calculated does not match the signature you provided. Check your key and signing method. </Message> What did I do to get here? -I created a new IAM user called WebPageUploader with explicit permissions -Created a bucket policy using the bucket policy generator and applied it to the bucket and saved it locally on my computer as a .json file (this is more open of a policy than I would like, but trying to start with this). { 'Id' : 'Policy1458150350258' , 'Version' : '2012-10-17' , 'Statement' : [ { 'Sid' : 'Stmt1458150348531' , 'Action' : 's3:-' , 'Effect' : 'Allow' , 'Resource' : 'arn:aws:s3:::MYBUCKET' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::ACCTNUMBER:user/WebPageUploader' ] } } ] } -Ran that .json file through the following python code (via link: https://aws.amazon.com/articles/1434 ). import base64, hmac, hashlib \u00a0 pol_file=open( 'c:/users/andrew/documents/s3_upload/upload_policy.json' , 'r' ) pol=pol_file.read().strip() policy=base64.b64encode(pol) \u00a0 signature=base64.b64encode(hmac.new( 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' , policy, hashlib.sha256).digest()) \u00a0 print 'Policy:' print policy print '' print '' print 'Signature:' print signature -put the right parts into the HTML <html> <head> <title>S3 POST Form</title> <meta http-equiv= 'Content-Type' content= 'text/html; charset=UTF-8' /> </head> \u00a0 <body> <form action= 'https://MYBUCKET.s3.amazonaws.com/' method= 'post' enctype= 'multipart/form-data' > <input type= 'hidden' name= 'key' value= 'uploads/${filename}' > <input type= 'hidden' name= 'AWSAccessKeyId' value= 'MYKEY' > <input type= 'hidden' name= 'success_action_redirect' value= 'https://s3.amazonaws.com/MYBUCKET/successful_upload.html' > <input type= 'hidden' name= 'acl' value= 'private' > <input type= 'hidden' name= 'policy' value= 'ew0KICAiSWQiOiAiUG9saWN5MTQ1ODE1MDM1MDI1OCIsDQogICJWZXJzaW9uIjogIjIwMTItMTAtMTciLA0KICAiU3RhdGVtZW50IjogWw0KICAgIHsNCiAgICAgICJTaWQiOiAiU3RtdDE0NTgxNTAzNDg1MzEiLA0KICAgICAgIkFjdGlvbiI6ICJzMzoqIiwNCiAgICAgICJFZmZlY3QiOiAiQWxsb3ciLA0KICAgICAgIlJlc291cmNlIjogImFybjphd3M6czM6OjphbmRyZXdjbGF1czIiLA0KICAgICAgIlByaW5jaXBhbCI6IHsNCiAgICAgICAgIkFXUyI6IFsNCiAgICAgICAgICAiYXJuOmF3czppYW06OjA4NDIwNzUxMjQ2NTp1c2VyL1dlYlBhZ2VVcGxvYWRlciINCiAgICAgICAgXQ0KICAgICAgfQ0KICAgIH0NCiAgXQ0KfQ==' > <input type= 'hidden' name= 'signature' value= 'bNXjERuP03iMe9/1IFpiFx6MnPtxo83UhKpm+pfsCiw=' > <!input type= 'hidden' name= 'Content-Type' value= 'image/jpeg' > <!-- Include any additional input fields here --> \u00a0 File to upload to S3: <input name= 'file' type= 'file' > <br> <input type= 'submit' value= 'Upload File to S3' > </form> </body> </html>", "Answers": {"usr-1": ["bump"], "usr-2": ["Anyone able to help?"], "usr-3": ["After a lot of work, I got this figured out. I wrote a detailed how to guide. Check it out here: http://www.thoughtsoftheconfused.com/index.php/2016/06/25/creating-your-own-secure-private-file-upload-website-with-amazon-s3/"], "usr-4": ["Five stars for posting your article. Very informative and valuable. Well written and easy to understand."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227611&tstart=100", "Title": "AWS S3 billing issue", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, i opened a ticket about S3 billing issue to support but i've been redirected here. (see case id 1679820921) I've executed some aws cli command to show my current s3 usage, but it seems that i've been charged from at least a year for huge amount of data i don't have on s3. Right now the was cli reports: root@raspberrypi:/usr/local/ec2# aws s3 ls hs-backup-dbmaster1g --recursive --summarize --human-readable 2016-03-06 12:52:02 79.8 GiB weekly/dbmaster1g-06032016-weekly/DBMASTER1G.tar 2016-03-13 12:49:49 80.2 GiB weekly/dbmaster1g-13032016-weekly/DBMASTER1G.tar 2016-02-28 12:50:19 79.4 GiB weekly/dbmaster1g-28022016-weekly/DBMASTER1G.tar Total Objects: 3 Total Size: 239.4 GiB In spite of that, cloud watch metrics reports a usage of 1.2TB for that bucket. Could you please double check on this and give me an explanation? Thanks, Carlo", "Answers": {"usr-1": ["Hi Carlo, I'd be interested to know whether you get this resolved - I've got a very similar issue. I've been trying to get to the reason why my Amazon S3 monthly cost has trebled over the last three months after about 2-3 years of it being fairly constant. I use it for server backups so have disabled all of them whilst I try to get to sort it out, so there's no data going into / out of S3 either. Using S3 explorer, my usage is 5.2TB which is approximately what I'd expect it to be. However, using Cloudwatch it reports it to be 17.1TB at the moment. Thanks Mike"], "usr-2": ["m3s-hub, we have reached out to you through our internal ticketing system with instructions on how to address this issue. For 'M. Taylor', and anyone else who is experiencing a similar issue, please continue reading. You can use this command example to list your incomplete multi-part uploads: aws --output json s3api list-multipart-uploads --bucket mybucket Normally, we would recommend cleaning up these incomplete parts manually using the AWS CLI. However, yesterday we announced a new Multi-Part-Upload Cleanup Lifecycle policy which will handle this work for you. You can read our about this new feature on our official blog here: AWS Blog: https://aws.amazon.com/blogs/aws/ Direct Link to article: https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/ Please follow the instructions included in Jeff's blog post to apply a Lifecycle policy to each of your buckets which you identify incomplete MultiPart Uploads, that Lifecycle policy will then handle cleaning up all your abandoned multi-part-uploads."], "usr-3": ["Thanks Joshua, In the mean time I've raised a support ticket and been given the same advice. It was old / failed multipart uploads which were contributing to the large amount of data that was being used. Just to re-iterate for anyone else, those multipart uploads don't show up either when browsing your files within AWS or when using 3rd party programs which list the files within S3 (e.g. S3 browser). However, the total space being used, including the multipart uploads, is shown if you look at the BucketSizeBytes metric in Cloudwatch. There are a number of methods for deleting old multipart uploads but I found by far the easiest was to setup a lifecylce rule which will delete any multipart uploads older than a certain number of days. It probably took about 24hrs for the lifecycle rule to be applied and all those old files to be deleted so be patient if you have a lot of files. You should then also be able to see the changes to the size of your data within Cloudwatch."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=228130&tstart=100", "Title": "Does replication work with versioning suspended?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Replication docs says: 'Both the source and destination buckets must be versioning-enabled when you configure replication on a bucket.' S3 console says: 'Once enabled, Versioning cannot be disabled, only suspended.' So how does this third state, 'suspended', affect replication? Thank you.", "Answers": {"usr-1": ["CRR requires Versioning to be enabled on both the source and destination buckets. If versioning is suspended on the source or destination, then CRR will fail to replicate objects which have been updated on the source after that change. S3 will not permit you to suspend versioning on a source bucket with a CRR configuration. While you can suspend versioning on the destination bucket, doing so will result in CRR effectively failing any replications which would have occurred from that point."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=228120&tstart=100", "Title": "Bucket Policy Editor", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "In the 'Bucket Policy Editor' I keep getting this error 'We encountered an internal error. Please try again.' policy: { 'Id': 'Policy1458741229198', 'Version': '2012-10-17', 'Statement': [ { 'Sid': 'Stmt1458741225276', 'Action': 's3:-', 'Effect': 'Allow', 'Resource': 'arn:aws:s3:::Bucket/-', 'Condition': { 'IpAddress': { 'aws:SourceIp': '0.0.0.-' } }, 'Principal': '-' } ] } IP is mine and bucket is mine not the crap I just stuck in there attached is a screen shot", "Answers": {"usr-1": ["Hello, Thanks for using the AWS Developer Forums! I have tested your current policy and receive the same error message. For a valid policy, you must make the following changes: 1) Change line \"aws:SourceIp\": \"0.0.0.*\" to \"aws:SourceIp\": \"0.0.0.0/0\" Explanation: the \"IPAddress\" key value specified in the condition must use CIDR notation. https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#AvailableKeys-iamV2 2) Ensure in the line \"Resource\": \"arn:aws:s3:::Bucket/*\" you replace \"Bucket\" with the name of the Bucket that the Policy is for. Example: \"Resource\": \"arn:aws:s3:::my-bucket-name/*\" Please note I have only tested the validity of your policy in the Bucket Policy Editor. I have not tested the access to the bucket using this access policy. A lot of example Bucket Policies are in our documentation here: http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html If I can be of further help please let me know. Best regards, Joel K. Amazon Web Services"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=228204&tstart=100", "Title": "Policy prevents bucket and policy deletion", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I have a bucket cdn-castle [dot] com that has the following policy: { 'Version': '2008-10-17', 'Id': 'PolicyForCloudFrontPrivateContent', 'Statement': [ { 'Sid': '1', 'Effect': 'Deny', 'Principal': { 'AWS': '-' }, 'Action': 's3:-', } ] } This in effect prevents me from editing the s3 policy. How can I delete this bucket? Thanks. Edited by: pooaws on Mar 24, 2016 1:05 PM", "Answers": {"usr-1": ["Hello, Thank you for posting to the AWS forums. Please try the following: The current (as of 2014-12-25) AWS CLI has a command to delete the bucket policy. aws s3api delete-bucket-policy <bucket-name> In the case of an explicit deny, you will need to use root access keys. Typically you don't want access keys for the root account, but can create them long enough to execute the necessary commands and then delete them. Refer to the CLI documentation on how to set access keys. Info on AWS CLI: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html Full documentation on the s3api commands: htttp://docs.aws.amazon.com/cli/latest/reference/s3api/index.html Please let us know if this helped you resolve."], "usr-2": ["Hi, Sorry for the late reply, using the root access key seems to have worked. Thanks for the help."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=228465&tstart=100", "Title": "PutObject Access Denied Cross account but GetObject works fine.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, Adding the policy below works for GetObject, but not PutObject for some reason ( although it worked for PutObject on the same-account role ) As you can see I tried 3 different Principals, and none of them worked for PutObject. I also tried adding the PutObjectAcl permissions but that didn't fix it either. { 'Sid': 'ThumborAllow', 'Effect': 'Allow', 'Principal': { 'AWS': [ 'arn:aws:iam::ACCOUNT-ID-A:role/thumborInstanceRole', 'arn:aws:iam::ACCOUNT-ID-B:root', 'arn:aws:iam::ACCOUNT-ID-B:role/aws-elasticbeanstalk-ec2-role', 'arn:aws:iam::ACCOUNT-ID-B:role/aws-elasticbeanstalk-service-role' ] }, 'Action': [ 's3:GetObject', 's3:PutObjectAcl', 's3:PutObjectVersionAcl', 's3:PutObject' ], 'Resource': 'arn:aws:s3:::BUCKET-NAME/-' } The error is as follows: ec2-user@ip-10-23-24-85 ~ $ aws s3 cp test.file s3://BUCKET-NAME/test.file upload failed: ./test.file to s3://BUCKET-NAME/test.file A client error (AccessDenied) occurred when calling the PutObject operation: Access Denied Why would this policy enable the GetObject permission but not PutObject cross accounts? Is there some additional special configuration that must be done in order to use PutObject across accounts ? THanks.", "Answers": {"usr-1": ["nevermind found the issue, had to give my role s3 full access. Edited by: flxsmart on Mar 29, 2016 2:31 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=229067&tstart=100", "Title": "Access Denied when trying to get Bucket Location", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Given: 2 amazon accounts A and B. Account A has two buckets B1 and B2. Each of the buckets has the following policies set up to allow account B to list and get location of each bucket and both buckets are in Ireland. The question is why I can successfully get location of B1 from Python boto library with account B credentials, but get 'Access Denied' for B2? { 'Version' : '2012-10-17' , 'Id' : 'Policy1454928436063' , 'Statement' : [ { 'Sid' : 'Stmt1454928409407' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : 'arn:aws:iam::---Account B IAM---:root' } , 'Action' : [ 's3:ListBucket' , 's3:ListBucketMultipartUploads' , 's3:GetBucketLocation' ], 'Resource' : 'arn:aws:s3:::---Account A BucketName---' } , { 'Sid' : 'Stmt1454928409407' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : 'arn:aws:iam::---Account B IAM---:root' } , 'Action' : [ 's3:GetObjectAcl' , 's3:DeleteObject' , 's3:GetObject' , 's3:PutObjectAcl' , 's3:ListMultipartUploadParts' , 's3:PutObject' ], 'Resource' : 'arn:aws:s3:::---Account A BucketName---/-' } ] } Edited by: iliaznk on Apr 8, 2016 5:41 AM Edited by: iliaznk on Apr 8, 2016 5:43 AM", "Answers": {"usr-1": ["For some reason everything just works now!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=229219&tstart=100", "Title": "Do pre-signed URLs still work for uploading objects?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hey, I have used pre-signed URLs in the past to write files to a private bucket, but I am not able to get to work right now. The 'Everyone' group can list the contents of the bucket, but nothing else. I am using the latest Java SDK and I get a 403 error when I try to run the example from the documentation. [1] Has anyone else seen this or figured out why this isn't working? Thanks, Alan Field [1] http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObjectJavaSDK.html", "Answers": {"usr-1": ["Never mind. Turned out to be an issue with my credentials. Thanks, Alan"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=229788&tstart=100", "Title": "S3 Access Denied in Browser", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I am trying to expose an S3 bucket to a specific set of IAM users. I have created the policy below with my username, and yet I keep getting 'Access Denied' if I go to the URL with curl or a browser. What am I missing here? { 'Version' : '2012-10-17' , 'Statement' : [ { 'Sid' : '' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::<ACCOUNT_NUMBER>:user/ripcity' ] } , 'Action' : '-' , 'Resource' : 'arn:aws:s3:::my-test-bucket/-' } ] } Edited by: ripcity2015 on Apr 20, 2016 12:33 PM Edited by: ripcity2015 on Apr 20, 2016 12:34 PM", "Answers": {"usr-1": ["Hello, Please try the policy below: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::<ACCOUNT_NUMBER>:user/ripcity\" ] }, \"Action\": \"*\", \"Resource\": [ \"Resource\": \"arn:aws:s3:::my-test-bucket/*\", \"Resource\": \"arn:aws:s3:::my-test-bucket\" ] }, { \"Effect\": \"Deny\", \"Principal\": {}, \"Action\": \"s3:DeleteBucket\", \"Resource\": \"arn:aws:s3:::my-test-bucket/*\" } ] } Regards, Vlad, CloudBerry Lab"], "usr-2": ["Actually, I was not aware of the authentication method needed for the S3 URL's. For anyone stumbling across this post, read this: https://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=229838&tstart=100", "Title": "S3 Transfer Acceleration via the Console?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "If Transfer Acceleration is enabled on a bucket, is it automatically used for transfers done via the AWS Console? Edited by: ds2jd on Apr 21, 2016 8:37 AM", "Answers": {"usr-1": ["The console doesn't use Transfer Acceleration regardless of whether Transfer Acceleration is enabled or not."], "usr-2": ["You can use FREE CloudBerry Explorer to enable and upload with S3 Transfer Acceleration - http://www.cloudberrylab.com/blog/amazon-s3-transfer-acceleration-support-in-cloudberry-explorer-v4-6/"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=227942&tstart=100", "Title": "Files/Folders keep getting deleted", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, i'm a member of S3 for some years now and never had an issue. I'm using cloudberry explorer for amazon S3 in order to sync and upload folders with backup files that i use throughout the day. I just enable a new folder to upload called 'Public' with files and three more folders in it. Every time i upload the next day i find the folder empty like the files are completely deleted and i have to re upload them again. What can i do to narrow down the case ? are there deleted by amazon ? Any help will be greatly appreciated. Account Id: 484879790753", "Answers": {"usr-8": ["Well you are half way right it seams. The folder that i sync was inside another folder which has a sync as well from cloudberry. That sync has delete folders so since it was not part of the second sync inside that folder it deleted my first folder. Confused ? I KNOW !"], "usr-1": ["Can someone reply on this ? it is happening all over every time after one day or a week or completely random. I'm afraid i'm going to loose important stuff from my other backup folders tnx"], "usr-2": ["Hi, Do you have any lifecycle policies running on your bucket that could be moving/deleting the files from S3? Cheers, Stu..."], "usr-3": ["Not that i know off. Where do i find such a setting?"], "usr-4": ["From within S3: Select the Bucket you are using to store the folders Select 'Properties' Select 'Lifecycle' From here you can see if you have any lifecycle polices set or enabled. Cheers, Stu..."], "usr-5": ["nope as i suspected. no lifecyles. I tried to rename the folder and upload again but after a few days i enter the folder and it was empty."], "usr-6": ["You mentioned you are using CloudBerry to sync. Could the issue be here? If you are moving the folders via CloudBerry and a sync is also in place then could it be that once the folders are moved, a sync happens syncing you local drive (that no longer has the folders) to S3 at which point it proceeds to remove the existing folder as a part of the sync process? Cheers, Stu..."], "usr-7": ["yeah i thought of that only i do not have delete folders an option on the cloudberry and i have another sync job that works perfectly. I will try to recreate the folder and re create the job. It is very weird as it happens in total random without me even running cloudberry"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=229855&tstart=100", "Title": "S3 Lifecycle Conflicts for IA and Expiration - Options?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We have a larger S3 bucket that we'd like to do this: For one prefix - delete at 45 days For whole bucket - Infrequent Access at 60 days For another prefix - delete at 180 days However the S3 console isn't letting me have the first two at the same time - it errors, telling me the IA date of the whole bucket has to be less than the deletion date of a prefix. I'd really like to delete those before they ever get into IA, as they really are temporary that we don't look at beyond 45 days. Other options I considered, but won't work: Have whole bucket go to IA at 15 days - we can't, as you can't do IA at less than 30 days Have whole bucket go to IA at 30 days- that would work, but we'd be charged for the 30 days storage for the deleted at 45 days files, that is less than optimal. have a specific IA lifecycle for the 45 day-delete prefix - it won't let me put that at longer than the delete Without having to move the files/how we get this to work, anyway to get this to work?", "Answers": {"usr-1": ["Hello, As you've discovered S3 does not allow overlapping prefixes for Lifecycle Rules. There are two possibilities that occur to me: One option would be to replace your \"whole bucket\" rule with individual rules for every prefix other than the ones you've got for 45 days and 180 days. Another option would be to have a separate bucket for files you want to delete at 45 days. Richard"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=230322&tstart=100", "Title": "Unable to add lifecycle rule", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm unable to add an automatic 'permanently delete' (after 2 days) object delete rule for the whole bucket because i'm getting a strange error: Found two rules with same prefix ' (Sic). The bucket already has an 'End and Clean up Incomplete Multipart Uploads' rule. Edited by: kernpack on Apr 28, 2016 5:43 AM", "Answers": {"usr-1": ["Hello, I would like to assist you. Please share the bucket name with me via a private message. Thanks and Regards, Abhilasha"], "usr-2": ["The solution is to simply integrate the \"normal files\" deletion rule alongside the multipart one."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=230560&tstart=100", "Title": "Suddenly getting \"AWS HTTP Error...Client error 403\" accessing S3 bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "My site code and access credentials have not changed in months and have been used every day without incident. Now, suddenly I am seeing these errors when trying to download files from and upload files to an S3 bucket. I tried using brand new IAM credentials with full S3 access but that did not resolve the problem. IAM shows the last access time for the old credentials as 2016-04-29 16:15 CDT. The new credentials show 'N/A' as the Last Used time. The web server instance is i-6599ae99 and the bucket is rtirails5 in us-east-1. Edited by: Aaron Rice on May 2, 2016 8:31 AM", "Answers": {"usr-1": ["Solved my problem. The 403 errors were coming to me via BugZilla; the full error messages were being truncated. I checked the log files to get the full messages and found this included in the error report: \"RequestTimeTooSkewed (client): The difference between the request time and the current time is too large.\" The server's time was off by about 15 minutes. Setting it to the correct time resolved the problem."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=230130&tstart=100", "Title": "Any plans to fix the \"domain name taken\" issue?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We are working on launching a website hosting service that would be predicated on hosting static content on S3. However, we want our customers to be able to use any domain they wish and do not want to have to turn them away when someone else has 'stolen' the S3 bucket name that matches their domain name. Are there any plans to add some sort of mapping feature (e.g. with Route 53 aliases) that would allow us to have a domain name associated with any bucket? It is really a shame to have to use a different cloud provider because of this issue. Thanks AJS", "Answers": {"usr-1": ["I'm in the same boat, or in this case life raft since my boat is sinking."], "usr-2": ["Unfortunately, the feature is not supported at this time, but we have created an internal feature request on your behalf for this. We are unable to provide any guarantees on the implementation and deployment timelines. But you can keep checking our blog at https://aws.amazon.com/blogs/aws/ where all new feature announcements are made. The recommended work around for your use-case is that, you could use a single bucket to host all of your clients' domains (in different sub-directories) and use CloudFront to serve the content. i.e. one CloudFront distribution for each client domain using the 'Origin path' to point to the proper prefix on the S3 bucket path for that particular client while creating each distribution. Regards, Oluwamayowa"], "usr-3": ["Thanks - I appreciate the internal feature request. I know about the CloudFront option, but it appears that would significantly increase the cost - unless there is something I'm missing?"], "usr-4": ["You aren't missing anything. It adds the cost of CF (https://aws.amazon.com/cloudfront/pricing/). This is what I use now but I rarely need to serve content anywhere but Eastern US. The real drawback for me and CF is no way to really implement 301 without hacks. I use rel=\"canonical\" in place but it isn't ideal for me."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=230628&tstart=100", "Title": "Locked out of S3 Bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "AWS allows a policy to be applied to an S3 bucket that denies all access - and ignores any explicit permits granted. Unfortunately, this also covers any access to modify the policy or delete the bucket. If this happens to a bucket, is there any known mechanism to delete the policy and regain access to the bucket - or to just delete the bucket?", "Answers": {"usr-1": ["In case anyone else runs into this problem, the solution is to delete via the CLI (i.e. \"aws s3api delete-bucket-policy\" or just \"delete-bucket\") using root's access keys. It won't work via the GUI, even logged in as root, and won't work using the CLI when logged in as any other user even if the user has full admin rights."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=230965&tstart=100", "Title": "Unreasonable Glacier Restore Fee!", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have less than 2 GB of data stored on S3. Actually, totally 4 files. As their normal lifecycle process most of these files are currently on Glacier storage. Yesterday, as a test, I tried to restore and then download my smallest file from Glacier (which is only 161.9 MB). Today I see a cost of 36 cents on my current billing report, with the largest item as: Amazon Simple Storage Service USW2-Peak-Restore-Bytes-Delta $0.010 per GB - Glacier Restore Fee (USW2) 29.055 GB $0.29 What is wrong?", "Answers": {"usr-1": ["Hello, Regarding your question, I have reached out to AWS CS team with your question. They will reply back to you via outbound case."], "usr-2": ["This is my final assessment to this question after consulting AWS support: The closest answer to my question seems to be the explanation given in the http://aws.amazon.com/glacier/faqs/#How_will_I_be_charged_when_retrieving_large_amounts_of_data_from_Amazon_Glacier According to the explanation given there, the calculation (in my case) is something like this: Data stored in Glacier: 1.22 GB Data restored from Glacier: 0.16 GB Data that can be retrieved for free: 1.22 x 5% / 31 = 0.00197 GB (negligible in my case) Assuming retrieval completed in 4 hours: Retrieval rate: 0.16 GB / 4 h = 0.04 GB/h Peak billable retrieval rate: 0.04 GB/h - 0.00197 GB / 4 h = 0.0395 GB/h Amount to be paid: 0.0395 GB/h x $0.01 /GB x 744 h = $0.29 Admittedly, a very strange and complicated calculation. For a simple approximation I will use the following: Data restored (GB): s Time it took (hours): t Amount to be paid (cents): s / t x 720 In my example: 0.16 / 4 x 720 = 29 cents This explains why it costed 180 times of what I was expecting."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231001&tstart=100", "Title": "S3 not serving CSS. Metadata is text/CSS.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Pretty simple, trying to get a static site going on S3. Seems to be working except getting 404 on CSS. Its basic template right now. Just tossed it up mostly to test setting up S3. The Metadata is key: content type | value: text CSS. Permissions look good. http://ratecomp.pro.s3-website-us-west-2.amazonaws.com/ stumped...", "Answers": {"usr-1": ["Figured it out. bad theme."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231088&tstart=100", "Title": "Lost access to the S3 Bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "... related to Support Case ID1738266261 Hi, During our development we by mistake applied following deny/- policy to a Bucket. So now we lost all access to it. Can somebody somehow remove policy please to restore access ? If that is not possible we just wanted to request Bucket deletion to do some 'housekeeping' as Bucket is in some invalid state now? Thank you, Illya { 'Id' : 'Policy1462459238980' , 'Version' : '2012-10-17' , 'Statement' : [ { 'Sid' : 'Stmt1462459218753' , 'Action' : 's3:-' , 'Effect' : 'Deny' , 'Resource' : 'arn:aws:s3:::epamigma' , 'Principal' : '-' } ] }", "Answers": {"usr-1": ["Root account can remove the policy"], "usr-2": ["Hi, Only the root account holder can remove the bucket policy in this case. If the root is unable to delete via console, he can use the below command from the AWS CLI with the root credentials aws s3api delete-bucket-policy --bucket your_bucket_name Regards, Aishwarya"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=232244&tstart=75", "Title": "403: Forbidden when using version AWS iOS SDK 2.4.2", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi everyone, It works fine with AWS iOS SDK version 2.4.0, only get 403 when using 2.4.2 (for IPv6). I use Cognito Pool ID in Tokyo, S3 bucket in Singapore. Already tried create S3 Bucket in Tokyo to test but have the same problem. Here is the code: func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -> Bool { let credentialsProvider = AWSCognitoCredentialsProvider(regionType:.APNortheast1, identityPoolId: AppConfig.cognitoPoolID) let configuration = AWSServiceConfiguration(region:.APSoutheast1, credentialsProvider:credentialsProvider) AWSServiceManager.defaultServiceManager().defaultServiceConfiguration = configuration return true } \u00a0 \u00a0 func upload(dataToUpload: NSData) { \u00a0 let transferUtility = AWSS3TransferUtility.defaultS3TransferUtility() let uploadExpression = AWSS3TransferUtilityUploadExpression() uploadExpression.progressBlock = { (task: AWSS3TransferUtilityTask, currentProgress: NSProgress) -> Void in print(currentProgress.fractionCompleted) } transferUtility.uploadData(dataToUpload, bucket: AppConfig.s3bucketName, key: 'temp/' + file.fileName, contentType: 'image/jpeg' , expression: uploadExpression, completionHander: { (uploadTask, error) -> Void in print( 'error' , error) print( 'uploadTask.request' , uploadTask.request) print( 'uploadTask.response' , uploadTask.response) } ) } Here is the response header: { status code: 403, headers { Connection = close; 'Content-Type' = 'application/xml' ; Date = 'Tue, 24 May 2016 15:15:24 GMT' ; Server = AmazonS3; 'Transfer-Encoding' = Identity; 'x-amz-id-2' = 'a8FZ4jEh11pXg0Wmg/Fbz1RdOYDu2px139/IeFx51zl+W/AzBjXK942jBj5Bqo5RbskTp6idLUU=' ; 'x-amz-request-id' = 54D217A6616C0188; } } Bucket policy { 'Sid' : 'Stmt1464065918500' , 'Effect' : 'Allow' , 'Principal' : '-' , 'Action' : [ 's3:GetObject' , 's3:PutObject' ], 'Resource' : 'arn:aws:s3:::bucket_name/temp/-' } Edited by: ngothanhtai on May 24, 2016 9:48 AM Edited by: ngothanhtai on May 24, 2016 9:48 AM", "Answers": {"usr-1": ["Here is the answer: https://github.com/aws/aws-sdk-ios/pull/389"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231979&tstart=75", "Title": "s3 lifecycle rules not emptying folder with over 100,000 files", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have a lifecycle rule set for an entire bucket. it doesn't appear to be clearing out the bucket (set to delete older than 1 day). Files are months old btw. Its been set since last night with no change region: us-west-2 Edited by: jeffreysteinmetz on May 20, 2016 2:41 PM", "Answers": {"usr-1": ["Hello, I would like to assist you. Please share your bucket name with me via a private message. Thanks and Regards, Abhilasha"], "usr-2": ["looks like it finally emptied after a few days, but now I have another bucket with the same story. Will direct message you with bucket name. Edited by: jeffreysteinmetz on May 24, 2016 1:03 PM"], "usr-3": ["Part of the challenge is quickly finding the cases where lifecycles were missed or are missing files. I solved similar storage tracking issues by watching the daily trend of standard storage and versions per bucket and path prefix. The webservice i have used is Insight4Storage on AWS marketplace, it crawls storage and saves total size by path/prefix , bucket and storage class in elasticsearch, and has trend charts by storage class, bucket, and total size by path prefix."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233308&tstart=75", "Title": "CNAME for subdomain not working", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "My questions are as follows: Why doesn't my CNAME work and Do I need Route 53 or is it making things worse? I am trying to get the subdomain studentaccess.cincinnatisymphony.org to resolve to this bucket: http://studentaccess.cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I already had issues with my current domain registrar. I had to have them manually insert the full AWS S3 address into the Name Record because it was too long for their online form. Because of this, they made the mistake of leaving off the studentaccess part from that AWS URL as the host. So Network Solutions has the CNAME record as such: Alias: studentaccess.cincinnatisymphony.org Other Host: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com They told me they fixed it and I'm waiting for a data cleanse for it to all go through, but in the mean time, the url they made up: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I was able to create a bucket there hoping to get some kind of content to show up. Nothing works. I also have studentaccess.cincinnatisymphony.org set up in Route 53 to point to the correct S3 bucket through an A record. This may be unnecessary, I don't know. I have other subdomains pointing to EC2 servers, but that was way too much power for what we need here. Help? Edited by: cso-snow on Jun 10, 2016 7:42 AM", "Answers": {"usr-1": ["it seems to be working for me."], "usr-2": ["It is all working for me now. I did not need Route 53 at all. My DNS provider finally spelled everything correctly. The biggest problems I had was an internal firewall issue with the subdomain not resolving in house, but working outside. Checked with a free proxy server."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231094&tstart=100", "Title": "Signature not valid signing with nodeJS.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Here's my signature: policy = { 'expiration': '2016-06-10T12:00:00.000Z', 'conditions': [ {'bucket': 'mybucket'}, , {'acl': 'public-read'}, , {'x-amz-server-side-encryption': 'AES256'}, {'x-amz-credential': 'ACCESSKEY/20160509/us-west-2/s3/aws4_request'}, {'x-amz-algorithm':'AWS4-HMAC-SHA256'}, {'x-amz-meta-uuid': '14365123651274'}, {'x-amz-date': '20160509T000000Z' } ] }; Here's my conversion: function getSignatureKey(key, dateStamp, regionName, serviceName) { var kDate = CryptoJS.HmacSHA256(dateStamp, 'AWS4' + key); var kRegion = CryptoJS.HmacSHA256(regionName, kDate); var kService = CryptoJS.HmacSHA256(serviceName, kRegion); var kSigning = CryptoJS.HmacSHA256('aws4_request', kService); return kSigning; } var base64Policy = new Buffer(JSON.stringify(policy), 'utf-8').toString('base64'); var signatureKey = getSignatureKey(MYSECRETKEY, '2016-06-10T12:00:00.000Z', 'us-west-2', 's3'); var s3Signature = CryptoJS.HmacSHA256(base64Policy, signatureKey).toString(CryptoJS.enc.Hex); Here's my form: let formData: FormData = new FormData(); let xhr: XMLHttpRequest = new XMLHttpRequest(); //Build AWS S3 Request formData.append('key', 'folder' + '/' + this.file.name); formData.append('acl', 'public-read'); formData.append('Content-Type', 'image/jpeg'); formData.append('X-Amz-Date', '20160509T000000Z'); formData.append('x-amz-server-side-encryption', 'AES256'); formData.append('x-amz-meta-uuid', '14365123651274'); formData.append('X-Amz-Algorithm', 'AWS4-HMAC-SHA256'); formData.append('X-Amz-Credential', 'ACCESSKEY/20160509/us-west-2/s3/aws4_request'); formData.append('X-Amz-Signature', s3signature); formData.append('Policy', base64policy); formData.append('file', this.file); xhr.open('POST', 'http://mybucket.s3.amazonaws.com/', true); xhr.send(formData); Am I doing anything wrong? S3 is responding that the signature is not valid-but I simply cannot see that I am doing anything wrong. Been stuck on this for days, so help would be truly appreciated.", "Answers": {"usr-1": ["Was passing in the wrong date. should be getSignatureKey(secretAccessKey, date, region, serviceName); Closing."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233344&tstart=75", "Title": "logs filling with \"access denied\" logs on a brand new S3 bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have just started trying out the S3 service. I created a single bucket (using an administrator level IAM user) and I uploaded a single image file. I then enabled logging for that bucket and when I checked the log 'folder' a little while later, I found that it was filled up with 'access denied' logs. During the day, these logs were coming in on average almost one per minute, and continued through the night though at a reduced rate. Has anyone else experienced this? Other than browsing to that image myself a couple of times, and also connecting to it through a web application (using the access keys of a restricted IAM user) there should be no one and no service trying to access that bucket. Thank you in advance for any insight.", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=232961&tstart=75", "Title": "How do I point my newly purchased AWS domain to my S3 bucket?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I just recently purchased a domain name (my-main-page.com) from amazon and need to point it to the corresponding S3 bucket. There are some instructions on set-up, like making two buckets (my-main-page.com and www.my-main-page.com), etc which i was able to follow, but the remaining process is a complete mystery to me. I need a step-by-step explanation on how to fill in the blanks (i guess) on the Route 53 page. I need a simple and effective way to make the connection to my S3 bucket. The terms associated with DNS are totally not understandable by me. Please see attached picture. Thanks for any help. ~gg", "Answers": {"usr-8": ["Thank you for your comment about \"FYI your .jpg files are really in PNG format. PNG is better, but the names have the wrong endings.\" Do you mean that i have used .jpg in place of .png? How can i tell if my jpg file is in the png format? (I just answered this question myself: I guess you right click on the picture and select \"Properties\" from the drop-down menu. Thanks for pointing this out to me, much appreciated. ~gg) Sometimes i try and save a picture in jpg even though it says png in the name field, since the picture was/is viewable in my xnview software i thought maybe it was converted to jpg but apparently not. If you could explain a little i would appreciate it. Thanks, ~gg Edited by: gary green on Jun 14, 2016 12:44 AM"], "usr-1": ["Hello Gary, thank you for posting on the AWS forums! I believe you have followed the tutorial at the following link: https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html I have checked your buckets and I can see that you have already completed the Static Website Hosting configuration, put the correct bucket policy for public access on my-main-page.com and set up requests redirection from www.my-main-page.com. According to your screenshot, you should already have the Hosted Zones in Route53, so you would just need to perform the configuration outlined in step 3.2 from the page linked above. For your convenience, below are the steps to complete the configuration as outlined in the tutorial, starting from the Route53 dashboard in your screenshot: Click on the \"Hosted zones\" link Click on \"my-main-page.com\" Click on \"Create Record Set\" Fill in the details on the \"Create Record Set panel\" as per the attached picture, replacing \"example.com\" with \"my-main-page.com\" Click on \"Create\" Repeat the same steps for the \"www.my-main-page.com\" Hosted Zone, replacing \"example.com\" with \"www.my-main-page.com\". Everything should be correctly set up at this point, and you should be able to browse to your website. I hope this information helps you! Have a great day! Andrea S."], "usr-2": ["Dear Andrea, Thanks very much for answering my question and for your help. Using the information you provided i was able to complete the process of pointing my domain name to the correct bucket. In fact it works very well: The www. redirects to the DN \"my-main-page.com\" very quickly and everything seems to work fine. I have attached a picture of the final set-up: Please note the \"www\" in the \"edit record set\" name field. I had to experiment some to get it to come out right - I don't want to be too critical because i have a tendency to not update things on my own websites as i should, and as Amazon should. So i am very happy about getting this working correctly. If you have other people needing some instruction with setting up their own Route 53 with the same problems i have had please feel free to use my pictures for reference. Sincerely, gary green Edited by: gary green on Jun 10, 2016 9:37 PM"], "usr-3": ["FYI your .jpg files are really in PNG format. PNG is better, but the names have the wrong endings."], "usr-4": ["Hi Andrea. Now when i click on a link on the webpage i get this: http://my-main-page.com/shedandshelter.s3.amazonaws.com/index.html The link is not opening up into a window of it's own. Pease help correct this. Thanks, ~gg"], "usr-5": ["Hello, I checked your website and found that the URL on the page is pointing to \"http://my-main-page.com/shedandshelter.s3.amazonaws.com/index.html\". It means that the requested page exists in \"my-main-page.com\" bucket. If the page does not exist in the bucket, it will return 404 error as it is doing currently. When I separately browsed \"shedandshelter.s3.amazonaws.com/index.html\" or \"http://s3.amazonaws.com/shedandshelter/index.html\", I was able to get the page. It means that the desired page exist in a separate bucket named \"shedandshelter\". If you want to access the page at using 'my-main-page.com' domain, you should create a folder named \"shedandshelter\" in bucket 'my-main-page.com' and have \"index.html\" inside the folder \"shedandshelter\". Once you do that, you should change your URL to \"http://my-main-page.com/shedandshelter/index.html\". Thanks, Jay"], "usr-6": ["Hi Jay. (I have found the source of the problem. Now everything works great! 14 Jun 16) Thank you for your detailed response. The solution you outlined will not work for my situation because i cannot change the URL of my many websites because people bookmark these current URL's and i am able to make some income from the websites. Also, i want to mention that my website: SHED & SHELTER (shedandshelter.s3.amazonaws.com/index.html), for example has many links to my external S3 buckets. I have never had a problem like this. I have always been able to link to my other buckets in the S3 system. Maybe what i will have to do is point the domain name: my-main-page.com - to another folder named: my-main-page and leave off the .com? That is probably what i have to do. my-main-page.com needs to point to: my-main-page.s3.amazonaws.com/index.html The above will work and if people bookmark that URL that is OK. What do you think? Thanks for your help. ~gary green Edited by: gary green on Jun 14, 2016 12:00 AM"], "usr-7": ["Thanks Jay for your response. I just found out what the problem is/was: I needed to add \"http://\" in front of the rest of the URL. After doing that everything worked fine. Sorry to bother you. ~gg"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233364&tstart=75", "Title": "Huge number of \"NoSuchKey\" errors when retrieving old files", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, Last week I created a simple application that takes a file prefix as an argument, and does the following: list 1000 keys, download all of them and delete them from S3, use the last key as the marker for the next list and so on. It ran for days without problems and processed ~184 million files so far. But this week I started seeing a large number (hundreds of thousands) of 'NoSuchKey' errors when sending GET requests to the keys received from LIST. This doesn't happen for each LIST request, and the same GET request can succeed once and fail the second time. The files have been in this S3 bucket for several months, none of them are new. Any suggestions? The only information I found about these errors is that they can happen for new files immediately after uploading, which isn't the case here.", "Answers": {"usr-1": ["Update: I re-ran the application again (same way I did multiple times today and yesterday), and this time it processed the remaining couple hundred thousands files without getting a single error. This seemed to have been either a temporary issue or a strange bug in the app."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233288&tstart=75", "Title": "2 buckets, 1 works (as a web site)", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "i create a bucket and website and it works ok. i created another but it is getting 403 Forbidden and Code: AccessDenied , etc., on all access attempts. both site are configured alike as far as i can see. when i delete error.html then the message below An Error Occurred While Attempting to Retrieve a Custom Error Document changes from Code: AccessDenied to Code: NoSuchKey ... so i know i am getting to the correct bucket. the 2nd website is a hostname under the 1st which is the domain apex name (e.g like www.example.com but a different name). both are DNS served by Route 53 . any idea what i might be missing? permissions are the same.", "Answers": {"usr-1": ["i found a typo in the policy which i should have posted for maybe faster resolution"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233374&tstart=75", "Title": "S3 as a folder in windows File Explorer", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Is there an App to put Amazon S3 as a folder in windows File Explorer, in the same way that OneDrive and Dropbox are folders?", "Answers": {"usr-1": ["Hi We have a separate product called CloudBerry Drive that helps you map cloud storage as a local disk. Check out the link below on how to get started with Amazon S3 http://www.cloudberrylab.com/blog/how-to-use-amazon-s3-with-cloudberry-drive/ Thanks Andy"], "usr-2": ["Thank you sounds good. Tell me is there any amazon cost involved? For example if there are no uploads or downloads of files in a period but windows file explorer is open all the time and regularly used would there be a cost from Amazon?"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233358&tstart=75", "Title": "different responses from different buckets - software inconsistency?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "newpost: this issue is a bug in the web console which caused me to not save the bucket policy. see: https://forums.aws.amazon.com/thread.jspa?threadID=235937andtstart=0 oldpost: i am getting different responses from different buckets as if different software was involved: ==> lt1/sr_use1 /home2/sr_use1 1> lynx -mime_header http://stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> Last-Modified: Sun, 12 Jun 2016 05:04:54 GMT ==> ETag: 'd41d8cd98f00b204e9800998ecf8427e' ==> x-amz-error-code: NoSuchKey ==> x-amz-error-message: The specified key does not exist. ==> x-amz-error-detail-Key: thisdoesnotexist.html ==> x-amz-request-id: 79DF161EA4B9D9E8 ==> x-amz-id-2: gDvVCd92l04suxYkfFUFsjzUFMy4vxN1kAEf51rDkA4XqGvSDtGHOReq70e9Z3fND/G/il7QvPI= ==> Content-Type: text/html ==> Date: Sun, 12 Jun 2016 05:13:38 GMT ==> Connection: close ==> Server: AmazonS3 ==> ==> lt1/sr_use1 /home2/sr_use1 2> lynx -mime_header http://work.stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> x-amz-request-id: 4B6656B4C73FE964 ==> x-amz-id-2: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> Content-Type: text/html; charset=utf-8 ==> Content-Length: 549 ==> Date: Sun, 12 Jun 2016 05:13:49 GMT ==> Server: AmazonS3 ==> Connection: close ==> ==> <html> ==> <head><title>404 Not Found</title></head> ==> <body> ==> <h1>404 Not Found</h1> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: thisdoesnotexist.html ==> RequestId: 4B6656B4C73FE964 ==> HostId: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> ==> <h3>An Error Occurred While Attempting to Retrieve a Custom Error Document</h3> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: error.html ==> ==> <hr/> ==> </body> ==> </html> ==> lt1/sr_use1 /home2/sr_use1 3> anyone know why? since i am getting other different behavior for the same configuration i really am suspecting software inconsistency. Edited by: stratusrelay on Jul 24, 2016 10:48 PM", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233520&tstart=75", "Title": "EntityTooSmall error, solved by deleting earlier parts from bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, We stumbled upon EntityTooSmall errors with uploads that did not seem to have problems with part sizes. By trial and error we found out that by removing earlier parts the problem went away. Can earlier parts with different UploadId affect future uploads? Or was this a malfunction in the service? --hajautettu (Some further details: 6500 parts, 863 files, approximate time range: 2016-04-28T11:16:47.000Z', '2016-05-30T20:02:08.000Z'. AWS SDK for Javascript, version available in Lambda nodejs4.3 runtime)", "Answers": {"usr-1": ["Deleting parts did not seem to be the solution. Some files still refuse to upload. Will continue to diagnose this."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234026&tstart=75", "Title": "Endless loop when listing objects in bucket using NextContinuationToken", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I encountered a strange issue that seems to relate to special-chars being used in object-keys. I am listing objects the way that it is explained at https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingJava.html . The bucket contains 40.000+ objects, some with special chars in their name, e.g. 'Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac'. I noticed, that, although the continuation-token changes, the keys that I get back from AWS were repeated sometimes and I was stuck in an endless loop (truncated == true but the same keys are returned over and over again). I then decreased the maxObjectSize to 1 to try to find the object-key where the looping starts. This was an object-key that contained a special-char: 'Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac'. So the trigger to start this looping-behavior seems to be, that the last object in a fetched batch contains special-chars which seems like a serious bug to me. Loop output: Alabama Shakes_Gimme All Your Love.flac Alexandrina_Flori De Spin.flac Alexandrina_La Ville Ombres.flac Alexandrina_Muguri Florali.flac Alexandrina_Noaptea.flac Alexandrina_Orasul Umbre.flac Alexandrina_Pijamale Reci.flac Alexandrina_Spre Sud.flac Alexandrina_Tu Cu Soarele.flac Alexandrina_Turturica.flac Alogte Oho Jonas And His Sounds Of Joy_Zota Yinne.flac AlunaGeorge_I'm In Control (The Magician Remix).flac Amason_California Dreamin'.flac Amason_I Want To Know What Love Is.flac Anchorsong_Butterflies.flac Anchorsong_Ceremony.flac Anchorsong_Expo.flac Anchorsong_Kajo.flac Anchorsong_Last Feast.flac Anchorsong_Oriental Suite.flac Anchorsong_Rendezvous.flac Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Thanks! Clemens", "Answers": {"usr-1": ["Hi Clemens, Thanks for bringing this to our attention. We are currently investigating this and we will post an update soon. Thanks, Vik."], "usr-2": ["Hi Clemens, This issue is now resolved, let us know if you still face any problems. Thanks again for reporting and contributing to the forums. Best, Vik."], "usr-3": ["We've got the same problem with one of our customser! There's an endless loop that is split across 10 responses of ListBucketResult (1000 objects in each). There're about 2 million objects in total. The last object returned in the 10-th subsequent list response is the marker that was used for the 1-st request. The problem discovered for 2 buckets of the account. The problem is reproducible with very high rate. The buckets are in eu-west-1 region,and EC2 instance is in us-east-1b region. The bucket name, account id and the responses are not for public, but I can provide it to AWS support contact. Here're the response headers returned by AWS to subsequent bucket listing requests. Appropriate GET request contains sensitive information and can be provided to someone from AWS team if needed. 1. x-amz-id-2: rkO5vBQ789qsn4hIfX6KSjONtkr+a/sZxJIixx4ukpkhlAg54Zf8I2cWfOBlQB4/xKaFkJ/iI80= x-amz-request-id: 461B1DACBBF7BD30 \u00a0 2. x-amz-id-2: 8IOZfjuU+08qqlGvzhRn7Le1nOKwUMmRImFzMD4cuji/NDkCWWOyA91tBnyaI8s9qHkV4dAqTC8= x-amz-request-id: C6BCE9367D02763A \u00a0 3. x-amz-id-2: YCmb+KqRgNKe2ZRm8Mdp7gDgBD0c0NGA6EJ98GUKnHyTPcZ4zE6rQgrt+m4yvFxhGi43sUQEzQs= x-amz-request-id: 46DEFC7BC2E162B3 \u00a0 4. x-amz-id-2: dkXNlWz8So41qh9y/rs4K84M+L0Lyb953Mhohch3CSb7TGs+VOGTzqRH+b6TbbJnduEBt7TeeIo= x-amz-request-id: 1A2A09356474340A \u00a0 5. x-amz-id-2: +pVvGbA6k1LDJbqS9mT6KZtvX0fxhujxenMEom8gRs0eT6hUYpey9zZCADDx5WQfEa4P2DBhz3Y= x-amz-request-id: 0D3479695E4C9DD8 \u00a0 6. x-amz-id-2: pVvoeKSSAg+ytcJG1flXvDrQHqs9pGOglPlZPMIs+pW0ggmcbLCMIBLYK1X4qIBjHea+/3Oyaeo= x-amz-request-id: 3D6C22F4FD749CC6 \u00a0 7. x-amz-id-2: oFZ+kQG1fx+lyXkmrWSrr4ED5UFOGwodsJH2mgG+Wj3MQzQwRBVwuODr+xqCnJ3BhDRZeqiH9TQ= x-amz-request-id: 279AEEAF1C298D12 \u00a0 8. x-amz-id-2: uFuukh36N4DPc0Yp/4z/mfsoc5vulavg8WzhLjDuG4iVZAJJvw6FZtIE9H2i5RlUcAMe5lCtEg8= x-amz-request-id: D257A9EC48C4DCA4 \u00a0 9. x-amz-id-2: +axYUYEtx4czwHhnALTUG+CvbqeqonOE/7P1irvJs7n8NuAaQMaiNqG93s82HtB3rsTDz4qnc68= x-amz-request-id: 434B6D164840B28D \u00a0 10. !!! START LOOP. The Marker is the same as in the first request.!!! x-amz-id-2: uhJar8UMTeGjFASAFBVJgJ83ZoCWF1K0fOfdt3V10rke8Y5dN2Lhpv1nN87cqt0JuR5iwYSTf6k= x-amz-request-id: 32E28E5E47D8C8BB \u00a0 11. !!! CONTINUE LOOP !!! x-amz-id-2: kTw23GENbj2+9ThJm8HdFZUVbtzMkFYuG/hO5X2yHNWIgghQNwAKpSfeAukNH/1XNnDCzUrEbB0= x-amz-request-id: 85DF9AD7D5F1E3C2 \u00a0 ... In this case the repeated marker appeared exactly at ListBucketResult boarder. However it can appear somewhere in the middle that makes detecting the issue very hard. Sincerely, IP"], "usr-4": ["Figured out the issue. It was MS .Net framework that removes some unprintable control characters from an URI (particularly, \\u200f) when sending a request"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234105&tstart=75", "Title": "s3.amazonaws.com got blocked in Russia", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello! Yesterday our goverment blocked s3.amazonaws.com url. It still remains blocked by some ISP in Russia. We use s3 for storing files in our projects. We use browser upload with CORS and direct download with presigned URLs. Upload is not blocked since we can use endpoint -.s3-eu-west-1.amazonaws.com. But presigned URLs are formed like bucket-name.s3.amazonaws.com/... so it is partially blocked in Russia. Can some advise how to cope with this? Is there a way work with S3, but not so use bucket-name.s3.amazonaws.com address? Best regards, Nemtsov Georgiy Edited by: gnemtsov on Jun 22, 2016 11:59 PM", "Answers": {"usr-1": ["Hello Georgiy, thanks for bring this into our atention. I'm sorry to hear you are in this kind of trouble. I have already requested the opinion of an S3 Engineer, to see if there is possible to workaround this issue and allow you to use our service. We will let you know as soon as I got any new on this Rgrds Javier C#"], "usr-2": ["Hello Georgiy, S3 has a list of all regional endpoints at https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region that you can use. You can also create presigned URLs using other endpoints (presigned url feature is not limited to the s3.amazonaws.com endpoint). Please refer to the following link for additional information: http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html Best Regards, Pavithra K Edited by: Pavithra@AWS on Jun 23, 2016 10:56 AM"], "usr-3": ["Thanks for information. Today Amazon S3 was unblocked in Russia, because Amazon fulfilled our goverments' request to block some prohibited content on S3. Hope this won't repeat in our country."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234509&tstart=75", "Title": "Problem Setting Up a Static Website Using a Custom Domain", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I followed the steps shown on this walkthrough essentially word-for-word to create a website that can be accessed at http://mattchen.co . I bought the domain name through Route 53, which I'm also using to handle the DNS aspect of the website hosting. The problem: If I access http://www.mattchen.co , then I am not automatically redirected to http://mattchen.co (although according to the article, I should be.) What did I do wrong?", "Answers": {"usr-1": ["You are missing a DNS record for www as discussed under \"Step 3.2: Add Alias Records for example.com and www.example.com\"."], "usr-2": ["Hi D. Svanlund, I thought I had followed that step correctly\u2014in fact, I even undid it and then did it again just to be sure. I've attached a screenshot which might provide more detail. Can you clarify why pursuing this step did not seem to be successful? Thanks, M"], "usr-3": ["The screenshot shows that you have a separate Route 53 hosted zone for the www domain. That's not necessary (and it won't work out of the box). Instead, create a new record set (type A) for www inside your example.com hosted zone."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234434&tstart=75", "Title": "s3 rule to archive to glacier help", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am trying to archive old logs to glacier from an S3 bucket and it's simply not working. Basically I have an s3 bucket admin.logs in which there is a folder api. (this is phase 1). So in that bucket I create a life-cycle rule. Step1 Apply the Rule to: I select A prefix api/ Step2 Configure Rule: the 2nd box Archive to the Glacier Storage Class 30 days after the objects creation date is setup. Step 3 is just the info, re-confirming I see; This rule will apply to Objects with the prefix: api/ in the admin.logs bucket Archive to the Glacier Storage Class 30 days after the object's creation date. Now, I still see things from 2+ months ago in the S3 folder. Also, I don't see a place where you set the glacier vault info. I created a vault for this and see no data. So, I'm not sure if this is right OR do I really need to bash script some aws s3 ls | find mtime +30, etc. Thanks for all read/replies.", "Answers": {"usr-1": ["If you set up the rule very recently, give it a couple of days since the transition isn't immediate. In the S3 Management Console, files that have been moved to Glacier storage will have the \"Storage Class\" listed as \"Glacier\". Note that you will still see the files in S3. They are technically stored in Glacier if they have a storage class saying so, but all files are still managed through the S3 web interface or client as usual. Even though they are in Glacier, you won't see them through the Glacier service when you use the S3/Glacier integration."], "usr-2": ["Ah, funny I didn't see that \"storage class\" column, I just expected it to just disappear from the S3 bucket and appear in a Glacier vault. Then when I went to check, I realized I never gave a vault name, and then wondered even more. So, I guess the reference is still there with the same glacier rules/rates, but I have the luxury of using the S3 CLI commands if I ever need to copy back (knowing there will be a delay), but just surprised I didn't see that class column (sigh). Thanks for the simple and correct answer."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234588&tstart=75", "Title": "Can't match S3 POST example (in Elixir)", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm trying to work through the S3 signing example . But I'm not getting the interim answers in their docs. I started with: policy = \\% { 'conditions' => [ \\% { 'bucket' => 'sigv4examplebucket' } , [ 'starts-with' , '$key' , 'user/user1/' ], \\% { 'acl' => 'public-read' } , \\% { 'success_action_redirect' => 'http=>//sigv4examplebucket.s3.amazonaws.com/successful_upload.html' } , [ 'starts-with' , '$Content-Type' , 'image/' ], \\% { 'x-amz-meta-uuid' => '14365123651274' } , \\% { 'x-amz-server-side-encryption' => 'AES256' } , [ 'starts-with' , '$x-amz-meta-tag' , '' ], \\% { 'x-amz-credential' => 'AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request' } , \\% { 'x-amz-algorithm' => 'AWS4-HMAC-SHA256' } , \\% { 'x-amz-date' => '20151229T000000Z' } ] } \u00a0 stringToSign = policy |> Poison.encode! |> Base.encode64 But the `stringToSign` comes out rather shorter than the Amazon example. For the signature I have these helpers: @secret_key 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' def signing_key2(secret, date, region, service) do hash_sha256( 'AWS4' \\<\\> secret, date) |> hash_sha256(region) |> hash_sha256(service) |> hash_sha256( 'aws4_request' ) end \u00a0 def hash_sha256(secret, msg) do hash_sha256_bis(secret, msg) |> Base.url_encode64 end \u00a0 def hash_sha256_bis(secret, msg) do :crypto.hmac(:sha256, secret, msg) end and ran signing_key2(@secret_key, '20151229' , 'us-east-1' , 's3' ) but this came out shorter than the test answer. Would welcome some pointers. Edited by: hotbelgo on Jul 3, 2016 1:23 AM Edited by: hotbelgo on Jul 3, 2016 1:25 AM Edited by: hotbelgo on Jul 3, 2016 1:26 AM", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233864&tstart=75", "Title": "aws s3 sync how many put/get requests are that", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "let say i sync a directory whit 1000 files to s3 the first time (so no file on s3 yet) how many put requests are that ? 1 or 1000 if all files are on s3 how many then ? does the command GET each file one by one checks the state and then and PUT them on if needed mean could doing a sync on large directorys end up with tens of thousand of put and get requests ?", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235006&tstart=75", "Title": "response-content-disposition parameter is not working in S3 GET request", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "The response-content-disposition parameter supported by the GET request ( http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html ) is no longer working. It ignores the value specified in the parameter and just returns the file with the content-disposition information in the stored resource. For example, the following request should cause the file to be returned with a content disposition of: inline;filename='foobar.pdf';filename-=UTF-8''foobar.pdf GET https://s3.amazonaws.com/ <bucket>/<docID>?response-content-disposition=inline\\%3Bfilename\\%3D\\%22foobar.pdf\\%22\\%3Bfilename-\\%3DUTF-8\\%27\\%27foobar.pdfandSignature=<signature>andExpires=<expires>andAWSAccessKeyId=<key> See RFC 5987 ( https://tools.ietf.org/html/rfc5987 ) for an explanation of the content-disposition format, though I don't think that matters for this bug. When I use this parameter, the filename comes back with the original content disposition stored with the file, which in my case causes the file to be downloaded. This was working earlier this year and perhaps as recently as a few weeks ago - I have no way to verify. Has something changed? Edited by: BenGilbert on Jul 8, 2016 5:02 PM", "Answers": {"usr-1": ["I am also experiencing this issue. Has anyone found a workaround? For now I am manually setting the ContentDisposition by copying the object into itself with the new ContentDisposition in the metadata. I also had to set MetadataDirective=true. It works, but... meh. Edited by: lucashansen on Jul 10, 2016 4:31 PM"], "usr-2": ["We're also experiencing the same thing but it's an intermittent problem for us. We have a test in place which checks the Content-Disposition header has the expected value from a signed S3 URL with the response-content-disposition specified. The test is failing about 50% of the time (with no code changes in between runs). I have an example independent of our application. If you run the following curl command repeatedly then sometimes the content disposition is as expected (inline; filename=\"filename.txt\" ...) and the sometimes it simply returns the content disposition from the object metadata (attachment; filename=\"file\"; ...) curl -s -D - 'https://kev-test.s3.amazonaws.com/449/UffU47CEyYekTRVaG1V0BjlaGjPP8ZEo?response-content-disposition=inline%3B%20filename%3D%22filename.txt%22%3B%20filename%2A%3DUTF-8%27%27filename%252Etxt&Signature=NnqjRWGRHuuZV8m2KuFdHRLrGA0%3D&AWSAccessKeyId=&ltAccess Key Removed&gt&Expires=1483228800&response-content-type=image/gif' -o /dev/null"], "usr-3": ["Thank you for the report. S3 is aware of this issue and we are working to resolve it."], "usr-4": ["Hello, Our service team has finished the deployment of a fix. In case you still encounter any issues, let us know the requestid and x-amz-id2s for some failed requests and we'll look into it. Eduardo C"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235254&tstart=75", "Title": "CloudWatch metrics not visible for all S3 buckets", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have 4 S3 buckets. They are all 'standard' type. In the list of available metrics in CloudWatch, I only see metrics for 1 bucket. I thought maybe the metrics need to be enabled on the bucket in the S3 console, but I can't find any settings related to CloudWatch. What am I missing?", "Answers": {"usr-1": ["Are any of your S3 buckets with \"missing\" CloudWatch metrics empty? I don't believe that CloudWatch generates NumberOfObjects and BucketSizeBytes metrics for empty buckets..."], "usr-2": ["Good idea, but no, they're not empty."], "usr-3": ["I figured it out. S3 shows \"Global\" as the region, and when the region selector is clicked, the message \"S3 does not require a region\". However each bucket can be associated with a different region. The 3 buckets that were missing in CloudWatch were in a different region, and I had to switch to that region in CloudWatch to see their metrics."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235265&tstart=75", "Title": "intermittent access problems", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I've had a support request for this open for five days, but not getting much joy (nothing besides 'looking into it'). downloading objects from s3.amazonaws.com/bucket/key is behaving very oddly - sometimes it works. sometimes it fails (403) for the same object. Just using wget/curl here - nothing fancy. The DNS is being resolved to the same IP address. We have recently changed the bucket policy here, however there are only allow rules (no deny) and even then, the indeterministic nature of the problem seems highly unusual. Just wondering if anyone has seen this before.", "Answers": {"usr-1": ["Hi, We are currently investigating and our team will follow up as soon as we have something. Thanks."], "usr-2": ["To wrap this up, eventually (took 13 days!) support came back - there were two issues: a) I took the example from http://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html - but it's a user policy (not a bucket policy) and the ${aws.username} caused the policy to fail evaluation (http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html says that anonymous = not present = means that the value is not in the current request information, and any attempt to match it fails and causes the request to be denied.) b) There was an issue with S3 where because of the fail above (but also an object ACL that said \"pass\"), S3 would occasionally return \"pass\". This has now been fixed so that it always returns fail."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235287&tstart=75", "Title": "Costs for access in different Regions", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am working with other vendors that deliver content to our S3 account. Their system automatically creates the buckets to store the images delivered. The Buckets they have created are in Virginia (us-east-1). All of our services and other buckets are in Oregon (us-west-2). What costs could I expect for users accessing images in a different region? Web site visitors will be provided a url to the image i.e. https://s3.amazonaws.com/glvar-a/IMG-58412203_1.jpg I have PHP processes that will have to connect cross region to search for files containing specific strings in the file names. Is there an additional cost to poll a bucket in a different region? The result should contain information for 50 files or less. NOTE: We are not transferring the files between regions, just obtaining file information. Thank you! Steven", "Answers": {"usr-1": ["You can find S3's pricing here: https://aws.amazon.com/s3/pricing/ Pricing for requests to each region varies depending on the region being accessed and the request being submitted. This includes differences in the cost for PUT/COPY/POST/LIST in addition GET or other requests. Additionally there is a data transfer pricing to also consider for data transferred out of each region to the internet (or to EC2 instances in a different region) and that cost also varies by region."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235364&tstart=75", "Title": "S3 lifecycle event notifications", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, Is there a way to get event notifications (i.e. to SQS) for objects that transition to Glacier? I saw in another forum post that with bucket logging enabled, there are S3.EXPIRE.OBJECT and S3.TRANSITION.OBJECT log entries, but what I really need is an SQS notification on these actions. Thanks, Joe", "Answers": {"usr-1": ["I opened a support case and was informed that this feature does not yet exist, but that a feature request is being submitted: \"Unfortunately, Expiration and Transition of objects from lifecycle rules do not trigger event notifications. \"You will not receive event notifications from automatic deletes from lifecycle policies or from failed operations.\" http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#notification-how-to-event-types-and-destinations However, as mentioned, I will be putting in a feature request to have this added to S3 on your behalf. I can't speak to when or if this feature will be added, but I suggest following our S3 blog for news about our latest releases: S3 Blog: https://aws.amazon.com/blogs/aws/category/amazon-s3/ \""]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235868&tstart=75", "Title": "Why does AWS add \\r\\n when recreating pre-sgined URL policies?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm struggling to emulate the test signatures at http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html . I can eventually get these answers but only by very careful attention to \\r\\n sequences in my test examples (e.g. note the double line break in the amazon example). I can't hope to get an exact amount of line spaces when I start to work with Elixir data structures and their json encoders, and so far at least I'm getting a series of SignatureDoesNotMatch errors (which are of course hard to debug, see also see http://stackoverflow.com/questions/38525846/s3-pre-signed-upload-signaturedoesnotmatch ). Edited by: hotbelgo on Jul 23, 2016 1:26 AM", "Answers": {"usr-1": ["Turns out that the \\r\\n are only added when there in your html, as there is the AWS example code. Without that, everything behaves more normally."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236125&tstart=75", "Title": "Import/Export PendingCustomer action required", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hey, since 2 weeks my import job is pending and I'm waiting for AWS to complete the job but as of today my status changed to: JOB FOUND JobId: EU-E2KUH CreationDate: Fri Jul 08 10:26:10 CEST 2016 JobType: Import LocationCode: AtAWS LocationMessage: Your device is at AWS. ProgressCode: PendingCustomer ProgressMessage: The specified job requires action from customr's end. ErrorCount: 0 LogBucket: null LogKey: null Carrier: null TrackingNumber: null I didn't get any notifications so I'm wondering what the next steps are? Which actions do I have to perform? thx", "Answers": {"usr-1": ["Hi ta2002. We have reached out to you for some information. If you have any questions or have not received our reach out email, please PM me and I can help you with this. Frank"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223688&tstart=125", "Title": "Does S3 capable to upload file via torrent?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have seen S3 can be use as a seed to download torrent file. But does S3 itself could try to get file by torrent?", "Answers": {"usr-1": ["Hello, Unfortunately this is not possible at this time. The complete torrent capabilities of S3 can be found here: http://docs.aws.amazon.com/AmazonS3/latest/dev/S3Torrent.html Kind regards, Chris"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223732&tstart=125", "Title": "Unable to delete empty bucket created by Elastic Beanstalk", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Removed a beanstalk but the bucket created by the beanstalk was not removed: elasticbeanstalk-us-east-1-868840956234 I've tried removing it through the management console as well as the AWS CLI. CLI shows the following: srvdsks-MacBook-Pro:server-app $ aws s3 rb s3://elasticbeanstalk-us-east-1-868840956234 --force remove_bucket failed: s3://elasticbeanstalk-us-east-1-868840956234/ A client error (AccessDenied) occurred when calling the DeleteBucket operation: Access Denied Attached a screen shot of the error from the console, as well. If possible then please just remove this bucket. Barring that, let me know what I can do to kill it or what other info is needed. Thanks!", "Answers": {"usr-1": ["Eric, Sorry you are having an issue deleting this bucket. I have examined this bucket and see that the bucket policy is preventing the deletion. This is the section of the bucket policy that is preventing the deletion: { \"Sid\": \"eb-58950a8c-feb6-11e2-89e0-0800277d041b\", \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:DeleteBucket\", \"Resource\": \"arn:aws:s3:::elasticbeanstalk-us-east-1-868840956234\" } ] } If you edit the bucket policy and remove this section, you will be able to delete the bucket."], "usr-2": ["This was basically it. The deeper issue was caused by the absence of the principal that created the bucket--in this case, an IAM user that no longer existed. I also had to modify the policy to reflect an existing principal but it would also have sufficed to completely remove the security policy. Not really anything anyone can be but its kind of frustrating to periodically find these unresolved dependencies Thanks, Lee!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223773&tstart=125", "Title": "For some files, object isn't accessible at your-bucket-name.s3.amazon.com", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "For some files in my bucket, the object isn't acessible via url at the url style: http://your-bucket-name.s3.amazonaws.com and is only accessible at : http://s3.amazonaws.com/your-bucket-name However, for other objects in the bucket, the http://s3.amazonaws.com/your-bucket-name works fine as the url to display the object. Why might this be? Images are breaking on my site. I believe we use the s3 api to get the url path for an object key, and it is returning the http://your-bucket-name.s3.amazonaws.com format.", "Answers": {"usr-1": ["Never mind, I believe I messed up. Need to do more research."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223829&tstart=125", "Title": "S3 Event Triggers Multiple Times", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm using Kinesis Firehose to write logs from Cloudwatch into S3. The log file gets written just once to a bucket and only has a single version. The 'ObjectCreated:-' event triggers a lambda function that parses the zip file content. However, the lambda function is being triggered 3 times, I can see each independent execution of the lambda function on the same log file in the logs. The function executes once when I would expect it, then another time about a minute later, and then again 2 to 3 minutes after that. Are these events firing as the object is being committed to other AZs? Is there something that I might have misconfigured, or is there some way to stop the additional triggers on the same event?", "Answers": {"usr-1": ["Hello, Can you send me some of the bucket and key combinations that triggered the duplicate Lambda invocations? In the meantime, you may want to investigate your function logs to ensure that your function is terminating successfully. Lambda will retry function executions that appear to error or time out. Thanks, Carl"], "usr-2": ["Carl, I think it was the Lambda function retry I was seeing. I forgot a context.done(), so all the way through the logs it looked like everything was fine, but I wasn't paying attention to the end: END RequestId: 9cea8b01-c127-11e5-816c-d93d28c80b86 REPORT RequestId: 9cea8b01-c127-11e5-816c-d93d28c80b86 Duration: 334.83 ms Billed Duration: 400 ms Memory Size: 512 MB Max Memory Used: 36 MB Process exited before completing request Thanks for the suggestion pointing me in the right direction."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223945&tstart=125", "Title": "AWS CLI S3 upload intermittent error", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I have a backup procedure that uploads several files at night. However the last 2 days, some files started to fail with error: upload failed: D:\\file.bak to s3: //my-bucket/backups/SQLServer/file.bak Could not connect to the endpoint URL: 'https://my-bucket.s3.amazonaws.com/backups/SQLServer/file.bak?partNumber=82anduploadId=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' The command I'm executing is something like this: 'C:\\Program Files\\Amazon\\AWSCLI\\aws.exe' s3 cp 'D:\\file.bak' 's3://' my-bucket/backups/SQLServer/ ' --sse Since it's intermittent, I wonder if the S3 service had any problems in the past 2 days, or if I am doing something wrong (maybe I need to update something?) CLI version: aws-cli/1.9.12 Python/2.7.9 Windows/2008ServerR2 botocore/1.3.12 Any ideas? Thank you", "Answers": {"usr-1": ["FYI, for anyone reading this. I ended up implementing a retry method so I make sure the file gets uploaded. The underline is probably a connectivity issue that can happen anyway."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236380&tstart=75", "Title": "web hosting isn't serving specified index/error pages", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Well, I don't know what was wrong, but I was able to fix it. I moved my DNS zone over to Route53 (I had it at my registrar, where I'd put it while migrating things). Now that the FQDN is actually an alias to the AWS bucket directly (instead of cname shenanigans) it Just Works. I did have to wipe out my redirect config as that was causing a loop, but the bucket policy, index and error pages, etc seems to be working now. Original issue follows, so as to help anyone else who comes along this same path. So, I've no idea what I've done wrong, so I'll include what I've got set up currently with a minimum of redaction, so that you can see for yourself how it's behaving. Theoretically I have my account set up such that there's no harm in the public seeing any of this (MFA etc). I've swapped out part of the URL (look for 'EXAMPLE.COM, replace with draeath.net) to stop indexer bots and the casually malicious, so just swap that text if you'd like to have a peek yourself. TL;DR: my specified index and error pages are not used, yet you are able to directly request the pages I've specified (or indeed any object, as is my intent). Clearly, I've done something wrong or massively misunderstood how this is supposed to function? Any help you might offer would be helpful! Again, replace any occurrences of 'EXAMPLE.COM' in the below with 'draeath.net' Bucket name in question: video-archive.EXAMPLE.COM URL that I'm using: http://video-archive.EXAMPLE.COM CNAME configuration for the above: 'video-archive IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com.' DNS resolution test looks OK to me: $ dig video-archive.EXAMPLE.COM CNAME | grep -A1 'ANSWER SECTION' ;; ANSWER SECTION: video-archive.EXAMPLE.COM. 1798 IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com. Bucket policy: { 'Version' : '2012-10-17' , 'Id' : 'Policy1469943355310' , 'Statement' : [ { 'Sid' : 'Stmt1469943336200' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : 'MyPrincipalWasHereButSnippedForObviousReasons' } , 'Action' : 's3:-' , 'Resource' : 'arn:aws:s3:::video-archive.EXAMPLE.COM/-' } , { 'Sid' : 'Stmt1469943353219' , 'Effect' : 'Allow' , 'Principal' : '-' , 'Action' : 's3:GetObject' , 'Resource' : 'arn:aws:s3:::video-archive.EXAMPLE.COM/-' } ] } CORS config: (not that I'm using any scripting) <?xml version= '1.0' encoding= 'UTF-8' ?> <CORSConfiguration xmlns= 'http://s3.amazonaws.com/doc/2006-03-01/' > <CORSRule> <AllowedOrigin>-</AllowedOrigin> <AllowedMethod>GET</AllowedMethod> <AllowedMethod>HEAD</AllowedMethod> <MaxAgeSeconds>3000</MaxAgeSeconds> <AllowedHeader>Authorization</AllowedHeader> </CORSRule> </CORSConfiguration> I've also added custom redirect rules after trying things as above, but this seems to have done exactly nothing: <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/index.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>401</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>403</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>405</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>406</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> </RoutingRules> I have 'enable website hosting' enabled on the bucket, with 'index.html' for index page and 'error.html' for the error page. Both of these objects exist in the root of the bucket with those names exactly - they are tiny HTML documents with appropriate Content-Type metadata. They are accessible when directly accessed, eg like so: index.html error.html Yet, finally, if you try to access the root of the bucket you get the standard AccessDenied XML back. If you try to access a folder in the bucket without a trailing slash, you the same, and if you include the slash you get a 0-byte octet-stream. Edited by: draeath on Jul 31, 2016 9:44 AM - adding routing config XML Edited by: draeath on Jul 31, 2016 10:36 AM - solved my issue", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=223981&tstart=125", "Title": "S3 new bucket error", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I am attempting to create some new buckets (victorlevine.net, www.victorlevine.net, logs.victorlevine.net) with the region 'US Standard', however I am experiencing an error - 'A conflicting, conditional operating is currently in progress against this resource.' I waited a 1/2 hour and still experienced same error. Any help with this issue would be greatly appreciated,", "Answers": {"usr-1": ["Resolved itself"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=224015&tstart=125", "Title": "Folder references stored as STANDARD, not STANDARD IA", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "OK, I'm just guessing it's the folder references. I've uploaded a bunch of files to S3 (~250GB), organised in a directory tree. I used s3 sync with storage class set to STANDARD_IA and everything went smoothly. However, in CloudWatch I see that a whopping 97.001 bytes is stored as STANDARD. I then follow with a s3api list-objects and see that only directories have the class of STANDARD - and they all have a size of 0. In that output, no object with non-zero size has the STANDARD class. Now my question here is - is that documented behaviour? Is that behaviour by design? I've briefly went through http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf, but it's a tad too large to quickly find the answer. What exactly is stored as STANDARD, if directory references are 0 bytes? I would agree that this behaviour is confusing, to say the least. If you have your whole bucket and all its folders and directories set to STANDARD_IA, there should either be a visible message in the Console, or nothing should be stored as STANDARD.", "Answers": {"usr-1": ["Hello, I will ask engineering to comment on this. I'll post back as soon as I can. Richard"], "usr-2": ["Thanks Richard. Just an FYI - since then, I've uploaded one new file (~5GB) and overwritten one other file (size changed from ~3GB to ~4.5GB). Now CloudWatch indicates there are 450 bytes stored as STANDARD."], "usr-3": ["Hi, An item uploaded with STANDARD_IA storage class is charged for minimum 128 Kb, even when it's just a few bytes size or zero (like folders). Amazon uploads such small items with STANDARD storage class to avoid overcharge. Surprisingly, Amazon saves your money sectoid, now working for CloudBerry Lab."], "usr-4": ["Well, I wouldn't necessarily say that's surprising However, I did not upload any such small items, they're all quite big. Also, output from s3api list-objects doesn't show a single object with non-zero size stored as STANDARD. Only zero-sized objects - folder references - are stored as STANDARD. That still doesn't add up to those whatever amount CloudWatch reports as stored as STANDARD."], "usr-5": ["Hello, Thanks for your patience, this is due to Amazon charging to store the key names themselves, we bill for the key length too. \"Standard - Infrequent Access Storage has a minimum object size of 128KB. Smaller objects will be charged for 128KB of storage\" ~ http://aws.amazon.com/s3/pricing/. I hope this helps, if you have any further question, please do not hesitate to re-contact us, we will happily assist. Best regards, Jil"], "usr-6": ["Thanks Jil, that does make sense."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=224076&tstart=125", "Title": "append to existing object?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "is there a way to append to an existing S3 object? (not a multipart upload)", "Answers": {"usr-1": ["Hello, No, there is no way to append to an existing S3 object. Richard"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=224435&tstart=125", "Title": "URGENT - Please reactivate my account > 24hrs ago requesting re-instatement", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi. My account 954749500283 was suspended for billing purposes, we've paid the outstanding invoices. I opened a support ticket yesterday morning > 24hrs ago requesting re-instatement, but haven't heard back. It won't let me access the account because it is 'suspended' and my website is down. Nobody answers the tickets? Edited by: fdeluy on Jan 30, 2016 8:07 AM", "Answers": {"usr-1": ["Hello, Thank you for writing into the Forums. I have asked a member of our CS team to look into the case that you have created. For future reference, for urgent matters please use the Call or Phone feature, which enables you to speak live with our 24/7 team of customer service members who can help with any account or billing related request. Regards, Troy W"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=224952&tstart=125", "Title": "accidentally deleted an unversioned bucket - is there a way to recover it?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I accidentally deleted an unversioned bucket Is there a way to recover it? Thanks, David", "Answers": {"usr-1": ["Hi David, I'm afraid not. Once a bucket is deleted it's gone. Sorry about that. Richard"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225088&tstart=125", "Title": "Advice on S3 User Permissions and Setup", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi All I'm new to AWS and don't know too much of what can and can't be done or how it should be done. I'm creating a new cloud application which is a very small scale dropbox (private customers only though so no public registering) with some added features. I want to use S3 as the file storage but just wondering how to set it up. Here is what i've planned in short. I have several companies who will each have a bucket with their specific files in it. I want only them to access their own bucket but also I want MyAdmin to be able to access all companies buckets. Obviously my admin will have read write access to all but each company will have read/write users and a readonly users. On top of this I want to have a database with information on each company and their specific enhanced features (that they have chosen) which mainly involves in monitoring files and sending out notifications. The plan is to develop a desktop app in .NET and use that as background software for file syncing and then have a cloud server which monitors the changes and sends specific notifications. So I am thinking, do i setup each company with two IAM users (readonly and readwrite)? or is IAM meant to be for internal use only? Or do I set up the users in a seperate database and then authenticate using that and then point them to the appropriate bucket? If this is the case how best to setup S3 to do this? Hope that all makes sense. Thanks Anthony", "Answers": {"usr-1": ["Hello, Both approaches has their pros and cons. Separate IAM users: -Easy to set up. You gust generate an access key and use it in your app for authentication. -Difficult to manage. If you access key is updated, you would also have to update it on side of your client. -Not secure. You would also have to store your access keys on your client's site. If they compromise it, you would have to rotate a key (see previous step) Separate database: It's actually called \"Web Identity Federation\" or \"Single sign on\". -Easy to manage. You don't have to create separate IAM users, your user authenticates using Active Directory or Facebook (you can also develop your own identity broker) and assume a set of permissions according to your application settings -More secure. You don't expose your access key at all. So nobody can steal it from your app. -Difficult to set up when comparing with using IAM credentials. You have to set up much more things such as your Identity broker, iam role etc. rather than just creating a user and assigning permissions. More information here: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html So, If you want reliable and scalable application, I could suggest you the second option. If you just want a quick solution, use IAM users and access keys. Regards, Vlad, CloudBerry Lab"], "usr-2": ["Thanks for the response Vlad. As time isn't really an issue at the moment I'd rather plan for it to be scalable so the second option would probably suit. What I don't really want to use are things like facebook etc to authenticate as the service isn't a public service as such. I'd rather setup the user myself and go from there. Can I setup users manually with Identity Brokers? Or do these brokers allow for any email address? Looking into it amazon has a service called Cognito. So for example can I give cognito an email address my.customer@company.com and setup a password? Not sure if you used cognito but have i got the methodology right in terms of: I register a new user in cognito (if this is possible) I set the user to have a particular access to a particular bucket in S3, done by IAM role? (Does that mean I still have to setup IAM Roles (ReadOnly and FullAccess) for each company?) Whenever the user logs in, cognito connects to S3 and gives back temporary access keys appropriate to the role that they have been assigned The user gets access to their bucket Thanks again."], "usr-3": ["Hello, Obviously, I am not so familiar with Cognito so I am not 100% sure. According to FAQ, it only supports Amazon, Facebook, Twitter, Digits, and Google. If you want to authenticate using your own domain, I could sugget you consider using one of the following 3rd party solutions: http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html Regarding your question about an IAM role. Yes, it will devine a set of permissions for your federated users. You will have the following workflow: Your user authenticates in your identity provider -> Issues a token -> Assumes a role with a corresponding set of permissions according to the token. Please let me know if you have more quesitons. Regards, Vlad, CloudBerry Lab Edited by: cloudberryvlad on Mar 2, 2016 1:23 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225318&tstart=125", "Title": "How to host a Dynamic website in s3 bucket?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have my website domain registered with other registrar. i just want to host my website on s3 bucket, So can you tell me the detailed steps how to do so? my website is dynamic so i also want to use AWS cloudfront and AWS WAF. Suggest me what will it cost? Please see the images uploaded. 1. I cannot transfer my domain to route 53. 2. My bandwidth usage per day Please give me detailed steps instructions. Thank You!! Edited by: ashutoshvct on Feb 12, 2016 3:23 AM", "Answers": {"usr-8": ["HI, Answering my own question !!! You just upload your data to s3 and configure it in route 53 and cloudFront. You can see many videos for that on YouTube."], "usr-1": ["Hello, Seems like we need to clarify several moments before you proceed. 1. What exactly do you mean by the \"Dynamic website\" term? Are you using PHP or any other similar solution for your website? In this case, it won't be possible for you to host it entirely on S3. You need a server that will serve your dymanic content to your customers. EC2 is desigend for such purposes. You can find a tutorial about how to host your website on AWS here: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-LAMP.html 2. CloudFront is a content delivery network managed by AWS, the purpose of it is to reduce latency for frequently accessed objects. It can only be used with a statcic obgects, such as images, videos, webpages. More details here: https://aws.amazon.com/cloudfront/ 3. WAF is AWS managed firewall. The purpose of it is to filter your traffic in order to enhance security of your web application. There is actuallly no direct relation betwen static/dynamic content and AWS WAF. More information here: https://aws.amazon.com/waf/ 4. Your bandwidth usage per day depends on how many visitors you have on your website and how many traffic they generate. There is not enough informaiton to answer this quesiotn now. Same situation with your costs. 5. Seems like you specified Indian address in your contact details. An ability to transfer or register Route53 domains is not available for this area now. More information here: http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-account-payment-aispl.html Actually, I could suggest to rethink your entire AWS usage stratagy. Try providing us with more certain informaiton about your requirements and I could suggest a solution for you. For example: PHP + MySQL web site that will be visited by 1000 users per day. Regards, Vlad, CloudBerry Lab"], "usr-2": ["Look to be more clear I have my lamp server hosted with 3GB ram, 100 websites are hosted on that, space is 85GB and 60 GB bandwidth per day. I want to move to AWS so which services i must use ? On an EC2 instance i will install an lamp server and then what ? My domain are hosted by other services like \"known host\" and other providers. The bandwidth cost is very High."], "usr-3": ["Thanks for the clarification. In your case I could suggest to start with the following infrastructutre: Autoscaling group with at least two m3.medium EC2 instances in different availability zones as your web server tier. An Elastic Load Balancer that distributes traffic between your instances. A db.m4.xlarge RDS instance for hosting your database. Optionally, you can offload your static content to S3 and CloudFront so it could reduce a load. All this staff including your bandwidth costs that are alredy included into the EC2 instance price will cost you around $0.067 $ per hour. Please note that this value is not final, I beleive it could be possible to optimize after perfroming some initial configurations and tests. Regarding the process of moving to AWS, here is the workfolw: 1) Design and develop your cloud infrastructure. 2) Migrate your website data 3) Test and optimize the environment. 4) Switch your DNS, so clients will go to the new website. In my opinion, steps 2 and 4 seems to be the most chalanging because you will have to think about how to perfrom them quickly so your customers will not encounter and downtime or data loss. Regards, Vlad, CloudBerry Lab mistake in calculations and wrong instance type"], "usr-4": ["Thanks for your response!! Btw You forgot bandwidth usage which is going to cost more. So please tell me a price in asia region as i live in india. Thank you once again."], "usr-5": ["Hello, I have already mentioned that bandwidth costs are already included into your EC2 costs. If you need more information particular details about EC2 pricing, feel free to vistit this page: https://aws.amazon.com/ec2/pricing/ Regards, Vlad, CloudBerry Lab"], "usr-6": ["Thanks for your reply !! I also had one question should i buy an instance in asia region as i live in india and my clients are mostly in asia region or i cloud host it in US EAST region which is quite cheap. Region : ASIA If i host an instance with 3.75GB ram t2.medium it will cost me $58.56 and the bandwidth charges are not even included. If i add the 50GB data transfer out per day it would cost $312. Which is quite expensive. Edited by: ashutoshvct on Feb 15, 2016 2:57 AM"], "usr-7": ["Hello, Ideally you should be running resources in region close to your user base. Latency from India to Singapore region will be much less than from India to US-east-1 (N Virginia). However you can opt to use Cloudfront service to serve frequently accessed data with lower latency."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225106&tstart=125", "Title": "How can i backup using S3?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I want to move my backup to AWS cloud. I Don't access my data frequently but when i do i don't want to wait for 3-5 to let the job complete as I read it in AWS Glacier Docs. I don't have backup on AWS Glacier. So i'm thinking about S3 backups - Standard Infrequent Data Storage. My data size is about 300GB-500GB. How secure will be my data? What will it cost me?", "Answers": {"usr-12": ["Thanks carl for your answer as, it was quite helpful . You may help me by answering my two unresolved question."], "usr-10": ["Thank you so much for your response and one more thing if you can add the download cost for 500GB data/month."], "usr-11": ["ashutoshvct, The retrieval costs for 500GB a month would be $5 in bandwidth costs + $.50 in GET request costs, assuming the retrieval was accomplished using ~500,000 GET requests. AWS provides a simple monthly cost calculator to help current and potential customers estimate their monthly costs across AWS services. The S3 section has support for the Standard IA storage class, including monthly storage, request, and retrieval bandwidth costs. The calculator can be found here: http://calculator.s3.amazonaws.com/index.html. I hope it helps, but please post any follow-up questions you might have. --Carl"], "usr-8": ["i don't want to know about your product, I'm interested in Amazon S3. The amazon S3 console is very easy to use. I want to know about S3 security on AWS."], "usr-9": ["Hello, Storing 500 GB of data in Standard-IA will cost you around 7 dollars per month. Regarding your questions about security. Access to data stored in Amazon S3 is restricted by default; only bucket and object owners have access to the Amazon S3 resources they create. There are multiple ways to control access to buckets and objects: -Identity and Access Management (IAM) Policies. -Access Control Lists (ACLs). -Bucket Policies. For maximum security, you can securely upload/download data to Amazon S3 via the SSL encrypted endpoints. Amazon also provides AES-256 Server-side encryption for protecting data at rest. You can also use your own encryption keys for that purposes. More information about security in AWS could be found here: https://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf If you have more questions, please let me know. Regards, Vlad, CloudBerry Lab"], "usr-1": ["Hi I suggest that you check out CloudBerry S3 Backup to automate Windows Computer backup to Amazon S3 and Glacier. It comes with the following features: Scheduling and Real-Time Cloud Backup Comes with one time fee and no recurring charges. No proprietary data format and you can access your data using other Amazon s3 tools. Supports all Amazon S3 regions and Reduced Redundancy Storage and Standard-IA. Amazon Glacier support Encryption & Compression Local Backup Incremental and Block Level Backup Network Locations Backup 15 day free trial is available http://www.cloudberrylab.com/backup How to sign up and get started with Amazon S3 Backup http://blog.cloudberrylab.com/2012/02/how-to-sign-up-for-amazon-s3-and.html Thanks Andy"], "usr-2": ["Why should i pay for the cloudberry software? I can easily use s3cli for that, which is free and you don't provide this software for OS X. There are other tools too which are free and provide backup and easy retrieval or Download. Thank You for your response BTW. Edited by: ashutoshvct on Feb 10, 2016 12:43 AM"], "usr-3": ["You can learn more about CloudBerry Backup for Mac and download the 15 day trial version at http://www.cloudberrylab.com/backupmac.aspx Both Windows and Mac version come with a free edition. p.s. we are not trying to sell you anything. We are just posting here as many other people browse the forum and what to see the options."], "usr-4": ["So, is the freeware available for 15 days trial or the pro version?"], "usr-5": ["There are two options, FREE and PRO Trial. When you start the product you choose which mode you like to run it."], "usr-6": ["My question remains unanswered as i wanted to know what it is going to cost me on s3 bucket and How secure s3 bucket is? So, please answer my question."], "usr-7": ["We support the multiple industry standard encryption algorithms including AES 128-256 bit, DES 64 bit, RC2 40-128bit, 3DES 112-168 bit and it does encrpyt/decrypt on the fly. But this is the PRO version only. In the US-East region you pay $0.0125 per GB for the Standard_IA storage type. you can learn about the storage pricing at http://aws.amazon.com/s3/pricing/"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225835&tstart=125", "Title": "Multiple buckets or folders - then credentials which route?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "ok I am migrating from an old NFS scenario to AWS and wish to move the data to S3. Currently the traditional is your basic NFS export, servers have a datapoint mounted. Filepath is always /data/customerID. Each customer should have the ability via the web to download only their data via the web as well. So, I am looking to see if it's better to have one bucket, then folders (similar to the current) OR a separate bucket for each customer. I don't know if I would have to create an access/secret key for every customer, store them, or a single bucket, never showing the URL, but when they export, just have S3 URL directly at their folder. I think the single bucket is the right path for management of security, etc. but just throwing this out there.", "Answers": {"usr-1": ["Hello, The following Security blog post will assist you in setting up separate folders for each user within a bucket: http://blogs.aws.amazon.com/security/post/Tx1P2T3LFXXCNB5/Writing-IAM-policies-Grant-access-to-user-specific-folders-in-an-Amazon-S3-bucke In the above example, each user needs to have unique IAM security credentials. I hope this helps! Kind regards, Chris"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226059&tstart=125", "Title": "Delete Multiple files from Multiple Folders", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I copied to s3 a folder with many child folders and every child folder has .DS_Store in them. I'd like to delete this file from all folders on an s3 bucket. Anyone know how to accomplish this?", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225579&tstart=125", "Title": "Should we create \"Instance Export\" bucket manually for new AZ ?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, Just observe that there is another AZ in our Region. We are using AWS Management Portal for vSphere and have already created a number of buckets for our company as 'export-to-S3-MyCompany- Region '. Should we create the S3 bucket 'export-to-S3-MyCompany-ap-southeast-3' ? Thanks", "Answers": {"usr-1": ["Hey tonyjk, Export buckets are based on regions not on AZs, so you don\u2019t need to create a new bucket each time a new AZ is added to a region. Only if a new region was added you would be missing a bucket. In that case, you could either create the bucket yourself or reconfigure your export bucket name at https://amp.aws.amazon.com that would automatically create a new set of buckets for you. Thanks"], "usr-2": ["Does it mean that when the new Bucket is create, OVA files have to copied across ? Edited by: TonyLee on Feb 21, 2016 9:57 PM"], "usr-3": ["Exporting the VM through AWS Management Portal for vCenter will create the OVA files in the S3 buckets for your on-premises use. Please let me know if this doesn't help. Thanks, Anbu"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225917&tstart=125", "Title": "Files have disappeared from S3 bucket?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "For sure Amazon did not lose my files on S3, I get that. However, I do not see any reason on the code-side why files would have been deleted... Is it possible to get more information on files that have been previously there, but now gone? User that deleted them, timestamp? Bucket: recruitmenteventapp-static Thanks!", "Answers": {"usr-1": ["If you have logging enabled on the bucket you could check the logs."], "usr-2": ["That is exactly what I enabled once I saw the option... I am hoping that there will be more files deleted so I can investigate from the logs (there are still files left), but I don't have a trace from when it happened... So far the logs only described something like DescribeTrails, GetBucketLocation,..."], "usr-3": ["It sounds like you have enabled CloudTrail logs for S3. The CloudTrail logs only show requests at the account level, ie, creating and deleting buckets, getting bucket location, etc. To see S3 object level requests, you should enable S3 server access logging described here: http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html."], "usr-4": ["Thanks for clearing up the logging. Still looking into a cause for the files that have disappeared..."], "usr-5": ["So in the end it was the incremental backup to S3 from my Virtualmin instance that caused files to disappear. The setting was to delete old (> 60d) backup files, which apparently deletes all old files (not just backup). Fixed it by putting the backup in a seperate bucket."], "usr-6": ["Hi, we are having same issue on latest days. We are noticing a lot of images suddenly disappeared without any cause and our customers have not made anything. All images are uploaded from Drupal with S3FS module and S3 Bucket doesn't have enabled versioning. They had to restore lost files one by one from content editor. We don't have any CloudTrail or similar AWS service. Edited by: bextsa on Aug 19, 2016 10:53 AM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=225176&tstart=125", "Title": "AWS Inter-region Charges?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I had a confusion regarding this : Example: I'm in (india)Asia and i have create a bucket in region US Standard. So when i retrieve my data from india will i be charged for inter-region retrieval charges or the normal data outbound charges. Thank You!!", "Answers": {"usr-1": ["Hi, You will be charged according to 'Data Transfer Out from Amazon S3 to Internet'. I would recommend you to go through AWS S3 Pricing for more details. Hope this helps. Regards, Jayakrishnan L."], "usr-2": ["I know that . I'm asking about inter-region data transfer out charges. Suppose i store my data in US Standard region and i live in india and retrieve it from there so will i be charged for inter-region data transfer. I want to store my backup so it will be accessed by me only."], "usr-3": ["Hi, Let me explain data transfer charges in detail. There is no Data Transfer charge between two Amazon Web Services within the same region (e.g., between Amazon EC2 US West and another AWS service in the US West). Data transferred between AWS services in different regions will be charged as Internet Data Transfer on both sides of the transfer. Inter-region transfer charges only occur if data is transferred between AWS regions. See \"data transfer\" for charges at our pricing page: http://aws.amazon.com/ec2/pricing/ See \"Billing\" at ec2 faqs page: https://aws.amazon.com/ec2/faqs/ I hope this clears it up for you."], "usr-4": ["Hello, Still i have doubt Let me put it this way. Suppose i have a website on s3 bucket in US standard region and some user tries to access it from Asia region(India), then i will be charged for Inter-region Data Transfer right?"], "usr-5": ["Hi, Answering my own question Here!!! Yes, i will be charged for intra- region data charges as I live in India and have created a availability zone in Singapore(Asia). Thank You!!!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226080&tstart=125", "Title": "Cannot access any S3 files in Seoul region", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Uploaded a file to a Seoul bucket, made public and accessible to everyone. The url indicates public but tried it in all browsers and with different files, all timeout.", "Answers": {"usr-1": ["Hello, I'm able to access a public file in a Seoul bucket without any problems. Did you follow these steps?: http://docs.aws.amazon.com/AmazonS3/latest/UG/EditingPermissionsonanObject.html If you're still experiencing issues: 1) Do you experience the same problem when the files are in a bucket in another region? 2) Have you tried accessing the file from a different network (e.g. 3g mobile data?) 3) Do you perhaps have a bucket policy in place on the bucket that denies access to the file? 4) Do you have a screenshot of the error message you receive? 5) Are you able to provide the details of the bucket and file? (you can send the public link via PM if you want so we can test it.) Kind regards, Chris"], "usr-2": ["I have the same setup in all regions, using a bucket policy to give read access to the files. Works in all regions but Seoul. Even used the API, checked ACL and Policy, comes back saying files are read for all. The only time I can access the files is if I spin up a vm in Korea, then I can access the files from a browser? PM an example file to you."], "usr-3": ["Thanks, I have replied via PM. -Chris"], "usr-4": ["My ISP was blocking traffic from AWS Seoul, but not other South Korea website traffic."], "usr-5": ["Thanks for the update. I'm glad to hear you found the cause of the problem. -Chris"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226139&tstart=125", "Title": "Android TransferUtility hangs with no message / error message /exception", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi there I am using S3 in an aAndroid app thru the transfer utility that as described here and I experience random 'hang ups'... I am using a listener to chain files and process them once they have been transferred, and randomly the listener stops receiving updates. There is nothing in the logs... A few seconds later, I see 'TransferService stopped' in the logs. I assume that maybe the download actually went through, but my listener is not called. Does this ring any bell to anyone ? Jean-Michel", "Answers": {"usr-1": ["My bad, something was wrong in my code."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226464&tstart=125", "Title": "Exception while multipart uploading a file using C#", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have a C# program that I wrote to uplaod files to S3. It worked great until I took it for a spin in our beta customer environment. From the machine they will be uploading the files (DICOM images), the program throws a exception. No idea what the exception, dont know how to reproduce. It works on most of the computers we have except he one in the hospital. The hospital one is running Windows 7 enterprise. and we are using SDK 2. Thanks for any help. Framework Version: v4.0.30319 Description: The process was terminated due to an unhandled exception. Exception Info: Amazon.Runtime.AmazonServiceException Stack: at Amazon.Runtime.Internal.ErrorCallbackHandler.InvokeSync(Amazon.Runtime.IExecutionContext) at Amazon.Runtime.Internal.PipelineHandler.InvokeSync(Amazon.Runtime.IExecutionContext) at Amazon.Runtime.Internal.MetricsHandler.InvokeSync(Amazon.Runtime.IExecutionContext) at Amazon.Runtime.Internal.RuntimePipeline.InvokeSync(Amazon.Runtime.IExecutionContext) at Amazon.Runtime.AmazonServiceClient.Invoke[ http://System.__Canon, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089 , http://System.__Canon, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089] (System.__Canon, Amazon.Runtime.Internal.Transform.IMarshaller`2<Amazon.Runtime.Internal.IRequest,Amazon.Runtime.AmazonWebServiceRequest>, Amazon.Runtime.Internal.Transform.ResponseUnmarshaller) at Amazon.S3.AmazonS3Client.InitiateMultipartUpload(Amazon.S3.Model.InitiateMultipartUploadRequest) at VitalEngineUploader.Form1.threadedUploadDirectory(System.Object) at System.Threading.QueueUserWorkItemCallback.WaitCallback_Context(System.Object) at System.Threading.ExecutionContext.RunInternal(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean) at System.Threading.ExecutionContext.Run(System.Threading.ExecutionContext, System.Threading.ContextCallback, System.Object, Boolean) at System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem() at System.Threading.ThreadPoolWorkQueue.Dispatch() at System.Threading._ThreadPoolWaitCallback.PerformWaitCallback() Request ID(s):", "Answers": {"usr-1": ["I figured this one out so wanted to close this question with what I found. There was an unhandled exception that I needed to capture and that showed me that the issue was with the proxy server. Snippet of the exception is below: MyHandler caught : A WebException with status ProtocolError was thrown. Amazon.Runtime.AmazonServiceException: A WebException with status Protocol Error was thrown. ---> System.Net.WebException: The remote server returned an error: (407) Proxy Authentication Required. at System.Net.HttpWebRequest.GetRequestStream(TransportContext& context) at System.Net.HttpWebRequest.GetRequestStream() at Amazon.Runtime.Internal.HttpRequest.GetRequestContent() ...... ...... This was showing that I needed to log in to the proxy server as the logged in user. I added the following lines to the app.config file and all was good from there on.' <system.net> <defaultProxy useDefaultCredentials=\u201dtrue\u201d> </defaultProxy> </system.net> Edited by: MacDWD on Mar 16, 2016 10:42 AM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226588&tstart=125", "Title": "Web access denied when accessing from EC2 private IP", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I've hosted a static website on Amazon S3, and now I'm trying to limit access to the site only from allowed IPs. I've also hosted a web proxy on the different AWS account's EC2 instance and like to allow only the proxy to access the website. The bucket policy setting has been done like the following, but I still got the '403 Access Denied' error even when accessing via the proxy. { 'Version' : '2008-10-17' , 'Statement' : [ { 'Sid' : 'DenyPublicReadForGetBucketObjects' , 'Effect' : 'Deny' , 'Principal' : { 'AWS' : '-' } , 'Action' : 's3:GetObject' , 'Resource' : 'arn:aws:s3:::BUCKET_NAME/-' , 'Condition' : { 'NotIpAddress' : { 'aws:SourceIp' : [ 'PROXY IPADDR' ] } } } , { 'Sid' : 'AllowPublicReadForGetBucketObjects' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : '-' } , 'Action' : 's3:GetObject' , 'Resource' : 'arn:aws:s3:::BUCKET_NAME/-' , 'Condition' : { 'IpAddress' : { 'aws:SourceIp' : [ 'PROXY IPADDR' ] } } } ] } After enabling logging I found out the access source was an internal IP of the proxy's instance. Therefore, I changed SourceIp to the internal IP but got the same result. What would be a good way to achieve it? Thanks for any help.", "Answers": {"usr-1": ["This problem seems to be similar to https://forums.aws.amazon.com/thread.jspa?threadID=226118."], "usr-2": ["Hello, \"When using Amazon S3 endpoints, you cannot use a bucket policy or an IAM policy to allow access from a VPC CIDR range (the private IP address range). VPC CIDR blocks can be overlapping or identical, which may lead to unexpected results. Instead, you can use a bucket policy to restrict access to a specific endpoint or to a specific VPC, and you can use your route tables to control which instances can access resources in Amazon S3 via the endpoint.\" please refer to this document for more info about vpc endpoints: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html Regards Fawzi H."], "usr-3": ["Understood. Thanks. I followed the following document, and everything works fine. http://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies-vpc-endpoint.html Edited by: taizooo on Mar 4, 2016 4:54 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226593&tstart=125", "Title": "S3 export status", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Two weeks ago, I shipped a box with six hard drives for an S3 export to disk. According to the API, three of the drives have been received but the jobs not started, while the other three are still 'NotReceived.' Is this delay to be expected, or is there maybe something wrong with the jobs? It seems very odd that half the drives were not received, considering they were all in the same package. JobIDs are: DWDXQ, CU87V, 7RYCJ, 8Y434, CCBMJ, 3WAPK", "Answers": {"usr-1": ["Hi N Ship. My apologies for the delay with your jobs. I will confirm with the data center that all of your devices have been received and PM you back with specific status. Frank"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226763&tstart=125", "Title": "can S3 lifecycle rules be recursive?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I have a bucket with the structure foo/1/bar/many files here foo/2/bar/many files here ... foo/1000/bar/many files here I want to permanently delete the many files after X days. Can I use a prefix of foo/-/bar to apply lifecycle rules? It doesn't seem to work. Would the prefix foo/ work? Can I stipulate recursive lifecycle rules?", "Answers": {"usr-1": ["The 'Prefix' element in lifecycle configuration does not support regular expressions. If you specify 'foo/' as the prefix in the lifecycle configuration, that would apply the lifecycle rule to all S3 objects in your bucket with key name beginning with foo/. Hence all of the following objects would become eligible for lifecycle rule if prefix is 'foo/' foo/1/bar/many files here foo/2/bar/many files here ... foo/1000/bar/many files here foo/something-else Hope this helps. Regards, Abhilasha"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=226900&tstart=125", "Title": "s3 Import Job 'Pending' for almost 2 weeks", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "My import job 'KBMVH' has been at AWS and pending for a while now. Can someone aw AWS comment on if there are any issues with this particular job?", "Answers": {"usr-1": ["Hi steven_sz. My apologies for the difficulties here. I see that you device has been received in our data center. I will get back to you with an update on when it will be processed. Frank"], "usr-2": ["Thanks you!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237745&tstart=50", "Title": "No metadata on uploaded items in S3 browser/console?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi! I've uploaded a file with custom metadata (x-amz-meta-blah), and the metadata are not visible in the S3 console/bucket browser in the metadata dropdown. If I GET the item I see the metadata fine. Is this a regression/bug? The documentation seems to imply the S3 console should show all the custom metadata. Thanks!", "Answers": {"usr-1": ["Looks like the metadata is visible today. Bug fixed? Who knows."], "usr-2": ["Looks like the problem was fixed sometime in the night."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237740&tstart=50", "Title": "GET requests not logged - log PUTs are logged", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I just set up two buckets, with bucket1 logging into bucket2/logs (configured in S3 Management Console). Then I download a file from bucket1 successfully using a presigned url - and with an error when I manually modify the url. Strangely enough bot GETs are not in the logs. What's in the logs are the PUT requests where aws-internal writes a log entry. What could be the reason for this behavior?", "Answers": {"usr-1": ["It just took some time until the download logs arrived. Funny that logs about writing logs appeared significantly earlier."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237892&tstart=50", "Title": "Simple bash script for s3 copying with include/exclude help", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Trying to write a basic 101 script to just copy nightly log files for specific domains to specific buckets. When it works, simply touch a file, and delete them (the touch is so a watchdog can tell that it completes). But my basic test is finishing saying DONE but it's not copying. #/bin/bash year=`date +\\%Y`; \u00a0 if /usr/local/bin/aws s3 cp /mnt/freenas1/backup_temp/ s3: //admin.logs.misc/domain1/$year/ --exclude '-' --include '-domain1-' then echo done fi I run that and it comes back with DONE, but I don't see the copy output, and the files never get to reach S3. There are multiple files like 2016.08.19.domain1.tar.gz, but I have others like domain2, etc. so I want pretty much - domain1 - to go (sorry but the - next to the words makes them bold, but Im sure it's understood) Thanks all", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237879&tstart=50", "Title": "Error in response of GET bucket location call", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We are performing GET bucket location call with V4 signature generated using 'us-east-1' region in the following URL format: https://s3.amazonaws.com/my-bucket-name?location Sometimes, it returns the error in following format: <?xml version= '1.0' encoding= 'UTF-8' ?> <Error> <Code>AuthorizationHeaderMalformed</Code> <Message>The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'us-west-1' </Message> <Region>us-west-1</Region> <RequestId>A1A1</RequestId> <HostId>B1B1</HostId> </Error> 1. Is the region 'us-west-1' mentioned in 'Region' tag the correct location of the bucket? 2. Or is it asking us to make GET bucket location call again with new V4 signature generated using this region (us-west-1 in current example)? 3. Can we make multiple GET bucket regions calls simultaneously for various buckets in various regions? Edited by: Hitesh on Aug 24, 2016 11:39 PM: Added the question of multiple calls simultaneously Edited by: Hitesh on Sep 8, 2016 2:47 AM: It was a multithreading issue in the code. No issue from S3.", "Answers": {"usr-1": ["It was a bug in our code for multi-threading. Nothing related to s3."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237678&tstart=50", "Title": "The specified key does not exist. NoSuchKey", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We have about 200 failures and rising with the error below (a few examples). I have a bunch of examples if you need them. The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: B7741E80FCB73606) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 53513F36E00991E1) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 310EBAB6F587FFD0) We have also seen issues with files being deleted/moved/missing as we can't access them anymore and get the same error above.", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237961&tstart=50", "Title": "523 I/O error", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I am always getting '523 unknown I/O' error on specific directories and this has been happening from 5 Aug and before 5 Aug things were fine. Edited by: azfar123 on Aug 24, 2016 5:10 AM", "Answers": {"usr-1": ["fixed,related with idmap service."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238257&tstart=50", "Title": "Slow download speed on S3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We have a fiber internet connexion that allows us to upload and download up to 100 Mb/s. Usually, when we download files from our S3 account, a 800 Mb file would take a few seconds to download. But in the past days, download speeds are really low. I created a new bucket and upload a blank file to test : the upload speed was 21543 KB/s, but the download speed is 1,7 MB/s. I did a download speed on speedtest.net and the the results are 931,29 Mbps for download and 928,63 Mbps for upload. Does any one know why the download speeds have decreased ? You can try my test file : https://s3.amazonaws.com/test2016-08-29/TestDoanloadSpeed.dmg Screen Capture 1 Screen Capture 2 Screen Capture 3", "Answers": {"usr-1": ["I don't know what you did Mark, but download speeds are back to normal. Thanks !"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238310&tstart=50", "Title": "S3 Bucket Inexplicably Empty", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Our S3 bucket with our website is suddenly empty. We are certain that we didn't delete it. The CloudWatch metrics for the bucket indicate that there are 90 objects in the bucket totalling about 16M, basically little or no change for weeks. But we can't see any objects in the bucket in the Management Console or using the AWS CLI., and the pages are not being served via the static web hosting. What could have happened, and what do we do? Edited by: brianm on Aug 30, 2016 11:11 AM I figured out what happened. The problem was on our side.", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238481&tstart=50", "Title": "Ambiguous use of 'continue' for AWSS3TransferUtility.downloadData() swift3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "It's my first attempt with AWSS3. I'm looking at the aws-sdk-ios-samples on GitHub and I'm trying to port the S3TransferUtility-Sample / SecondViewController.start(sender : UIButton) function into Swift3. So far I have this: public func downloadCollection(_ name: String) { Logger.profile(self, function: #function) \u00a0 let expression = AWSS3TransferUtilityDownloadExpression() expression.progressBlock = { (task, progress) in DispatchQueue.main.async { if let download = self.downloadList[task.key] { download.progress=Float(progress.fractionCompleted) NotificationCenter.default.post(name: Notification.Name(kNoteProgress), object: nil, userInfo: [ 'path' : task.key, 'progress' : download.progress]) } } } self.awss3CompletionHandler = { (task, location, data, error) -> Void in DispatchQueue.main.async { let download = self.downloadList[task.key] if ((error) != nil) { Logger.add( 'ERROR downloading AWS file \\(error)' ) NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [ 'path' : task.key, 'error' : error]) } else if (download!.progress != 1.0) { Logger.add( 'ERROR downloading AWS file - did not receive whole file, percent recieved \\(download!.progress - 100) percent' ) NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [ 'path' : task.key, 'error' : error]) } self.downloadList[task.key] = nil NotificationCenter.default.post(name: Notification.Name(kNoteDownloadFinished), object: nil, userInfo: [ 'path' : task.key]) } } var xferUtility : AWSS3TransferUtility = AWSS3TransferUtility() xferUtility.downloadData(fromBucket: URLDownloader.AWS_BUCKET_NAME, key: '\\(name).iOS.zip' , expression: expression, completionHander: awss3CompletionHandler). continue { (task) -> Any? in if let error = task.error { Logger.add( 'ERROR downloading AWS file \\(error.localizedDescription)' ) } if let exception = task.exception { Logger.add( 'ERROR downloading AWS file \\(exception.description)' ) } return nil } } It's the xferUtility.downloadData that is causing the error. It says Ambiguous use of 'continue' when it tries to evaluate the downloadData call or, rather, the continue attached to its result. The downloadData is this open func downloadData(fromBucket bucket: String, key: String, expression: AWSS3TransferUtilityDownloadExpression?, completionHander completionHandler: AWSS3.AWSS3TransferUtilityDownloadCompletionHandlerBlock? = nil) -> AWSTask<AWSS3TransferUtilityDownloadTask> which returns a AWSTask<AWSS3TransferUtilityDownloadTask>. The AWSTask is where we find the continue... open func `continue`(_ block: AWSCore.AWSContinuationBlock) -> AWSTask<AnyObject> There are several `continue` functions but this appears to be the correct one based on the body of the closure in the sample app. Anyone know how to translate this so Swift3 will like it? I'm using Xcode 8.0 beta 6 (8S201h). TIA Edited by: Dozer on Aug 31, 2016 2:17 PM (better formatting)", "Answers": {"usr-1": ["I think you now need to add an open parenthesis after continue and before the {: completionHander: awss3CompletionHandler).continue ( { (task) -> Any? in and then close it after the } in your next-to-last line. Edited by: jasonmusser on Sep 15, 2016 1:15 PM"], "usr-2": ["@jasonmusser Thanks, that got me past the compiler errors."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238818&tstart=50", "Title": "Incorrect S3 documentation for PHP SDK v3 for Multipart uploads-Please fix?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Three locations need updated to properly show how to work with the current PHP v3 SDK and are currently INCORRECT as they stand, resulting in much confusion as this appears to be undocumented anywhere. They are still showing the old v2 sdk method and can be updated to be accurate with one very minor change. Can someone please get this updated in these locations? URL 1: http://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html URL 2: http://docs.amazonaws.cn/en_us/AmazonS3/latest/dev/LLuploadFilePHP.html These pages indicate completing the multipart upload as follows: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'Parts' => $parts, )); This is only valid for the old v2 SDK. It needs fixed to be the following for the v3 SDK: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'MultipartUpload' => array( 'Parts' => $parts, ), )); URL 3: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/migration.html This migration guide could easily be updated to reflect the simple change of wrapping the 'Parts' parameter inside a new array 'MultipartUpload' as shown above. This would be huge help to anyone migrating. Thanks! Edited by: dustout on Sep 6, 2016 8:35 PM Edited by: dustout on Sep 6, 2016 8:35 PM", "Answers": {"usr-1": ["Hello, Thank you for providing this information. I have passed this on to the necessary teams. Regards, Tim"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238904&tstart=50", "Title": "Can clients download files without making them public?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have a website in which I have linked files for sale that are in my S3 account. However I find that clients can only download them if I make the files public, in other words I grant everyone permission, which I think is risky. Is there a simple way to arrange for clients to download non-public files? The clients are people who come to my website, pay money and leave their email address.", "Answers": {"usr-1": ["Hi Jeffrey, The best is to look at the \u00abAuthenticating REST Requests\u00bb here - http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html You will need to work with IAM policies for your user, this product might be helpful in this case - (IAM policy manager for buckets / objects). Edited by: evgeny on Sep 8, 2016 5:06 AM"], "usr-2": ["Thank you Evgeny. The problem is that article is too technical for me. I need a simple solution. I could ask a different question: What is the risk if I make saleable files public? Can a person who has paid for and downloaded one file then download all the others without paying for them?"], "usr-3": ["Yes, you can create Pre-Signed URLs which gives the person the ability to download that file (and only that file) until the URL expires (you can set how long you want the URL to be valid for). To do this in PHP, you will want something like this: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-url.html Or this: http://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238909&tstart=50", "Title": "Cannot open own media files after download with libCurl + headers", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I download my own media files from my own S3 bucket using my own libCurl C++ application. If I mark a file as public and download it with a simple libCurl GET request (no additional headers), the downloaded file works perfectly fine. However, if I mark the exact same file as private and add headers to the GET request (Host, x-amz-date, x-amz-content-sha256 and the Authorization Header), I cannot open the downloaded file. For mp4 files: ffmpeg tells me that the moov atom of the file cannot be found. For png files: Windows tells me the 'file is not a valid bitmap file, or its format is not supported' The filesizes of a working and a 'broken' file differ slightly. The broken file is about 350 bytes bigger... My question is: Why does adding headers to the GET request alter the downloaded file? And what can I do to fix this? Thanks in advance!", "Answers": {"usr-1": ["Using a Hex Editor I was able to find out that the HTTP response is written into the received media file at the beginning. (Again: only if I include additional headers in the HTTP request. Otherwise, the received file is fine.) How can I prevent that?"], "usr-2": ["I found the solution myself. Just had to remove this line from my code: curl_easy_setopt(curl, CURLOPT_HEADER, true); Initially, I thought this line would allow me to add additional headers to my HTTP request... Instead, it includes the HTTP response in the output, which was my problem."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239459&tstart=50", "Title": "S3 subdirectories should be accessible via same parent virtual hosted-style", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "For years, until around the beginning of this month, (around 2016-09-01), I have been able to use a bucket as a virtual hosted-style domain, along with the subdirectories. To make it simple: bucket is www.jazeee-example.com contents: /index.html /some-icon.ico /directory/app.js Starting from scratch, Steps to reproduce: Create a new bucket called www.jazeee-example.com In bucket properties, Open Static Website Hosting Enable website hosting Observe the newly provided URL: http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com Upload the files above, including the directory/app.js As part of upload, set permissions to make everything public for all files. Access the files via URL: curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/index.html - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/some-icon.ico - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/directory/app.js - FAILS As a side note, within s3 management console, The URL for /directory/app.js is https://s3.amazonaws.com/www.jazeee-example.com/directory/app.js Effectively, s3 doesn't seem to work anymore as a virtual hosted-style site, if I have subdirectories as part of my site. Of course, it works if I use the path-style site, but one cannot use the standard documentation for AWS static site hosting unless your website is completely flat. http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Am I doing something wrong? Seems like the default process for static website hosting changed in the last few weeks, and now S3 only hosts root level files via 'virtual hosted-style' domain name.", "Answers": {"usr-1": ["Looks like this is specific to .js files. I am able to download the same file if renamed to .css curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/css/app.css works"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239563&tstart=50", "Title": "Cannot get S3 POST with SSE-C keys working.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm attempting to create an S3 POST with SSE-C keys generated from KMS. Despite my best efforts, the request fails with 'The calculated MD5 hash of the key did not match the hash that was provided.' I have included the proper headings according to the S3 documentation . I generate the x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 parameters with the following: var kmsParams = { KeyId: 'alias/myKey' , KeySpec: 'AES_256' } ; kms.generateDataKey(kmsParams, function(err, data) { if (err) { console.log(err) } else { var key = new Buffer(data.Plaintext).toString( 'Base64' ), hash = crypto.createHash( 'md5' ).update(key).digest( 'base64' ); } ); What am I doing wrong here? I realize that the customer encryption key should be 32 bytes (256 bits) long. However, generateDataKey seems to return a plaintext string that is 44 bytes long, even if I explicitly specify the NumberOfBytes parameter as '32'. Can someone explain this discrepancy for me? Even if the key was not correctly formatted, I imagine I would receive an error message to that effect. I've tried to generate the MD5 hash in a number of ways, but it continues to fail with the aforementioned error message. If I execute a standard PUT request, it works fine: var kmsParams = { KeyId: 'alias/myKey' , KeySpec: 'AES_256' } ; kms.generateDataKey(kmsParams, function(err, data) { if (err) { console.log(err) } else { var s3Params = { Bucket: s3Bucket, Key: filename, Body: myVar, SSECustomerAlgorithm: 'AES256' , SSECustomerKey: data.Plaintext } ; s3.putObject(s3Params, function(err, data) { if (err) { console.log(err) } else { console.log(data) } ; } ); } );", "Answers": {"usr-1": ["Resolved! The hash value should be: hash = crypto.createHash('md5').update(data.Plaintext).digest('Base64'); Only data.Plaintext needs to be hashed as Base64. If you modify it in any manner (e.g. Base64'd, toString), it'll return the error."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239491&tstart=50", "Title": "Big delay on S3 -> Lambda triggers", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm currently seeing a huge delay (approximately 20 hours) on objects added to S3 triggering Lambdas attached to the ObjectCreated event. As far as I can tell, no configuration has been changed, and the same Lambdas were being triggered within a few seconds only a couple of days ago. Is there some sort of rate-limiting / throttling that we've inadvertently run afoul of? Or is something quite badly broken on the S3 end? Or should we just not be expecting stuff to always run instantly (I've always been led to believe the events are supposed to be instant so I doubt it's that) Happy to provide any more information necessary if it'll help track things down. Everything in question is being run in us-east-1", "Answers": {"usr-1": ["Providing my own answer here following a conversation with AWS Support, in case anyone else runs into the same issue. It turns out that since our account has lately been running pretty heavy on Lambda usage, we were running into our concurrent execution limit - this appears to be global across the account and not per-function. So we had a few really hot functions clogging up the limit, and some of our quieter functions were being permanently throttled because they could never allocate a worker. As far as I can see there's no way to tell whether you're hitting or running near the concurrency limit, so I guess the only way to tell is to keep an eye on the CloudWatch logs for invocation count, duration, and throttles. A bit tricky if you have many, many lambdas, but there might be one or two that're particularly hungry for concurrency."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239549&tstart=50", "Title": "Archiving S3 to Glacier Requirements (Is a Vault Needed ?)", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "What are the S3 and Glacier requirements prior to moving data with Lifecycle policies into Glacier ? I need to upload Archival data via S3 so I keep the directory structure (I do know that I could upload data straight to Glacier but that would not be feasable for me) Data moves from my site to S3 buckets (via snowball and mirroring from onsite NAS to AWS S3 bucket . After I verify all data has been properly transferred, I create a Lifecycle policy so the whole bucket goes immediately (one day after synching) into Glacier for an indefinite time. My Questions: For GLACIER: It would seem logical to me to open a vault in Glacier but it is not mentioned in any documents - so is a vault needed ? For S3: Do I need to specify the type of storage of the bucket data prior to enabling the lifecycle management rule? No box is checked but the default when nothing is checked is Standard S3. Thank you Edited by: daradmin on Sep 19, 2016 12:25 PM Edited by: daradmin on Sep 19, 2016 12:26 PM", "Answers": {"usr-1": ["daradmin wrote: For GLACIER: It would seem logical to me to open a vault in Glacier but it is not mentioned in any documents - so is a vault needed ? No vault is needed. Files archived to Glacier through the S3 integration are managed internally by S3. You interact with Glacier solely through S3. The files will not be visible or otherwise available through Glacier directly. For S3: Do I need to specify the type of storage of the bucket data prior to enabling the lifecycle management rule? No box is checked but the default when nothing is checked is Standard S3. I'm not entirely sure what option you are referring to, but in general you don't need to do anything special beforehand."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239619&tstart=50", "Title": "www.domain.com redirect domain.com stopped working", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I transferred my domain name from another company to amazon aws route 53 since I was already using s3 buckets / route 53 for my site. I already set the www to redirect when I originally set it up and everything was working fine until the transfer. I noticed the ns was different for my redirect in route 53 than what was assigned and I changed it but that did not fix it. Please help plaportal.org is working but www.plaportal.org is not redirecting to it.", "Answers": {"usr-1": ["You seem to be missing a record for www. That goes into the same hosted zone as your plaportal.org A alias record. That single hosted zone will cover plaportal.org and any subdomains, such as www, that you want to add."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238993&tstart=50", "Title": "What is the risk if I make saleable files public in S3?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Can a person who has paid for and downloaded one file then browse and download all the others without paying for them when all files are made public inS3?", "Answers": {"usr-1": ["If they can access the bucket, yes. Why wouldn't you send them a pre-authenticated URL to the file? You can then restrict the time they are able to use the URL and thus download the file, also the URL is for that file only."], "usr-2": ["What is a pre-authenticated URL. Do I have to do this manually or can it be done automatically when they buy from my website?"], "usr-3": ["You can do this via the API. The PHP documentation is here (by virtue that it was the first 'useful' link that came up when I typed \"presigned s3 url\" into Google. https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-url.html Worth mentioning that this would mean removing \"Public\" permissions from your S3 Objects... Edited by: mal on Sep 10, 2016 2:16 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240674&tstart=50", "Title": "S3 file (Snowball imported) moving within failed", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I was trying to move a file from a folder in S3 to root location of bucket using CLI. Below is the command I tried from EC2 instance. aws s3 mv s3: //my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0 But I am seeing below exception in console. move failed: s3: //my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf to s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied \u00a0 Parameter validation failed: Invalid type for parameter UploadId, value: None, type: <type 'NoneType' >, valid types: <type 'basestring' > I am logged in as an IAM user, EC2 instance which ran command from has the role assigned and S3 bucket policy defines the necessary role. Also, from command line, I ran dzdo su before running AWS CLI. These files I try to move from a folder to root location, are imported using Snowball import and are SSE enabled. Should I add any other parameters or I miss something? Any inputs are highly appreciated.", "Answers": {"usr-1": ["I had to add --sse in my move command, as I had enabled Server Side encryption in my bucket. Then, it worked fine. aws s3 mv s3://my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf --sse"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239773&tstart=50", "Title": "S3 used for Elastic Beanstalk without explicit user consent", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I'm using several instances of EB and what is troubling me is that I have some S3 usage (when I checked my free tier usage ). The Elastic Beanstalk instances are part of EC2, and becuase EC2 uses EBS (as seen here https://eu-central-1.console.aws.amazon.com/ec2/v2/home?region=eu-central-1#Volumes:sort=desc:createTime ), its (free tier) storage is advertised as: - 30 GB of Amazon Elastic Block Storage in any combination of General Purpose (SSD) or Magnetic, plus 2 million I/Os (with EBS Magnetic) and 1 GB of snapshot storage So, if EB (as part of EC2) uses EBS, why was S3 added/used (especially without prior consent to service)? Why would I need S3 when I already have 30 GB on EBS (free)? This discrepancy (i.e. S3 not being required for EC2 instances) is also aparent from the SIMPLE MONTHLY CALCULATOR -- the EC2 (with its EBS) is distinct and does not auto-include S3 (which is a separate service and is not added to EC2). Example calculation: http://s3.amazonaws.com/calculator/index.html#key=calc-02ACBD08-3223-4866-8B4E-566FF0EEBA48andr=FRA Looking forward to your explanation", "Answers": {"usr-1": ["Hi, I'm very sorry to hear that you were charged for your S3 usage when you created Elastic Beanstalk environments. I've gone through the steps to create my own environments and I also wasn't able to see any warnings about free-tier usage or any mentions of S3 buckets until after the environment was created. I've taken it up with the Beanstalk team to let them know they should update the console to reflect the mandatory S3 usage. Best Regards, Omar S."], "usr-2": ["Our Beanstalk team have reached back. We are working on including a warning for s3 usage on our beanstalk console. Thanks for bringing this to our attention and we apologize for any inconvenience caused. One thing to note is that the charge was covered by the s3 free tier [1] assigned to you upon sign up to AWS https://aws.amazon.com/s3/pricing/ [1] Regards, Kay"], "usr-3": ["Thank you Omar and Kay for acknowledging this issue and quickly adding into the pipline to be fixed. @Kay: regarding the usage being covered with S3 free tier, there is another issue I have uncovered. Please see https://forums.aws.amazon.com/thread.jspa?threadID=239823 for details. Sincerely Yours, Robert"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240809&tstart=50", "Title": "How to entirely block requests to specific bucket?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Nevermind, found a solution. Please delete the thread. Edited by: frontender674 on Oct 9, 2016 1:32 AM", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240937&tstart=50", "Title": "Getting different results for S3 command-line vs the Golang library", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm getting different results for s3client.ListObjects() in Golang vs the equivalent AWS cli operation. In particular (with the Landsat public data), aws s3 ls s3: //landsat-pds/L8/139/ will list all of the 'folders' under that path. Using the Go interface, I only get the index.html entry. The Golang example ( gist ) does the following: params := ands3.ListObjectsInput { Bucket: aws.String(bucket), Prefix: aws.String(prefix), Delimiter: aws.String( '/' ), } \u00a0 resp, err := s3client.ListObjects(params) ... for _, key := range resp.Contents { fmt.Printf( '\\%+v\\n\\n' , -key) } \u00a0 Results in the single entry: { ETag: '\\'2c889e724e44d3bd7a002371417816d4\\'' , Key: 'L8/139/index.html' , LastModified: 2015-06-01 18:27:04 +0000 UTC, Owner: { DisplayName: 'landsat-pds' , ID: 'f5f9bd0f2036a9a809aae8e2bfdc5a70ad6bf696695739b71efc1cf5ccdc2718' } , Size: 3264, StorageClass: 'STANDARD' } Am I missing an option on the S3 request? Or is there a bucket setting I'm missing? Thanks!", "Answers": {"usr-1": ["It turns out the in the ListObjectsOutput struct there's a CommonPrefixes array, and the 'folders' are being returned in that field. Not the Contents field as I thought. I've updated the gist with code that works. Edited by: EricBlood on Oct 11, 2016 11:11 AM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248143&tstart=0", "Title": "Bucket endpoint address", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello all, I have created a bucket and trying to use from an application and it is giving the following error: 'error: S3ServiceException:The bucket you are attempting to access must be addressed using the specified endpoint.' I am using this format: s3://bucketname. I know the format is not an issue because I am able to use this format for another public bucket. I think the permissions on my bucket may be an issue but I am not sure. Can someone pl. help? Thank you in advance.", "Answers": {"usr-1": ["I did not include region and hence this error. Now its all good."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248485&tstart=0", "Title": "How to debug SignatureDoesNotMatch on uploading to pre-signed urls", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am using Boto3 on python to generate presigned urls successfully. s3 = boto3.client('s3') url = s3.generate_presigned_url('get_object', Params={ 'Bucket': bucket, 'Key': key,}) I successfully get a url of the following format: https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId= https://forums.aws.amazon.com/ andExpires=1486126711andSignature= https://forums.aws.amazon.com/ The AWSAccessKeyId matches my S3 user's was accesskeyid so looks good. Here is my curl request to upload: curl -v --upload-file example.jpg 'https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId= https://forums.aws.amazon.com/ andExpires=1486126711andSignature= https://forums.aws.amazon.com/ ' It keeps failing! With the following error: <?xml version='1.0' encoding='UTF-8'?> <Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>andltAccess Key Removedandgt</AWSAccessKeyId><StringToSign> Edited by: Hammad Ahmed on Feb 3, 2017 4:23 AM", "Answers": {"usr-1": ["Changed 'get_object' to 'put_object' as I was trying to put on a get pre-signed url."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248685&tstart=0", "Title": "S3 redirection rule timeout", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello! Following the AWS tutorial mentioned below, I've setup a redirect for my S3 bucket, but when a non-existing object is requested, the request times out after 30s. The bucket and Lambda function are in the same region. What am I doing wrong? Thanks! https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/ <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect> <Protocol>https</Protocol> <HostName>26j5rec0k3.execute-api.us-east-1.amazonaws.com</HostName> <ReplaceKeyPrefixWith>prod/resizeImage?key=</ReplaceKeyPrefixWith> <HttpRedirectCode>307</HttpRedirectCode> </Redirect> </RoutingRule> </RoutingRules>", "Answers": {"usr-1": ["I was using HTTPS instead of HTTP."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248782&tstart=0", "Title": "Enabling S3 Request Metrics from Cloudformation", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I may have misssed something in the docs, but is there a way to enable the S3 Request Metrics from Cloudformation instead of the AWS CLI?", "Answers": {"usr-1": ["No, I'm sorry; CloudFormation does not currently support configuring request metrics or the other recently-added S3 features (analytics and inventory). We are investigating this and will update this thread when we have more information available."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249016&tstart=0", "Title": "Can not acess S3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "We are getting a 403 error back when we try to add more items to S3 via the API. It was working up to 2000 puts and we are appilcable for the free tier which has a limit of 2000 puts, so I am suspecting that I have to somehow enable usage for more than 2000 puts. How do I do that?", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249007&tstart=0", "Title": "Read-after-write consistency in us-east-1?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, we're currently in eu-west-1 and consider an additional setup in the US. An important question is if there are still differences regarding the S3 consistency model, depending on the US region we choose. There is a blog post (8/2015) saying that for the US Standard (us-east-1) region, read-after-write consistency can only be expected through the endpoint s3-external-1.amazonaws.com: https://instrumentalapp.com/blog/why-you-should-stop-using-the-us-standard-region-in-s3-right-now/ An 'expired' forum post (7/2015) from AWS has the same statement: https://forums.aws.amazon.com/ann.jspa?annID=3112 An AWS blog post (8/2015) does not mention any exception regarding endpoints: https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-s3-introduces-new-usability-enhancements/ Searching in the current AWS docs, I also don't find this exception any more: http://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel https://aws.amazon.com/s3/faqs/?nc1=h_ls", "Answers": {"usr-1": ["We do provide Read-after-write consistency in us-east-1 now and this happened at some point last year. Please be mindful there is still no Read-after-update (sometimes) and this is consistent with all the regions."], "usr-2": ["Alright, thanks! That's the answer I hoped for."], "usr-3": ["Can you clarify if this means (for region us-east-1) 1. Yes, read-after-write consistency is supported only if you use urls that begin with s3-external-1.amazonaws.com or 2. Yes, read-after-write consistency is supported on s3.amazonaws.com urls"], "usr-4": ["S3 is Read-after-Write consistent in all regions and doesn't matter what endpoint you use. Hope this helps."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248968&tstart=0", "Title": "Adding server-side-encryption to client request causes a 403 Forbidden", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I added 'x-amz-server-side-encryption', 'AES256' to the form data posted to S3. Before I added that, uploading files was working smoothly. Then adding that, I started getting a 403 (Forbidden) response. I don't see in AWS's docs anywhere that there is a special bucket policy or configuration necessary to use this parameter. I also tried the suggested bucket policy for denying requests that DON'T request SSE, but as expected, that disallowed non-encryption requests, but still didn't allow encryption ones (ones that have that parameter). Is there missing info in the docs for making requests that specify SSE?", "Answers": {"usr-1": ["Answering my own question: I was using the form POST method of uploading a file to S3. I forgot that it requires a policy document in the post that can contain certain restrictions. The policy document being sent needed to allow encryption (`{\"x-amz-server-side-encryption\": \"AES256\"}` as one of my conditions)"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249090&tstart=0", "Title": "Static website with S3 SSL timeout", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I hope I am missing something simple here. I have created a static website with S3 and successfully assigned my subdomain cdn.bbcw.org to the bucket. According to one tutorial I read, the bucket name should match the subdomain - so my bucket is named cdn.bbcw.org (and I moved the 6+ million files from my old bucket over to it). I created a Certificate for cdn.bbcw.org and validated it, then a CloudFront distribution assigned to that bucket with an CNAME alternate name cdn.bbcw.org. For the time being I used HHTP or HTTPS but I just now changed it to Redirect HTTP to HTTPS. Either way, accessing https://cdn.bbcw.org gives me a timeout message. Perhaps it's related that accessing https://cdn.bbcw.org.s3-website-us-west-1.amazonaws.com also gives a timeout (accessing via HTTP works fine both on my custom domain and the AWS endpoint). Can someone tell me a step I might have missed? I have read things about problems with a period in the bucket name - but I had a similar problem before I created a new bucket. As a bonus question - Am I charged for moving the millions of files to another bucket in the same region? I see there are no data transfer charges but will this constitute a PUT, COPY, POST, or LIST request? I suppose it does since each directory must be listed during the move process?", "Answers": {"usr-1": ["I must have been following one of the numerous inaccurate or out-dated tutorials. The simple solution was the point my Route 53 to the CloudFront distribution URL rather than the s3 website endpoint. I was not aware that the basic address https://cdn.bbcw.org would give an Access Denied error while the actual files would work. My bad - it is in the Amazon documentation table here: http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249185&tstart=0", "Title": "S3 Download Speed slow", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, i'm looking at consistently poor download performance from several hosts whilst pulling s3-ia-objects from the frankfurt s3 region, e.g. ~ 10-15 Mb/s from 3 different endpoints with the same ISP. It is affecting multiple buckets and their objects, like this one: http://s3.eu-central-1.amazonaws.com/4f48caf1d8bcbef8/c5b38f8b3625d2b6/zerofile.raw This 100MB file has been downloaded with FreeBSD's fetch command on FreeBSD 11.0-RELEASE, but curl and wget give the same result. traceroute -P TCP s3.eu-central-1.amazonaws.com gives: traceroute to s3.eu-central-1.amazonaws.com (52.219.73.20), 64 hops max, 40 byte packets 1 10.0.32.1 (10.0.32.1) 0.964 ms 0.904 ms 0.958 ms 2 129.196-4-62.wifi-dyn.isp.proximus.be (62.4.196.129) 18.967 ms 18.906 ms 18.916 ms 3 ae-83-100.iarstr4.isp.belgacom.be (91.183.242.128) 22.175 ms 21.891 ms 21.922 ms 4 ae-13-1000.ibrstr5.isp.belgacom.be (91.183.246.112) 22.397 ms 22.427 ms 22.157 ms 5 - - - 6 - 94.102.160.37 (94.102.160.37) 22.350 ms 22.221 ms 7 - 94.102.162.35 (94.102.162.35) 26.933 ms 26.701 ms 8 195.66.225.175 (195.66.225.175) 26.683 ms 27.572 ms 26.965 ms 9 - - - 10 - - - 11 - - - 12 - - - 13 - - - 14 54.239.4.216 (54.239.4.216) 38.632 ms 54.239.5.134 (54.239.5.134) 41.842 ms 54.239.5.174 (54.239.5.174) 39.015 ms 15 - - - .. and gives timeouts including 64th hop. N.B: BSD traceroute as included in OSX and FreeBSD has this in the bugs section of the manpage: When using protocols other than UDP, functionality is reduced. In particular, the last packet will often appear to be lost, because even though it reaches the destination host, there's no way to know that because no ICMP message is sent back. In the TCP case, traceroute should listen for a RST from the destination host (or an intermediate router that's filtering packets), but this is not implemented yet. mtr --report s3.eu-central-1.amazonaws.com gives: Start: Mon Feb 13 23:29:14 2017 HOST: restore Loss\\% Snt Last Avg Best Wrst StDev 1.|-- 10.0.32.1 0.0\\% 10 1.0 1.1 1.0 1.2 0.0 2.|-- 129.196-4-62.wifi-dyn.isp 0.0\\% 10 19.5 19.3 19.0 19.5 0.0 3.|-- ae-83-100.iarstr4.isp.bel 0.0\\% 10 22.7 22.4 21.8 24.0 0.6 4.|-- ae-13-1000.ibrstr5.isp.be 0.0\\% 10 22.3 22.1 21.8 22.4 0.0 5.|-- 80.84.23.40 80.0\\% 10 22.5 22.5 22.5 22.5 0.0 6.|-- 94.102.160.37 80.0\\% 10 22.5 22.7 22.5 22.9 0.0 7.|-- 94.102.162.35 40.0\\% 10 27.2 27.5 27.1 28.9 0.6 8.|-- 195.66.225.175 0.0\\% 10 26.6 27.3 26.6 28.0 0.0 9.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 10.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 11.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 12.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 13.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 14.|-- 54.239.4.216 0.0\\% 5 40.8 40.7 40.5 40.8 0.0 15.|-- ??? 100.0 4 0.0 0.0 0.0 0.0 0.0 These performance issues have not been observed whilst downloading to a ec2-instance within the same frankfurt region or whilst downloading from several other providers like Azure or GCE from the same hosts, with or without ssl. Also these hosts have no problem downloading from a ec2 instance in the frankfurt region. Any thoughts?", "Answers": {"usr-1": ["Hello, the issue is resolved now. The problem appears that S3 does not send TCP Timestamps and as a result, FreeBSDs TCP Window Scaling mechanism fails to increase the receive buffer size accordingly, therefore resulting in lower troughput over higher latency links. Edited by: sydneymeyer on Feb 16, 2017 2:34 PM"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248687&tstart=0", "Title": "IAM Role for S3 no longer working, EC2 can't trace route to s3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "My IAM Role for S3 from my EC2 was working a few months ago but now is not working. I can't trace route to s3.amazonaws.com from this troubled instance. My other instance has no issue using its IAM role to connect with S3. Any idea why it may have stopped working? I tried the network security group settings to allow any IP inbound and outbound but it still does not allow me to connect. I'm not using any S3 Bucket Policies, just IAM Roles. One works and one doesn't. I've tried detaching and re-attaching the full access policy for S3 to the IAM role but it did not help either. I've also tried stopping and starting the instance and it did not help. Has anyone seen anything similar or know what I can try to get it working again?", "Answers": {"usr-1": ["Looks like adding the default VPC security group solved the issue."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249363&tstart=0", "Title": "Get Content-Encoding Metadata From CloudTrail Log File", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I'm using the .NET SDK to do a GetObjectStreamAsync() from an encrypted CloudTrail log written to S3. The log itself is stored as a json.gz file, however the response stream is the actual json file. I assume this is because these objects are stored with a Content-Encoding : gzip metadata entry. However, when you perform a GetObjectMetadataAsync() call, only the Content-Type header is returned, the Content-Encoding value in the headers is null. I would also expect to see these in the Response.Metadata collection, but only user defined metadata is returned there. When downloading different files written from CloudTrail and CloudWatch Logs, not being able to identify if the Stream from the GetObject response is actually the json file (as in CloudTrail) or the gzip file (as in CloudWatch Logs because the Content-Type is application/octet-stream) makes it difficult to parse these. Is there any way to retrieve the Content-Encoding metadata header without knowing what file I'm getting ahead of time? I'd like to be able to know if I need to unzip the file after downloading it or if I can read it directly. Unfortunately, even the json content from a CloudTrail log when written to a file are marked as Archive, so I can't check that to be able to tell, and .gz (and no extension gz files) aren't marked as compressed.", "Answers": {"usr-1": ["How are these \"encrypted CloudTrail log\" files written to S3? GetObjectMetadataAsync will return Content-Encoding if it is set on the object, so it sounds like the objects just don't have this header set correctly. Do the files have other header values like Content-Type which could help to decide how to treat the file? Of course, if all else fails, you may just need to try unzipping the file, and if that fails then try to read it as JSON."], "usr-2": ["The logs are being delivered directly from CloudTrail to S3. Please see the attached screen shots. You can see that the metadata is set, but it is not included in the GetObjectMetadataResponse object, however, it is being utilized somewhere to natively unzip the file and provide just the inner json in the response stream. Unfortunately, these metadata tags are written for CloudWatch logs that are written to S3, they're just stored as application/octet-stream. So, I can use that, but it would be nice to be able to see the metadata info."], "usr-3": ["This was fixed as part of the change for thread https://forums.aws.amazon.com/thread.jspa?threadID=249042"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249374&tstart=0", "Title": "500 error when setting S3 event notifications to a FIFO SQS queue?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I tried adding an SQS FIFO queue for file events on a bucket through the Web UI, and I keep getting a nice: We encountered an internal error. Please try again. I checked the XML getting sent along and it's sending along a pretty simple request. <NotificationConfiguration> <QueueConfiguration> <Id>log-mod</Id> <Filter> <S3Key> <FilterRule> <Name>Prefix</Name> <Value>Unprocessed/</Value> </FilterRule> <FilterRule> <Name>Suffix</Name> <Value>json.gz</Value> </FilterRule> </S3Key> </Filter> <Queue>arn:aws:sqs:us-west-2:123456789012:bucket-events.fifo</Queue> <Event>s3:ObjectCreated:-</Event> </QueueConfiguration> </NotificationConfiguration> I selected a non-FIFO queue and it worked fine. Is this a known problem? Edited by: marki on Feb 16, 2017 1:52 PM 'non-SQS' -> 'non-FIFO'", "Answers": {"usr-1": ["FIFO queues are not supported for S3 event notifications. Please see http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-service-compatibility I'll file a request for the error message to be improved and that this limitation be documented on the S3 side as well."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249836&tstart=0", "Title": "File Upload To S3 With New Console", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi There, A functionality I appreciated with the old S3 Console, was the fact I could upload an entire folder to S3. This was extremely handy given that the assets folder for websites I deployed, would have various sub folders for CSS, JS,etc. I love the look and feel of the new console, but from what I can see, I cannot upload an entire folder, only selecting specific files. Maybe I'm missing something to the new upload process, but it would be quite annoying if I've to create all the individual subdirectories I need first and then upload the files individually. Any help and clarification would be very much appreciated!", "Answers": {"usr-1": ["Hi, The new console supports drag and drop of folders in upload for Chrome only. Are you using Chrome or a different browser? Thanks."], "usr-2": ["Ah I see now that the upload is indeed possible. I'm using Chrome, but I think the console is missing some sort of graphical confirmation that would indicate it supports uploading a folder. Like maybe when the folder is over the upload screen, it changes display, like how it did in the old console. I understand that since this is new - probably ironing things out. But thanks for the clarification!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249865&tstart=0", "Title": "How to get back to NEW console?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I was checking out the new S3 console and wanted to compare it to the old one, so I clicked 'Switch Back to Old Console'. Now I am stuck with the old one - how do I get the new one back? Thanks.", "Answers": {"usr-1": ["Hello, If you go to the properties of any bucket and look on the right under storage management you should see a Opt- In link. Hope this helps!"], "usr-2": ["That worked - glad I asked, though - I never would have found it on my own. Thanks."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249874&tstart=0", "Title": "Cannot delete bucket due to IP restricted bucket policy", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi All I've done something stupid but thankfully, only to to an empty bucket. I set a policy to deny everything to everything from everything with a condition of IpAddress xxx.xxx.xxx.xxx. The policy works but now I have no permissions to either remove the policy or delete the bucket. Classic IP restriction rule gone wrong. What should I do? I've tried to delete the bucket using s3curl.pl which doesn't work Many thanks Edited by: DaHilster on Feb 22, 2017 8:09 AM", "Answers": {"usr-1": ["I managed to get out to the internet on the restricted IP so crisis over. One typo when setting this type of bucket policy and you are locked out! One to be very, very careful with"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250270&tstart=0", "Title": "Internal Error While Trying to Access S3 via Console", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I see the below error code when I click on the S3 service through my console. Kindly help. { 'errorCode' : 'InternalError' }", "Answers": {"usr-1": ["Hi, We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. We apologize for the inconvenience caused."], "usr-2": ["Hi, Update at 2:08 PM PST: As of 1:49 PM PST, we are fully recovered for operations for adding new objects in S3, which was our last operation showing a high error rate. The Amazon S3 service is operating normally. Update at 1:12 PM PST: S3 object retrieval, listing and deletion are fully recovered now. We are still working to recover normal operations for adding new objects to S3. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. Again, we apologize for the inconvenience caused. Thanks!!!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250290&tstart=0", "Title": "My Bucket has gone missing!", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "My Bucket https://s3.amazonaws.com/audio.expositorysermons.net is gone. I did not delete it, but it and all it's contents are gone. I have been and am still paying for the storage. How and why did this happen?", "Answers": {"usr-1": ["Hello, Please refer to https://status.aws.amazon.com as there is currently an event in progress for S3. Unfortunately an ETA cannot be provided here unless the status indicates it. Thanks, Matt J"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250425&tstart=0", "Title": "S3 US East (N. Virginia) still down on March 1, 2017?", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, I have several buckets in US East (N. Virginia). All of them seem fine except for one. Is anyone else having this problem? (I have opened a ticket with aws support but wanted to know if I am alone in the world on this.) Jimmy Edited by: jimmy_s_0 on Mar 1, 2017 9:51 AM", "Answers": {"usr-1": ["Nevermind. The logs are from the wrong day. Edited by: vinayan3 on Mar 1, 2017 10:31 AM"], "usr-2": ["I deleted everything from this bucket, then re-added all of it and now it works fine. I had ~50,000 objects in here so maybe that was it?"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251277&tstart=0", "Title": "Enable Paid Metric for S3 Storage Management API", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, for one of my project, i create buckets programmatically but i also need to enable paid metrics but couldn't find a way to do the last one programmatically. Is the Storage Management dashboard the only way or there is an API ? i looked up through the s3 API but couldnt find the solution. Thank for your help.", "Answers": {"usr-1": ["Hi incognito, You can enable enhanced bucket metric programmatically through the PUT Bucket metrics API: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html This API should be available in any of the latest SDKs. -Adam Edited by: adamwataws on Mar 13, 2017 9:04 AM"], "usr-2": ["Thank you for you reply, i was using the nodejs sdk for the method putBucketMetricsConfiguration http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putBucketMetricsConfiguration-property Here is the code let params = { Bucket: 'bucketname', Id: \"metricid\", MetricsConfiguration: { Id: \"metricid\", }, }; s3.putBucketMetricsConfiguration(params, function(err, data) { if (err) callback(err, null); // an error occurred else callback(null, data); // successful response }); The code did add the metric filter but didn't enable paid metric. So i tried 'EntireBucket' as an id, this time the paid metrics got enabled. It took me a while to find out that EntireBucket isn't just a random value but refers to the bucket internally. Thank your for your help, your link saves the day."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251309&tstart=0", "Title": "Escalating costs", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi. I use S3 as a server backup for various domains, email and websites, under my control. I'm happy with the service thank you. I have noticed that my monthly charges increase every month, even though the storage size remains the same. Support tells me:+ 'In my investigation I could see that actually your S3 buckets are keeping the same amount every month and what is increasing is the Data Transfer out usage. The traffic you're seeing may have originated from another customer in another region, causing the inbound traffic, and your instances will have responded with response packets, resulting in outgoing data transfer. It doesn't imply that your instance initiated this traffic with another region; rather, your instance is responding to requests coming to it from another region.'+ They advise that I ensure my security groups are configured to block unsolicited inbound traffic. -My bucket policy is: { 'Version': '2008-10-17', 'Id': 'Policy11111111111111', 'Statement': [ { 'Sid': 'Stmt111111111', 'Effect': 'Allow', 'Principal': { 'AWS': 'arn:aws:iam::111111111111:root' }, 'Action': [ 's3:GetBucketAcl', 's3:GetBucketPolicy' ], 'Resource': 'arn:aws:s3:::my_bucket' }, { 'Sid': 'Stmt1111111111', 'Effect': 'Allow', 'Principal': { 'AWS': 'arn:aws:iam::111111111111:root' }, 'Action': 's3:PutObject', 'Resource': 'arn:aws:s3:::my_bucket/-' } ] } - What do I have to do to block unsolicited inbound traffic? Any advice appreciated :)) Ted", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251304&tstart=0", "Title": "Cleaning Amazon S3 Bucket Every 30 Days", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I have read the official blog post but cannot figure out how to actually configure this service. What I would like to do is remove all files that are 30 days old and older. Could someone please guide me step by step or link me to an article that goes step by step with this? I need to make sure I'm not setting this up incorrectly, don't want to delete anything by mistake. Thanks.", "Answers": {"usr-1": ["I think the easiest way to accomplish this would be to enable versioning on the bucket, and then set up a Lifecycle rule to delete all records that were inserted more than 30 days ago. All of this can be setup in the web management console."], "usr-2": ["Hello, Any article to show how to set this up? Sounds simple, but when I go to do it, I can't ever seem to figure out how to setup the Lifecycle for objects. Thanks."], "usr-3": ["Hello, Please refer to following links for information on enabling lifecycle configuration on S3 buckets: http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html http://docs.aws.amazon.com/AmazonS3/latest/UG/LifecycleConfiguration.html Best Regards, Abhilasha"], "usr-4": ["Hello, Thanks, I have managed to configure it. Now I will wait to see if it works. Thanks for everyone's help."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250333&tstart=0", "Title": "S3 Outage", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, Have you had an ETA? We need more information if need to go to my DR site. The Service Health dashboard tells us nothing about ETA or outage Regards,", "Answers": {"usr-1": ["+1 to this. AWS - PLEASE update the Service Health dashboard to accurately reflect what is going on and provide an ETA if you can. It is absolutely ridiculous that a service this ubiquitous does not have accurate updates on the dashboard, let alone a ball-park ETA to resolution."], "usr-2": ["Hello, Please refer to https://status.aws.amazon.com for any updates on the current event. Unfortunately an ETA cannot be provided here unless the status indicates it. Thanks, Matt J"], "usr-3": ["Hello, I apologize for the inconvenience and confusion. The information posted at the top of the Service Health Dashboard was the only message that could be initially posted, as the outage was impacting our ability to update the Service Health Dashboard as well. That has since been repaired, and you can see the latest accurate information regarding this outage and any ETAs here: http://status.aws.amazon.com/ We're still working on complete recovery. Regards, Nick"], "usr-4": ["Thank you Nick! My customers are very happy to resume normal operations."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250319&tstart=0", "Title": "S3 API Operations are timing out.", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Not long ago all my S3 API calls started timing out. Is anyone else seeing this? Thanks in advance", "Answers": {"usr-12": ["https://status.aws.amazon.com Amazon is looking into it."], "usr-13": ["we're also experiencing the same issues but not only in us-east-1, but also sa-east-1 and ap-southeast-1 status.aws.amazon.com doesn't give any info about the outage yet"], "usr-10": ["Unable to execute HTTP request: Connect to s3.amazonaws.com:443 http://s3.amazonaws.com/10.188.125.192 failed: connect timed out"], "usr-11": ["{ \"errorCode\" : \"InternalError\" } Same error"], "usr-16": ["Quoted from top of AWS health page (https://status.aws.amazon.com): Increased Error Rates We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue."], "usr-17": ["Hello, We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. We apologize for the inconvenience caused."], "usr-14": ["Same issue. An issue appeared on my AWS Health Dashboard for about a minute. Then it disappeared . Not in Event Log either... That renders AWS Health pretty useless for outages (what else is it useful then?)."], "usr-15": ["Yes, I have the same problem. Even my S3 dashboard returns: \"errorCode\" : \"InternalError\""], "usr-18": ["It has been 1.5 hours now. What is the ETA for a fix, or some sort of recovery?"], "usr-19": ["Still not working. Can't call API. Public files are not accessible. Can access s3 console, but shows 0 buckets 0 regions."], "usr-8": ["Add me to the \"me too list\" entire site is has no images"], "usr-9": ["Like wise. Seems to be affecting all US-EAST-1 region. Was able to access the frontend momentarily and saw the new Dashboard view but all my buckets were gone, when I tried to change to old view I get: { \"errorCode\" : \"InternalError\" }"], "usr-20": ["Hi, Update at 2:08 PM PST: As of 1:49 PM PST, we are fully recovered for operations for adding new objects in S3, which was our last operation showing a high error rate. The Amazon S3 service is operating normally. Update at 1:12 PM PST: S3 object retrieval, listing and deletion are fully recovered now. We are still working to recover normal operations for adding new objects to S3. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. Again, we apologize for the inconvenience caused. Thanks!!!"], "usr-1": ["I'm experiencing the same on my end."], "usr-2": ["I'm getting an InternalError when trying to access the S3 via the web console."], "usr-3": ["Yes, same here. And, trying to view S3 buckets via old S3 console page am receiving \"InternalError\" message."], "usr-4": ["Yes same here."], "usr-5": ["Yes calls to S3 are timing out from our servers in multiple locations."], "usr-6": ["Yup totally broken here as well, public files aren't reachable either. US-East if it matters."], "usr-7": ["{ \"errorCode\" : \"InternalError\" } . when clicking on s3 from the dashboard"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251463&tstart=0", "Title": "Trouble with hosting a static site with route 53 domain", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Dear friends, I've been through the steps on this tutorial multiple times: http://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html I've got to a point where I don't know why its not working. The domain benjaminearl.eu was bought from route 53. I added the site files in one bucket and made another with the sub-domain (www.benjaminearl.eu). Initiated static site hosting and set the sub-domain to redirect to the root. I added the permission to the root. I set up the hosted zone in route 53 with two different records, 1 to the root and the other to the sub-domain. This is my first attempt at setting up a static site using AWS so I could have made a mistake along the way, but I have triple checked through this. If anyone has any idea what I'm doing wrong and how to fix it, I would be extremely thankful! Thanks, ben", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241088&tstart=25", "Title": "S3 PUT event not firing or S3 PUT event fired but target lambda not invoked", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I've found some similar threads of others with this kind of problem ( here and here ), each usually picked up by a helpful Amazonian who moved the issue into PM/outbound case. Hard to pick up from there on what might have been wrong or what we might be able to do about it. I have the same situation though. An S3 upload should be firing a put event which in this case should be triggering a lambda. I have a prefix and suffix, and no erroneous - wildcard characters or anything that. All details look good. I've checked policies in place and again believe I have it all correct. Has worked in the past with what is there. Except it's not working. Not sure even how to tell if it's: A failure of the S3 event to fire A failure of the successfully fired event to invoke the lambda Just a delay of unknown time and no visibility.. I can turn on S3 logging and try again so I will do that in an attempt to prove that the event fired. But even if it did, I've still got two possibilities up there which I think are beyond my powers to check.", "Answers": {"usr-1": ["S3 logging threw up a potential complication. The file is reasonably large: 84Mb or so. So the aws s3 cp command I used to send it to s3 has done it in parts. Here's a bit of S3 log with names/details/paths redacted: 2016-10-13-01-23-46-6BC498BD2494F054:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 7F54842991086462 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=4&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 588563 699 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-23-46-6BC498BD2494F054:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser DE07BD8414887017 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=3&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 477196 44 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-24-29-F55493585D169BB5:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 289E27072615EA46 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=9&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 613931 44 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:34 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser FFC55CD462773B30 REST.POST.UPLOADS the/path/in/s3/to/theFile.txt \"POST /the/path/in/s3/to/theFile.txt?uploads HTTP/1.1\" 200 - 396 - 28 27 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:35 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 894037D4F930207B REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=10&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 827641 76 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:3:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser AB35D94F22B6B7F7 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=7&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 823806 28 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-04-3884E0B191B2426C:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser FC8CC39836DBDEA9 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=2&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 794594 27 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-05-D796648F8D7E7606:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 434CDB2D32DF666A REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=6&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 763831 24 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-05-D796648F8D7E7606:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser CE72DEA1354888F5 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=5&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 788509 35 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-15-0648064B407A4B3E:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 69D76983BA5787F2 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=8&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 829206 271 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-29-57-BF6AC51FDA206167:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 8E2A10EA49A61358 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=1&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 785924 64 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-33-08-8CA4DFB56E8669D0:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:31:38 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 7517302CA82E7714 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=11&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 3613314 105501 108 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - That's 11 REST.PUT.PART entries but no clear completion/finale to the put or sign the event fired ? Would that event log ?"], "usr-2": ["I just did another test which told me a lot. I made an empty .txt file and uploaded it. The S3 put event is configured with a prefix for a directory and a suffix of '.txt' in other words the lambda should be invoked for every txt file going into that directory on the S3 bucket. For the 0 byte empty file, the lambda was invoked immediately. For the 84 Mb text file which we're hoping the lambda will process...nothing so far. This means that I'm confident my event is configured right, and that my lambda's permissions are good. I believe I have some kind of infrastructure issue preventing this S3 event from invoking the lambda on the large file. Is there a file size limit I need to be aware of ? Perhaps it's the part breakdown by the S3 upload ?"], "usr-3": ["I made a support request about that one and support indicated that the aws s3 cp command via the CLI does a multipart upload. I think that's a good thing for reliability of a large file transfer...but it does mean I needed my trigger to react to \"ObjectCreated:CompleteMultipartUpload\" rather than (or as well as) \"ObjectCreated:Put\". Seems so obvious now ! Anyway, sharing that tip here in case it helps anybody else. Thank you Austin@AWS."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241204&tstart=25", "Title": "Accessing S3 Bucket from Another Account", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hello, I am attempting to access an S3 Bucket that's in my Dev Account from my Prod Account using IAM Roles. I have a server role on the Instance in the Prod account and a policy to allow the role to access the S3 Bucket in the Dev Account. I then have a bucket policy allowing the Prod 'IAM root' access to the bucket. When I login to the instance and attempt to curl or use AWS cli I get Access Denied. I've checked to make sure the instance was getting Credentials and sure enough it is. I'm a bit lost on where else to go to check. Please any help would be greatly appreciated! Here is my IAM Policy in my Prod Account attached to my role: Show Policy { 'Version' : '2012-10-17' , 'Statement' : [ { 'Sid' : 'Stmt1476454325248' , 'Action' : [ 's3:Get-' ], 'Effect' : 'Allow' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } , { 'Sid' : 'Stmt1476454375685' , 'Action' : [ 's3:List-' ], 'Effect' : 'Allow' , 'Resource' : 'arn:aws:s3:::my-bucket/' } ] } Here is my S3 bucket Policy in my Dev Account: { 'Version' : '2012-10-17' , 'Statement' : [ { 'Effect' : 'Allow' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::my-account-number:root' ] } , 'Action' : 's3:Get-' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } , { 'Effect' : 'Allow' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::my-account:root' ] } , 'Action' : 's3:List-' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } ] }", "Answers": {"usr-1": ["Nvm. I figured it out."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241175&tstart=25", "Title": "Can't access to bucket from root account", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I added this policy to one of our buckets, and now I can't get access even from root. How I can remove or reset this policy? { 'Sid' : 'Access-to-specific-VPC-only' , 'Action' : 's3:-' , 'Effect' : 'Deny' , 'Resource' : [ 'arn:aws:s3:::examplebucket' , 'arn:aws:s3:::examplebucket/-' ], 'Condition' : { 'StringNotEquals' : { 'aws:sourceVpc' : 'vpc---------' } } , 'Principal' : '-' }", "Answers": {"usr-1": ["I solved this issue by myself"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241229&tstart=25", "Title": "Files disappearing from S3 bucket", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Here are links to four files that I uploaded in the last week, but have now disappeared from my bucket: https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-naypyidaw.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-london.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-brussels.jpg https://s3.amazonaws.com/gh-resource/ghImage/SWjqzgXy9rGCYvpRF-ottawa.jpg I know they successfully uploaded because I saw them on my website multiple times before they disappeared. The last file above (ottawa), I just now re-uploaded, so that I could look at the permissions and see if there was an expiry date or expiry rule. Permissions give everyone read/download permission and all permissions to myself. Expiry date is None, expiry rule is N/A. This has been happening regularly for the last year or so. What could be causing this?", "Answers": {"usr-1": ["OK, I got some good advice on StackOverflow. If you ever have this same experience that I had, enable logging on your bucket. Then you can see who's deleting the files and when they are being deleted. In my case, I enabled the logging, but then never even had to look at the log to figure out the problem. Since I was the only one with access to the files, I knew it had to be me. And it was, some old code that I meant to go back and fix, but never did. Once I realized that it had to be me, I was able to find the problem. Lesson learned -- files with no expiry date and no expiry rule don't just disappear. If they seem to, then there's more to the story."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241310&tstart=25", "Title": "AWS CLI make bucket: data must be a byte string", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "AWS CLI: creating a new bucket in ap-southeast-2 always returns error: data must be a byte string . Same command works perfectly fine for other regions. Any idea why I can not create S3 buckets in Sydney? # aws s3 mb s3: //test-1476747511 --region ap-southeast-2 make_bucket failed: s3: //test-1476747511/ data must be a byte string # aws s3 mb s3: //test-1476747511 --region us-east-1 make_bucket: s3: //test-1476747511/ # aws --region us-east-1 s3 ls | grep test 2016-10-18 10:48:07 test-1476747511 # aws --version aws-cli/1.7.18 Python/2.7.9 Linux/3.16.0-4-amd64 \u00a0 # dpkg -s python-openssl | grep 'Version' Version: 0.14-1 Any suggestions welcome", "Answers": {"usr-1": ["...and update to the latest version of awscli fixed the issue."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244872&tstart=25", "Title": "Can't access S3 bucket as root user or IAM user with AdministratorAccess", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I can't view the S3 bucket as root user or IAM user with AdministratorAccess. I'm getting the following error from AWS management console or using AWS CLI. From Management Cosole : Sorry! You do not have permissions to view this bucket Using AWS CLI : An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I can't access the bucket even using root account or any IAM account with AWS Managed Policy (AdministratorAccess) Please help resolving this. Edited by: rbandi on Dec 12, 2016 5:54 PM", "Answers": {"usr-1": ["The following documentation link shows - as the root user, you can still access the bucket by modifying the bucket policy to allow root user access. But I can't access the bucket policy. When I click on properties, I get an error saying \" Sorry! You do not have permissions to view this bucket\". http://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html Can someone from Amazon please help?"], "usr-2": ["Answering my own question. I was able to resolve this issue with new s3 console. With new s3 console, I can modify the bucket policy with root account. I couldn't do the same with the old s3 console."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246051&tstart=25", "Title": "Old redirect won't go away", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Good news: I changed my S3 static website redirection rules this morning and it took effect immediately! Bad news: I made a mistake, corrected it right away, but I can't get the correction to take effect! Its been an hour already and no change. Is there any way to force the new rules into effect?", "Answers": {"usr-1": ["Tech support advised me to invalidate my CloudFront cache, and I did, and it worked!"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246201&tstart=25", "Title": "Regionless Bucket cannot be deleted", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "To whom it may concern, I have an S3 bucket that I am trying to delete, but I cannot because it doesn't have an associated region. When logged into the console, when I click on the bucket I see: Error retrieving bucket region Bucket: bmp-order-api-dev-serverlessdeploymentbucket-1m75vmy8aoo3p Please see the attached screenshot. Can you please delete this bucket? It's causing a number of AWS toolkits to fail horribly ;-/ Thank you! ~ Matthew", "Answers": {"usr-1": ["As of today the Bucket has been destroyed without me doing anything. Hopefully, that was intentional"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246396&tstart=25", "Title": "Amazon S3 Access & Secret Keys", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have just now created an Amazon S3 Account and created a unique BUCKET in storage management. Then using a Synology NAS I attempted to set up a New Hyper Backup Task > Data Backup > Amazon S3 Service, and get an 'authentication error' when entering my Access and Secret Keys. In reading Amazon's support docs I am understanding that my Access Key is simply my Amazon User Account (Email) in the form of User@Domain.com and the Secret Key is simply my password. Am I missing something here? Is there a time delay in when the account is created until it is replicated and usable on the S3 server? Are there any specific PORTS needing to be opened in my firewall for allowing the authentication to complete successfully? I am currently using: Synology DS412+ Current DSM version: DSM 6.0.2-8451 DSM date: 2016/08/17 With Hyper Backup Version: 2.0.1-0384 Any help would be GREATLY appreciated! Thanks! Michael", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246864&tstart=25", "Title": "Inventory function stopped working", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Has anyone else had any issues with the new S3 Inventory function in the last couple days? I have several daily inventory jobs setup and the last ones fired on Jan 8. I haven't touched the inventory jobs, bucket permissions, or anything related in about a month.", "Answers": {"usr-1": ["Thank you for your comment. The issue with the S3 Inventory list was fixed. If you have further questions, we would like to hear more details on your issue. You can provide us more information by opening a case https://aws.amazon.com/contact-us/ and titling it, \"Object Catalog, Jan 9\" so we may reference it to this post."]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244814&tstart=25", "Title": "All of sudden my code failed to write to S3", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I have the following Java code: public boolean writeToS3(String regionName, String key, byte[] contentAsBytes) { if (environment.equals('stage') || environment.equals('prods')) { AmazonS3 s3client = new AmazonS3Client(new InstanceProfileCredentialsProvider()); Region region = Region.getRegion(Regions.fromName(regionName)); s3client.setRegion(region); ByteArrayInputStream contentsAsStream = new ByteArrayInputStream(contentAsBytes); ObjectMetadata md = new ObjectMetadata(); md.setContentLength(contentAsBytes.length); PutObjectRequest putRequest = new PutObjectRequest(bucketName, key, contentsAsStream, md); int times = FlightEventBackupServiceImpl.maxGetTimes; do { try { s3client.putObject(putRequest); // error from this line } catch (AmazonClientException ace) { if (times == 1) { LOGGER.error('failed to write to S2', ace); return false; } } } while (--times > 0); } return true; } It worked fine for several weeks. Starting yesterday or today it stopped working and I got the following error: 11 Dec 2016 21:40:06,141 ERROR: com.gogoair.flighteventsaving.business.S3ServiceImpl - failed to write to S2 com.amazonaws.AmazonClientException: Data read has a different length than the expected: dataLength=0; expectedLength=1631 ; includeSkipped=false; in.getClass()=class com.amazonaws.internal.ReleasableInputStream; markedS upported=true; marked=0; resetSinceLastMarked=false; markCount=1; resetCount=0 at com.amazonaws.util.LengthCheckInputStream.checkLength(LengthCheckInputStream.java:150) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:110) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream.read(MD5DigestCalculatingInputStream.java:98) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:151) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at org.apache.http.entity.InputStreamEntity.writeTo(InputStreamEntity.java:142) at com.amazonaws.http.RepeatableInputStreamRequestEntity.writeTo(RepeatableInputStreamRequestEntity.java:153) at org.apache.http.entity.HttpEntityWrapper.writeTo(HttpEntityWrapper.java:96) at org.apache.http.impl.client.EntityEnclosingRequestWrapper$EntityWrapper.writeTo(EntityEnclosingRequestWrapper.java:112) at org.apache.http.impl.entity.EntitySerializer.serialize(EntitySerializer.java:117) at org.apache.http.impl.AbstractHttpClientConnection.sendRequestEntity(AbstractHttpClientConnection.java:265) at org.apache.http.impl.conn.ManagedClientConnectionImpl.sendRequestEntity(ManagedClientConnectionImpl.java:203) at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doSendRequest(SdkHttpRequestExecutor.java:63) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:685) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:487) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55) at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:843) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:597) at com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:363) at com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:329) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:308) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3659) at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1422) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:65) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:80) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.process(FlightEventBackupServiceImpl.java:134) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.backup(FlightEventBackupServiceImpl.java:95) at sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:108) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:745) Of course, each time the expectedLenth is different, but dataLength is always 0. Did AWS changed something on S3 recently? Thanks a lot! Daniel Edited by: danli33 on Dec 11, 2016 1:56 PM =========== By the way, the same code still works on SOX environment, but not work in stage. Edited by: danli33 on Dec 12, 2016 7:18 AM Edited by: danli33 on Dec 12, 2016 8:32 AM Edited by: danli33 on Dec 12, 2016 9:42 AM My bad. I should have add 'return true;' after uploading the data.", "Answers": {}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247444&tstart=25", "Title": "Authorization header is invalid -- one and only one ' ' (space) required", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "Hi, We have a publicly accessible bucket with the following policy { 'Version': '2008-10-17', 'Statement': [ { 'Sid': 'AllowPublicRead', 'Effect': 'Allow', 'Principal': { 'AWS': '-' }, 'Action': 's3:GetObject', 'Resource': 'arn:aws:s3:::xxxxxxxx/-' } ] } Until now, the images in the bucket was accessible via the browser. But today we get the following error 'Authorization header is invalid -- one and only one ' ' (space) required' The weird thing is if I open the same link in Chrome Incognito Mode, image loads perfectly fine. Any idea why? Thank you", "Answers": {"usr-1": ["Never mind, one of my chrome plugin was additing Authorisation header to my requests"]}, "awsTag": "S3"},
{"sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247636&tstart=25", "Title": "\"ConnectionError\" uploading large file to S3 via AWS CLI", "crawled": true, "dateScraped": "2017-04-05-21-59", "Question": "I am having problems uploading a 4.7 Gb file to my S3 bucket using the AWS CLI ('aws s3 cp myfile s3://mybucket --region us-east-1') from a Windows 10 workstation. It is persistently failing with either of these two errors: 'ConnectionError: ('Connection aborted.', error(10053, 'An established connection was aborted by the software in your host machine')) 'ConnectionError: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host')) The amount of time it takes before failing varies wildly from run to run. I have enabled debugging ('--debug') and I see a scary amount of the above throughout the duration of the command, as well as many other negative-sounding messages, such as: botocore.awsrequest - DEBUG - Waiting for 100 Continue response. botocore.awsrequest - DEBUG - No response seen from server, continuing to send the response body. botocore.endpoint - DEBUG - ConnectionError received when sending HTTP request. Traceback (most recent call last): File 'botocore\\endpoint.pyc' , line 204, in _get_response File 'botocore\\vendored\\requests\\sessions.pyc' , line 573, in send File 'botocore\\vendored\\requests\\adapters.pyc' , line 415, in send botocore.vendored.requests.packages.urllib3.connectionpool - INFO - Resetting dropped connection: mybucket.s3.amazonaws.com botocore.hooks - DEBUG - Event needs-retry.s3.UploadPart: calling handler <botocore.retryHandler object at 0x0330B190> botocore.retryHandler - DEBUG - retry needed, retryable exception caught: ( 'Connection aborted.' , error(10053, 'An established connection was aborted by the software in your host machine' )) Traceback (most recent call last): File 'botocore.retryHandler.pyc' , line 269, in _should_retry File 'botocore.retryHandler.pyc' , line 317, in __call__ File 'botocore.retryHandler.pyc' , line 223, in __call__ File 'botocore.retryHandler.pyc' , line 359, in _check_caught_exception Last month I was (eventually) able to upload a 3.9 Gb file after about half a dozen tries, but this time it is just not working. I have tried decreasing the config parameter 'max_concurrent_requests' to '1', but it didn't help (I didn't think it would, but it was worth a try). I have tried decreasing and increasing the config parameter 'multipart_chunksize' to '5MB' and '16MB' respectively, but it didn't help either. According to Windows' Task Manager my system is using 64\\% of its memory (19.4 MB for the AWS CLI), and virtually 0\\% of its Network (0.7 Mbps for the AWS CLI). I work from my home office, so I am using a residential high-speed internet service, but I don't have trouble moving files anywhere else. They may take some time, but they do get there. So I am more inclined to suspect either the backend (S3) or the application (AWS CLI). I've been working with AWS for only a month, so I'm willing to start from the possibility that I'm doing something wrong. Am I doing something wrong? Thanks in advance.", "Answers": {"usr-1": ["The silence is deafening. Please folks, somebody must have some suggestions... Thanks."], "usr-2": ["I have now broken it up into parts myself using \"hjsplit\" (used the default 1,400 Kb part size), initiated a multi-part upload myself using the \"s3api create-multipart-upload\" CLI command, and uploaded each of the 3,488 parts individually using the \"s3api upload-part\" CLI command. As far as I can tell, all of the parts were uploaded successfully. Now when I try running the \"s3api complete-multipart-upload\" CLI command, it simply returns: u'Error' Not helpful or informative at all. When I add \"--debug\" to the \"s3api complete-multipart-upload\" CLI command, I see in the output: <Error><Code>EntityTooSmall</Code><Message>Your proposed upload is smaller than the minimum allowed size</Message><ProposedSize>1433600</ProposedSize><MinSizeAllowed>5242880</MinSizeAllowed><PartNumber>1</PartNumber><ETag>637c6c3d8116f29e59e7f26aebd77fa9</ETag><RequestId>6A0136BD25C350CE</RequestId><HostId>jlaYrmtCAom6FcxUcQ/26MC174X3NHWe2nwKCQDVjQXhvFp4HqjbvO59IuwJTH3rewUHOvyE4Aw=</HostId></Error> My interpretation of that message is that I now have to redo the upload of the individual parts with part size of 6 Mb each. But before I do waste even more of my time (and money) doing that, I would just like to confirm that my interpretation of that error is correct. Can somebody please confirm that for me? Thanks. And in case anyone in AWS Development is reading this (which I highly doubt), it would be really nice of you to tell me that the file part size is too small when I am doing the upload of the part... not after I've spent a whole day (and burning through my data limit) doing uploads that won't work. This continues to be a truly horrible experience."], "usr-3": ["Got it to work by re-splitting my file into chunks larger than the minimum. Thanks for all the help."]}, "awsTag": "S3"}
]