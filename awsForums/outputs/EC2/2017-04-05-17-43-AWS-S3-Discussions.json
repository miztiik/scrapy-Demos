{"Answers": {"usr-1": ["For years, until around the beginning of this month, (around 2016-09-01), I have been able to use a bucket as a virtual hosted-style domain, along with the subdirectories. To make it simple: bucket is www.jazeee-example.com contents: /index.html /some-icon.ico /directory/app.js Starting from scratch, Steps to reproduce: Create a new bucket called www.jazeee-example.com In bucket properties, Open Static Website Hosting Enable website hosting Observe the newly provided URL: http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com Upload the files above, including the directory/app.js As part of upload, set permissions to make everything public for all files. Access the files via URL: curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/index.html - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/some-icon.ico - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/directory/app.js - FAILS As a side note, within s3 management console, The URL for /directory/app.js is https://s3.amazonaws.com/www.jazeee-example.com/directory/app.js Effectively, s3 doesn't seem to work anymore as a virtual hosted-style site, if I have subdirectories as part of my site. Of course, it works if I use the path-style site, but one cannot use the standard documentation for AWS static site hosting unless your website is completely flat. http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Am I doing something wrong? Seems like the default process for static website hosting changed in the last few weeks, and now S3 only hosts root level files via \"virtual hosted-style\" domain name."], "usr-2": ["Looks like this is specific to .js files. I am able to download the same file if renamed to .css curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/css/app.css works"]}, "Question": "For years, until around the beginning of this month, (around 2016-09-01), I have been able to use a bucket as a virtual hosted-style domain, along with the subdirectories. To make it simple: bucket is www.jazeee-example.com contents: /index.html /some-icon.ico /directory/app.js Starting from scratch, Steps to reproduce: Create a new bucket called www.jazeee-example.com In bucket properties, Open Static Website Hosting Enable website hosting Observe the newly provided URL: http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com Upload the files above, including the directory/app.js As part of upload, set permissions to make everything public for all files. Access the files via URL: curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/index.html - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/some-icon.ico - works curl http://www.jazeee-example.com.s3-website-us-east-1.amazonaws.com/directory/app.js - FAILS As a side note, within s3 management console, The URL for /directory/app.js is https://s3.amazonaws.com/www.jazeee-example.com/directory/app.js Effectively, s3 doesn't seem to work anymore as a virtual hosted-style site, if I have subdirectories as part of my site. Of course, it works if I use the path-style site, but one cannot use the standard documentation for AWS static site hosting unless your website is completely flat. http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Am I doing something wrong? Seems like the default process for static website hosting changed in the last few weeks, and now S3 only hosts root level files via 'virtual hosted-style' domain name.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239459&tstart=50"}{"Answers": {"usr-1": ["Nevermind, found a solution. Please delete the thread. Edited by: frontender674 on Oct 9, 2016 1:32 AM"]}, "Question": "Nevermind, found a solution. Please delete the thread. Edited by: frontender674 on Oct 9, 2016 1:32 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240809&tstart=50"}{"Answers": {"usr-1": ["Hello all, I have created a bucket and trying to use from an application and it is giving the following error: \"error: S3ServiceException:The bucket you are attempting to access must be addressed using the specified endpoint.\" I am using this format: s3://bucketname. I know the format is not an issue because I am able to use this format for another public bucket. I think the permissions on my bucket may be an issue but I am not sure. Can someone pl. help? Thank you in advance."], "usr-2": ["I did not include region and hence this error. Now its all good."]}, "Question": "Hello all, I have created a bucket and trying to use from an application and it is giving the following error: 'error: S3ServiceException:The bucket you are attempting to access must be addressed using the specified endpoint.' I am using this format: s3://bucketname. I know the format is not an issue because I am able to use this format for another public bucket. I think the permissions on my bucket may be an issue but I am not sure. Can someone pl. help? Thank you in advance.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248143&tstart=0"}{"Answers": {"usr-1": ["My IAM Role for S3 from my EC2 was working a few months ago but now is not working. I can't trace route to s3.amazonaws.com from this troubled instance. My other instance has no issue using its IAM role to connect with S3. Any idea why it may have stopped working? I tried the network security group settings to allow any IP inbound and outbound but it still does not allow me to connect. I'm not using any S3 Bucket Policies, just IAM Roles. One works and one doesn't. I've tried detaching and re-attaching the full access policy for S3 to the IAM role but it did not help either. I've also tried stopping and starting the instance and it did not help. Has anyone seen anything similar or know what I can try to get it working again?"], "usr-2": ["Looks like adding the default VPC security group solved the issue."]}, "Question": "My IAM Role for S3 from my EC2 was working a few months ago but now is not working. I can't trace route to s3.amazonaws.com from this troubled instance. My other instance has no issue using its IAM role to connect with S3. Any idea why it may have stopped working? I tried the network security group settings to allow any IP inbound and outbound but it still does not allow me to connect. I'm not using any S3 Bucket Policies, just IAM Roles. One works and one doesn't. I've tried detaching and re-attaching the full access policy for S3 to the IAM role but it did not help either. I've also tried stopping and starting the instance and it did not help. Has anyone seen anything similar or know what I can try to get it working again?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248687&tstart=0"}{"Answers": {"usr-1": ["Hello! Following the AWS tutorial mentioned below, I've setup a redirect for my S3 bucket, but when a non-existing object is requested, the request times out after 30s. The bucket and Lambda function are in the same region. What am I doing wrong? Thanks! https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/ <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect> <Protocol>https</Protocol> <HostName>26j5rec0k3.execute-api.us-east-1.amazonaws.com</HostName> <ReplaceKeyPrefixWith>prod/resizeImage?key=</ReplaceKeyPrefixWith> <HttpRedirectCode>307</HttpRedirectCode> </Redirect> </RoutingRule> </RoutingRules>"], "usr-2": ["I was using HTTPS instead of HTTP."]}, "Question": "Hello! Following the AWS tutorial mentioned below, I've setup a redirect for my S3 bucket, but when a non-existing object is requested, the request times out after 30s. The bucket and Lambda function are in the same region. What am I doing wrong? Thanks! https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/ <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect> <Protocol>https</Protocol> <HostName>26j5rec0k3.execute-api.us-east-1.amazonaws.com</HostName> <ReplaceKeyPrefixWith>prod/resizeImage?key=</ReplaceKeyPrefixWith> <HttpRedirectCode>307</HttpRedirectCode> </Redirect> </RoutingRule> </RoutingRules>", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248685&tstart=0"}{"Answers": {"usr-1": ["I am using Boto3 on python to generate presigned urls successfully. s3 = boto3.client('s3') url = s3.generate_presigned_url('get_object', Params={ 'Bucket': bucket, 'Key': key,}) I successfully get a url of the following format: https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId=https://forums.aws.amazon.com/&Expires=1486126711&Signature=https://forums.aws.amazon.com/ The AWSAccessKeyId matches my S3 user's was accesskeyid so looks good. Here is my curl request to upload: curl -v --upload-file example.jpg \"https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId=https://forums.aws.amazon.com/&Expires=1486126711&Signature=https://forums.aws.amazon.com/\" It keeps failing! With the following error: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>&ltAccess Key Removed&gt</AWSAccessKeyId><StringToSign> Edited by: Hammad Ahmed on Feb 3, 2017 4:23 AM"], "usr-2": ["Changed 'get_object' to 'put_object' as I was trying to put on a get pre-signed url."]}, "Question": "I am using Boto3 on python to generate presigned urls successfully. s3 = boto3.client('s3') url = s3.generate_presigned_url('get_object', Params={ 'Bucket': bucket, 'Key': key,}) I successfully get a url of the following format: https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId= https://forums.aws.amazon.com/ andExpires=1486126711andSignature= https://forums.aws.amazon.com/ The AWSAccessKeyId matches my S3 user's was accesskeyid so looks good. Here is my curl request to upload: curl -v --upload-file example.jpg 'https://projectx-dev.s3.amazonaws.com/8/profile.jpeg?AWSAccessKeyId= https://forums.aws.amazon.com/ andExpires=1486126711andSignature= https://forums.aws.amazon.com/ ' It keeps failing! With the following error: <?xml version='1.0' encoding='UTF-8'?> <Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>andltAccess Key Removedandgt</AWSAccessKeyId><StringToSign> Edited by: Hammad Ahmed on Feb 3, 2017 4:23 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248485&tstart=0"}{"Answers": {"usr-1": ["We are getting a 403 error back when we try to add more items to S3 via the API. It was working up to 2000 puts and we are appilcable for the free tier which has a limit of 2000 puts, so I am suspecting that I have to somehow enable usage for more than 2000 puts. How do I do that?"]}, "Question": "We are getting a 403 error back when we try to add more items to S3 via the API. It was working up to 2000 puts and we are appilcable for the free tier which has a limit of 2000 puts, so I am suspecting that I have to somehow enable usage for more than 2000 puts. How do I do that?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249016&tstart=0"}{"Answers": {"usr-1": ["I may have misssed something in the docs, but is there a way to enable the S3 Request Metrics from Cloudformation instead of the AWS CLI?"], "usr-2": ["No, I'm sorry; CloudFormation does not currently support configuring request metrics or the other recently-added S3 features (analytics and inventory). We are investigating this and will update this thread when we have more information available."]}, "Question": "I may have misssed something in the docs, but is there a way to enable the S3 Request Metrics from Cloudformation instead of the AWS CLI?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248782&tstart=0"}{"Answers": {"usr-1": ["Hi, we're currently in eu-west-1 and consider an additional setup in the US. An important question is if there are still differences regarding the S3 consistency model, depending on the US region we choose. There is a blog post (8/2015) saying that for the US Standard (us-east-1) region, read-after-write consistency can only be expected through the endpoint s3-external-1.amazonaws.com: https://instrumentalapp.com/blog/why-you-should-stop-using-the-us-standard-region-in-s3-right-now/ An \"expired\" forum post (7/2015) from AWS has the same statement: https://forums.aws.amazon.com/ann.jspa?annID=3112 An AWS blog post (8/2015) does not mention any exception regarding endpoints: https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-s3-introduces-new-usability-enhancements/ Searching in the current AWS docs, I also don't find this exception any more: http://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel https://aws.amazon.com/s3/faqs/?nc1=h_ls"], "usr-2": ["We do provide Read-after-write consistency in us-east-1 now and this happened at some point last year. Please be mindful there is still no Read-after-update (sometimes) and this is consistent with all the regions."], "usr-3": ["Alright, thanks! That's the answer I hoped for."], "usr-4": ["Can you clarify if this means (for region us-east-1) 1. Yes, read-after-write consistency is supported only if you use urls that begin with s3-external-1.amazonaws.com or 2. Yes, read-after-write consistency is supported on s3.amazonaws.com urls"], "usr-5": ["S3 is Read-after-Write consistent in all regions and doesn't matter what endpoint you use. Hope this helps."]}, "Question": "Hi, we're currently in eu-west-1 and consider an additional setup in the US. An important question is if there are still differences regarding the S3 consistency model, depending on the US region we choose. There is a blog post (8/2015) saying that for the US Standard (us-east-1) region, read-after-write consistency can only be expected through the endpoint s3-external-1.amazonaws.com: https://instrumentalapp.com/blog/why-you-should-stop-using-the-us-standard-region-in-s3-right-now/ An 'expired' forum post (7/2015) from AWS has the same statement: https://forums.aws.amazon.com/ann.jspa?annID=3112 An AWS blog post (8/2015) does not mention any exception regarding endpoints: https://aws.amazon.com/about-aws/whats-new/2015/08/amazon-s3-introduces-new-usability-enhancements/ Searching in the current AWS docs, I also don't find this exception any more: http://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel https://aws.amazon.com/s3/faqs/?nc1=h_ls", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249007&tstart=0"}{"Answers": {"usr-1": ["Hi, I hope I am missing something simple here. I have created a static website with S3 and successfully assigned my subdomain cdn.bbcw.org to the bucket. According to one tutorial I read, the bucket name should match the subdomain - so my bucket is named cdn.bbcw.org (and I moved the 6+ million files from my old bucket over to it). I created a Certificate for cdn.bbcw.org and validated it, then a CloudFront distribution assigned to that bucket with an CNAME alternate name cdn.bbcw.org. For the time being I used HHTP or HTTPS but I just now changed it to Redirect HTTP to HTTPS. Either way, accessing https://cdn.bbcw.org gives me a timeout message. Perhaps it's related that accessing https://cdn.bbcw.org.s3-website-us-west-1.amazonaws.com also gives a timeout (accessing via HTTP works fine both on my custom domain and the AWS endpoint). Can someone tell me a step I might have missed? I have read things about problems with a period in the bucket name - but I had a similar problem before I created a new bucket. As a bonus question - Am I charged for moving the millions of files to another bucket in the same region? I see there are no data transfer charges but will this constitute a PUT, COPY, POST, or LIST request? I suppose it does since each directory must be listed during the move process?"], "usr-2": ["I must have been following one of the numerous inaccurate or out-dated tutorials. The simple solution was the point my Route 53 to the CloudFront distribution URL rather than the s3 website endpoint. I was not aware that the basic address https://cdn.bbcw.org would give an Access Denied error while the actual files would work. My bad - it is in the Amazon documentation table here: http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteEndpoints.html"]}, "Question": "Hi, I hope I am missing something simple here. I have created a static website with S3 and successfully assigned my subdomain cdn.bbcw.org to the bucket. According to one tutorial I read, the bucket name should match the subdomain - so my bucket is named cdn.bbcw.org (and I moved the 6+ million files from my old bucket over to it). I created a Certificate for cdn.bbcw.org and validated it, then a CloudFront distribution assigned to that bucket with an CNAME alternate name cdn.bbcw.org. For the time being I used HHTP or HTTPS but I just now changed it to Redirect HTTP to HTTPS. Either way, accessing https://cdn.bbcw.org gives me a timeout message. Perhaps it's related that accessing https://cdn.bbcw.org.s3-website-us-west-1.amazonaws.com also gives a timeout (accessing via HTTP works fine both on my custom domain and the AWS endpoint). Can someone tell me a step I might have missed? I have read things about problems with a period in the bucket name - but I had a similar problem before I created a new bucket. As a bonus question - Am I charged for moving the millions of files to another bucket in the same region? I see there are no data transfer charges but will this constitute a PUT, COPY, POST, or LIST request? I suppose it does since each directory must be listed during the move process?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249090&tstart=0"}{"Answers": {"usr-1": ["Hello, i'm looking at consistently poor download performance from several hosts whilst pulling s3-ia-objects from the frankfurt s3 region, e.g. ~ 10-15 Mb/s from 3 different endpoints with the same ISP. It is affecting multiple buckets and their objects, like this one: http://s3.eu-central-1.amazonaws.com/4f48caf1d8bcbef8/c5b38f8b3625d2b6/zerofile.raw This 100MB file has been downloaded with FreeBSD's fetch command on FreeBSD 11.0-RELEASE, but curl and wget give the same result. traceroute -P TCP s3.eu-central-1.amazonaws.com gives: traceroute to s3.eu-central-1.amazonaws.com (52.219.73.20), 64 hops max, 40 byte packets 1 10.0.32.1 (10.0.32.1) 0.964 ms 0.904 ms 0.958 ms 2 129.196-4-62.wifi-dyn.isp.proximus.be (62.4.196.129) 18.967 ms 18.906 ms 18.916 ms 3 ae-83-100.iarstr4.isp.belgacom.be (91.183.242.128) 22.175 ms 21.891 ms 21.922 ms 4 ae-13-1000.ibrstr5.isp.belgacom.be (91.183.246.112) 22.397 ms 22.427 ms 22.157 ms 5 * * * 6 * 94.102.160.37 (94.102.160.37) 22.350 ms 22.221 ms 7 * 94.102.162.35 (94.102.162.35) 26.933 ms 26.701 ms 8 195.66.225.175 (195.66.225.175) 26.683 ms 27.572 ms 26.965 ms 9 * * * 10 * * * 11 * * * 12 * * * 13 * * * 14 54.239.4.216 (54.239.4.216) 38.632 ms 54.239.5.134 (54.239.5.134) 41.842 ms 54.239.5.174 (54.239.5.174) 39.015 ms 15 * * * .. and gives timeouts including 64th hop. N.B: BSD traceroute as included in OSX and FreeBSD has this in the bugs section of the manpage: When using protocols other than UDP, functionality is reduced. In particular, the last packet will often appear to be lost, because even though it reaches the destination host, there's no way to know that because no ICMP message is sent back. In the TCP case, traceroute should listen for a RST from the destination host (or an intermediate router that's filtering packets), but this is not implemented yet. mtr --report s3.eu-central-1.amazonaws.com gives: Start: Mon Feb 13 23:29:14 2017 HOST: restore Loss% Snt Last Avg Best Wrst StDev 1.|-- 10.0.32.1 0.0% 10 1.0 1.1 1.0 1.2 0.0 2.|-- 129.196-4-62.wifi-dyn.isp 0.0% 10 19.5 19.3 19.0 19.5 0.0 3.|-- ae-83-100.iarstr4.isp.bel 0.0% 10 22.7 22.4 21.8 24.0 0.6 4.|-- ae-13-1000.ibrstr5.isp.be 0.0% 10 22.3 22.1 21.8 22.4 0.0 5.|-- 80.84.23.40 80.0% 10 22.5 22.5 22.5 22.5 0.0 6.|-- 94.102.160.37 80.0% 10 22.5 22.7 22.5 22.9 0.0 7.|-- 94.102.162.35 40.0% 10 27.2 27.5 27.1 28.9 0.6 8.|-- 195.66.225.175 0.0% 10 26.6 27.3 26.6 28.0 0.0 9.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 10.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 11.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 12.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 13.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 14.|-- 54.239.4.216 0.0% 5 40.8 40.7 40.5 40.8 0.0 15.|-- ??? 100.0 4 0.0 0.0 0.0 0.0 0.0 These performance issues have not been observed whilst downloading to a ec2-instance within the same frankfurt region or whilst downloading from several other providers like Azure or GCE from the same hosts, with or without ssl. Also these hosts have no problem downloading from a ec2 instance in the frankfurt region. Any thoughts?"], "usr-2": ["Hello, the issue is resolved now. The problem appears that S3 does not send TCP Timestamps and as a result, FreeBSDs TCP Window Scaling mechanism fails to increase the receive buffer size accordingly, therefore resulting in lower troughput over higher latency links. Edited by: sydneymeyer on Feb 16, 2017 2:34 PM"]}, "Question": "Hello, i'm looking at consistently poor download performance from several hosts whilst pulling s3-ia-objects from the frankfurt s3 region, e.g. ~ 10-15 Mb/s from 3 different endpoints with the same ISP. It is affecting multiple buckets and their objects, like this one: http://s3.eu-central-1.amazonaws.com/4f48caf1d8bcbef8/c5b38f8b3625d2b6/zerofile.raw This 100MB file has been downloaded with FreeBSD's fetch command on FreeBSD 11.0-RELEASE, but curl and wget give the same result. traceroute -P TCP s3.eu-central-1.amazonaws.com gives: traceroute to s3.eu-central-1.amazonaws.com (52.219.73.20), 64 hops max, 40 byte packets 1 10.0.32.1 (10.0.32.1) 0.964 ms 0.904 ms 0.958 ms 2 129.196-4-62.wifi-dyn.isp.proximus.be (62.4.196.129) 18.967 ms 18.906 ms 18.916 ms 3 ae-83-100.iarstr4.isp.belgacom.be (91.183.242.128) 22.175 ms 21.891 ms 21.922 ms 4 ae-13-1000.ibrstr5.isp.belgacom.be (91.183.246.112) 22.397 ms 22.427 ms 22.157 ms 5 - - - 6 - 94.102.160.37 (94.102.160.37) 22.350 ms 22.221 ms 7 - 94.102.162.35 (94.102.162.35) 26.933 ms 26.701 ms 8 195.66.225.175 (195.66.225.175) 26.683 ms 27.572 ms 26.965 ms 9 - - - 10 - - - 11 - - - 12 - - - 13 - - - 14 54.239.4.216 (54.239.4.216) 38.632 ms 54.239.5.134 (54.239.5.134) 41.842 ms 54.239.5.174 (54.239.5.174) 39.015 ms 15 - - - .. and gives timeouts including 64th hop. N.B: BSD traceroute as included in OSX and FreeBSD has this in the bugs section of the manpage: When using protocols other than UDP, functionality is reduced. In particular, the last packet will often appear to be lost, because even though it reaches the destination host, there's no way to know that because no ICMP message is sent back. In the TCP case, traceroute should listen for a RST from the destination host (or an intermediate router that's filtering packets), but this is not implemented yet. mtr --report s3.eu-central-1.amazonaws.com gives: Start: Mon Feb 13 23:29:14 2017 HOST: restore Loss\\% Snt Last Avg Best Wrst StDev 1.|-- 10.0.32.1 0.0\\% 10 1.0 1.1 1.0 1.2 0.0 2.|-- 129.196-4-62.wifi-dyn.isp 0.0\\% 10 19.5 19.3 19.0 19.5 0.0 3.|-- ae-83-100.iarstr4.isp.bel 0.0\\% 10 22.7 22.4 21.8 24.0 0.6 4.|-- ae-13-1000.ibrstr5.isp.be 0.0\\% 10 22.3 22.1 21.8 22.4 0.0 5.|-- 80.84.23.40 80.0\\% 10 22.5 22.5 22.5 22.5 0.0 6.|-- 94.102.160.37 80.0\\% 10 22.5 22.7 22.5 22.9 0.0 7.|-- 94.102.162.35 40.0\\% 10 27.2 27.5 27.1 28.9 0.6 8.|-- 195.66.225.175 0.0\\% 10 26.6 27.3 26.6 28.0 0.0 9.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 10.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 11.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 12.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 13.|-- ??? 100.0 10 0.0 0.0 0.0 0.0 0.0 14.|-- 54.239.4.216 0.0\\% 5 40.8 40.7 40.5 40.8 0.0 15.|-- ??? 100.0 4 0.0 0.0 0.0 0.0 0.0 These performance issues have not been observed whilst downloading to a ec2-instance within the same frankfurt region or whilst downloading from several other providers like Azure or GCE from the same hosts, with or without ssl. Also these hosts have no problem downloading from a ec2 instance in the frankfurt region. Any thoughts?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249185&tstart=0"}{"Answers": {"usr-1": ["I added \"x-amz-server-side-encryption\", \"AES256\" to the form data posted to S3. Before I added that, uploading files was working smoothly. Then adding that, I started getting a 403 (Forbidden) response. I don't see in AWS's docs anywhere that there is a special bucket policy or configuration necessary to use this parameter. I also tried the suggested bucket policy for denying requests that DON'T request SSE, but as expected, that disallowed non-encryption requests, but still didn't allow encryption ones (ones that have that parameter). Is there missing info in the docs for making requests that specify SSE?"], "usr-2": ["Answering my own question: I was using the form POST method of uploading a file to S3. I forgot that it requires a policy document in the post that can contain certain restrictions. The policy document being sent needed to allow encryption (`{\"x-amz-server-side-encryption\": \"AES256\"}` as one of my conditions)"]}, "Question": "I added 'x-amz-server-side-encryption', 'AES256' to the form data posted to S3. Before I added that, uploading files was working smoothly. Then adding that, I started getting a 403 (Forbidden) response. I don't see in AWS's docs anywhere that there is a special bucket policy or configuration necessary to use this parameter. I also tried the suggested bucket policy for denying requests that DON'T request SSE, but as expected, that disallowed non-encryption requests, but still didn't allow encryption ones (ones that have that parameter). Is there missing info in the docs for making requests that specify SSE?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=248968&tstart=0"}{"Answers": {"usr-1": ["I'm using the .NET SDK to do a GetObjectStreamAsync() from an encrypted CloudTrail log written to S3. The log itself is stored as a json.gz file, however the response stream is the actual json file. I assume this is because these objects are stored with a Content-Encoding : gzip metadata entry. However, when you perform a GetObjectMetadataAsync() call, only the Content-Type header is returned, the Content-Encoding value in the headers is null. I would also expect to see these in the Response.Metadata collection, but only user defined metadata is returned there. When downloading different files written from CloudTrail and CloudWatch Logs, not being able to identify if the Stream from the GetObject response is actually the json file (as in CloudTrail) or the gzip file (as in CloudWatch Logs because the Content-Type is application/octet-stream) makes it difficult to parse these. Is there any way to retrieve the Content-Encoding metadata header without knowing what file I'm getting ahead of time? I'd like to be able to know if I need to unzip the file after downloading it or if I can read it directly. Unfortunately, even the json content from a CloudTrail log when written to a file are marked as Archive, so I can't check that to be able to tell, and .gz (and no extension gz files) aren't marked as compressed."], "usr-2": ["How are these \"encrypted CloudTrail log\" files written to S3? GetObjectMetadataAsync will return Content-Encoding if it is set on the object, so it sounds like the objects just don't have this header set correctly. Do the files have other header values like Content-Type which could help to decide how to treat the file? Of course, if all else fails, you may just need to try unzipping the file, and if that fails then try to read it as JSON."], "usr-3": ["The logs are being delivered directly from CloudTrail to S3. Please see the attached screen shots. You can see that the metadata is set, but it is not included in the GetObjectMetadataResponse object, however, it is being utilized somewhere to natively unzip the file and provide just the inner json in the response stream. Unfortunately, these metadata tags are written for CloudWatch logs that are written to S3, they're just stored as application/octet-stream. So, I can use that, but it would be nice to be able to see the metadata info."], "usr-4": ["This was fixed as part of the change for thread https://forums.aws.amazon.com/thread.jspa?threadID=249042"]}, "Question": "I'm using the .NET SDK to do a GetObjectStreamAsync() from an encrypted CloudTrail log written to S3. The log itself is stored as a json.gz file, however the response stream is the actual json file. I assume this is because these objects are stored with a Content-Encoding : gzip metadata entry. However, when you perform a GetObjectMetadataAsync() call, only the Content-Type header is returned, the Content-Encoding value in the headers is null. I would also expect to see these in the Response.Metadata collection, but only user defined metadata is returned there. When downloading different files written from CloudTrail and CloudWatch Logs, not being able to identify if the Stream from the GetObject response is actually the json file (as in CloudTrail) or the gzip file (as in CloudWatch Logs because the Content-Type is application/octet-stream) makes it difficult to parse these. Is there any way to retrieve the Content-Encoding metadata header without knowing what file I'm getting ahead of time? I'd like to be able to know if I need to unzip the file after downloading it or if I can read it directly. Unfortunately, even the json content from a CloudTrail log when written to a file are marked as Archive, so I can't check that to be able to tell, and .gz (and no extension gz files) aren't marked as compressed.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-43", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249363&tstart=0"}