{"Answers": {"usr-1": ["Dear friends, I've been through the steps on this tutorial multiple times: http://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html I've got to a point where I don't know why its not working. The domain benjaminearl.eu was bought from route 53. I added the site files in one bucket and made another with the sub-domain (www.benjaminearl.eu). Initiated static site hosting and set the sub-domain to redirect to the root. I added the permission to the root. I set up the hosted zone in route 53 with two different records, 1 to the root and the other to the sub-domain. This is my first attempt at setting up a static site using AWS so I could have made a mistake along the way, but I have triple checked through this. If anyone has any idea what I'm doing wrong and how to fix it, I would be extremely thankful! Thanks, ben"]}, "Question": "Dear friends, I've been through the steps on this tutorial multiple times: http://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html I've got to a point where I don't know why its not working. The domain benjaminearl.eu was bought from route 53. I added the site files in one bucket and made another with the sub-domain (www.benjaminearl.eu). Initiated static site hosting and set the sub-domain to redirect to the root. I added the permission to the root. I set up the hosted zone in route 53 with two different records, 1 to the root and the other to the sub-domain. This is my first attempt at setting up a static site using AWS so I could have made a mistake along the way, but I have triple checked through this. If anyone has any idea what I'm doing wrong and how to fix it, I would be extremely thankful! Thanks, ben", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251463&tstart=0"}{"Answers": {"usr-1": ["I tried adding an SQS FIFO queue for file events on a bucket through the Web UI, and I keep getting a nice: We encountered an internal error. Please try again. I checked the XML getting sent along and it's sending along a pretty simple request. <NotificationConfiguration> <QueueConfiguration> <Id>log-mod</Id> <Filter> <S3Key> <FilterRule> <Name>Prefix</Name> <Value>Unprocessed/</Value> </FilterRule> <FilterRule> <Name>Suffix</Name> <Value>json.gz</Value> </FilterRule> </S3Key> </Filter> <Queue>arn:aws:sqs:us-west-2:123456789012:bucket-events.fifo</Queue> <Event>s3:ObjectCreated:*</Event> </QueueConfiguration> </NotificationConfiguration> I selected a non-FIFO queue and it worked fine. Is this a known problem? Edited by: marki on Feb 16, 2017 1:52 PM \"non-SQS\" -> \"non-FIFO\""], "usr-2": ["FIFO queues are not supported for S3 event notifications. Please see http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html#FIFO-service-compatibility I'll file a request for the error message to be improved and that this limitation be documented on the S3 side as well."]}, "Question": "I tried adding an SQS FIFO queue for file events on a bucket through the Web UI, and I keep getting a nice: We encountered an internal error. Please try again. I checked the XML getting sent along and it's sending along a pretty simple request. <NotificationConfiguration> <QueueConfiguration> <Id>log-mod</Id> <Filter> <S3Key> <FilterRule> <Name>Prefix</Name> <Value>Unprocessed/</Value> </FilterRule> <FilterRule> <Name>Suffix</Name> <Value>json.gz</Value> </FilterRule> </S3Key> </Filter> <Queue>arn:aws:sqs:us-west-2:123456789012:bucket-events.fifo</Queue> <Event>s3:ObjectCreated:-</Event> </QueueConfiguration> </NotificationConfiguration> I selected a non-FIFO queue and it worked fine. Is this a known problem? Edited by: marki on Feb 16, 2017 1:52 PM 'non-SQS' -> 'non-FIFO'", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249374&tstart=0"}{"Answers": {"usr-1": ["Hi, I see the below error code when I click on the S3 service through my console. Kindly help. { \"errorCode\" : \"InternalError\" }"], "usr-2": ["Hi, We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. We apologize for the inconvenience caused."], "usr-3": ["Hi, Update at 2:08 PM PST: As of 1:49 PM PST, we are fully recovered for operations for adding new objects in S3, which was our last operation showing a high error rate. The Amazon S3 service is operating normally. Update at 1:12 PM PST: S3 object retrieval, listing and deletion are fully recovered now. We are still working to recover normal operations for adding new objects to S3. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. Again, we apologize for the inconvenience caused. Thanks!!!"]}, "Question": "Hi, I see the below error code when I click on the S3 service through my console. Kindly help. { 'errorCode' : 'InternalError' }", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250270&tstart=0"}{"Answers": {"usr-1": ["My Bucket https://s3.amazonaws.com/audio.expositorysermons.net is gone. I did not delete it, but it and all it's contents are gone. I have been and am still paying for the storage. How and why did this happen?"], "usr-2": ["Hello, Please refer to https://status.aws.amazon.com as there is currently an event in progress for S3. Unfortunately an ETA cannot be provided here unless the status indicates it. Thanks, Matt J"]}, "Question": "My Bucket https://s3.amazonaws.com/audio.expositorysermons.net is gone. I did not delete it, but it and all it's contents are gone. I have been and am still paying for the storage. How and why did this happen?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250290&tstart=0"}{"Answers": {"usr-1": ["Hi, I have several buckets in US East (N. Virginia). All of them seem fine except for one. Is anyone else having this problem? (I have opened a ticket with aws support but wanted to know if I am alone in the world on this.) Jimmy Edited by: jimmy_s_0 on Mar 1, 2017 9:51 AM"], "usr-2": ["Nevermind. The logs are from the wrong day. Edited by: vinayan3 on Mar 1, 2017 10:31 AM"], "usr-3": ["I deleted everything from this bucket, then re-added all of it and now it works fine. I had ~50,000 objects in here so maybe that was it?"]}, "Question": "Hi, I have several buckets in US East (N. Virginia). All of them seem fine except for one. Is anyone else having this problem? (I have opened a ticket with aws support but wanted to know if I am alone in the world on this.) Jimmy Edited by: jimmy_s_0 on Mar 1, 2017 9:51 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250425&tstart=0"}{"Answers": {"usr-1": ["Hi, for one of my project, i create buckets programmatically but i also need to enable paid metrics but couldn't find a way to do the last one programmatically. Is the Storage Management dashboard the only way or there is an API ? i looked up through the s3 API but couldnt find the solution. Thank for your help."], "usr-2": ["Hi incognito, You can enable enhanced bucket metric programmatically through the PUT Bucket metrics API: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html This API should be available in any of the latest SDKs. -Adam Edited by: adamwataws on Mar 13, 2017 9:04 AM"], "usr-3": ["Thank you for you reply, i was using the nodejs sdk for the method putBucketMetricsConfiguration http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putBucketMetricsConfiguration-property Here is the code let params = { Bucket: 'bucketname', Id: \"metricid\", MetricsConfiguration: { Id: \"metricid\", }, }; s3.putBucketMetricsConfiguration(params, function(err, data) { if (err) callback(err, null); // an error occurred else callback(null, data); // successful response }); The code did add the metric filter but didn't enable paid metric. So i tried 'EntireBucket' as an id, this time the paid metrics got enabled. It took me a while to find out that EntireBucket isn't just a random value but refers to the bucket internally. Thank your for your help, your link saves the day."]}, "Question": "Hi, for one of my project, i create buckets programmatically but i also need to enable paid metrics but couldn't find a way to do the last one programmatically. Is the Storage Management dashboard the only way or there is an API ? i looked up through the s3 API but couldnt find the solution. Thank for your help.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251277&tstart=0"}{"Answers": {"usr-1": ["Hi. I use S3 as a server backup for various domains, email and websites, under my control. I'm happy with the service thank you. I have noticed that my monthly charges increase every month, even though the storage size remains the same. Support tells me:+ \"In my investigation I could see that actually your S3 buckets are keeping the same amount every month and what is increasing is the Data Transfer out usage. The traffic you're seeing may have originated from another customer in another region, causing the inbound traffic, and your instances will have responded with response packets, resulting in outgoing data transfer. It doesn't imply that your instance initiated this traffic with another region; rather, your instance is responding to requests coming to it from another region.\"+ They advise that I ensure my security groups are configured to block unsolicited inbound traffic. *My bucket policy is: { \"Version\": \"2008-10-17\", \"Id\": \"Policy11111111111111\", \"Statement\": [ { \"Sid\": \"Stmt111111111\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111111111111:root\" }, \"Action\": [ \"s3:GetBucketAcl\", \"s3:GetBucketPolicy\" ], \"Resource\": \"arn:aws:s3:::my_bucket\" }, { \"Sid\": \"Stmt1111111111\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111111111111:root\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::my_bucket/*\" } ] } * What do I have to do to block unsolicited inbound traffic? Any advice appreciated :)) Ted"]}, "Question": "Hi. I use S3 as a server backup for various domains, email and websites, under my control. I'm happy with the service thank you. I have noticed that my monthly charges increase every month, even though the storage size remains the same. Support tells me:+ 'In my investigation I could see that actually your S3 buckets are keeping the same amount every month and what is increasing is the Data Transfer out usage. The traffic you're seeing may have originated from another customer in another region, causing the inbound traffic, and your instances will have responded with response packets, resulting in outgoing data transfer. It doesn't imply that your instance initiated this traffic with another region; rather, your instance is responding to requests coming to it from another region.'+ They advise that I ensure my security groups are configured to block unsolicited inbound traffic. -My bucket policy is: { 'Version': '2008-10-17', 'Id': 'Policy11111111111111', 'Statement': [ { 'Sid': 'Stmt111111111', 'Effect': 'Allow', 'Principal': { 'AWS': 'arn:aws:iam::111111111111:root' }, 'Action': [ 's3:GetBucketAcl', 's3:GetBucketPolicy' ], 'Resource': 'arn:aws:s3:::my_bucket' }, { 'Sid': 'Stmt1111111111', 'Effect': 'Allow', 'Principal': { 'AWS': 'arn:aws:iam::111111111111:root' }, 'Action': 's3:PutObject', 'Resource': 'arn:aws:s3:::my_bucket/-' } ] } - What do I have to do to block unsolicited inbound traffic? Any advice appreciated :)) Ted", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251309&tstart=0"}{"Answers": {"usr-1": ["Hello, I have read the official blog post but cannot figure out how to actually configure this service. What I would like to do is remove all files that are 30 days old and older. Could someone please guide me step by step or link me to an article that goes step by step with this? I need to make sure I'm not setting this up incorrectly, don't want to delete anything by mistake. Thanks."], "usr-2": ["I think the easiest way to accomplish this would be to enable versioning on the bucket, and then set up a Lifecycle rule to delete all records that were inserted more than 30 days ago. All of this can be setup in the web management console."], "usr-3": ["Hello, Any article to show how to set this up? Sounds simple, but when I go to do it, I can't ever seem to figure out how to setup the Lifecycle for objects. Thanks."], "usr-4": ["Hello, Please refer to following links for information on enabling lifecycle configuration on S3 buckets: http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html http://docs.aws.amazon.com/AmazonS3/latest/UG/LifecycleConfiguration.html Best Regards, Abhilasha"], "usr-5": ["Hello, Thanks, I have managed to configure it. Now I will wait to see if it works. Thanks for everyone's help."]}, "Question": "Hello, I have read the official blog post but cannot figure out how to actually configure this service. What I would like to do is remove all files that are 30 days old and older. Could someone please guide me step by step or link me to an article that goes step by step with this? I need to make sure I'm not setting this up incorrectly, don't want to delete anything by mistake. Thanks.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=251304&tstart=0"}{"Answers": {"usr-1": ["Hi All I've done something stupid but thankfully, only to to an empty bucket. I set a policy to deny everything to everything from everything with a condition of IpAddress xxx.xxx.xxx.xxx. The policy works but now I have no permissions to either remove the policy or delete the bucket. Classic IP restriction rule gone wrong. What should I do? I've tried to delete the bucket using s3curl.pl which doesn't work Many thanks Edited by: DaHilster on Feb 22, 2017 8:09 AM"], "usr-2": ["I managed to get out to the internet on the restricted IP so crisis over. One typo when setting this type of bucket policy and you are locked out! One to be very, very careful with"]}, "Question": "Hi All I've done something stupid but thankfully, only to to an empty bucket. I set a policy to deny everything to everything from everything with a condition of IpAddress xxx.xxx.xxx.xxx. The policy works but now I have no permissions to either remove the policy or delete the bucket. Classic IP restriction rule gone wrong. What should I do? I've tried to delete the bucket using s3curl.pl which doesn't work Many thanks Edited by: DaHilster on Feb 22, 2017 8:09 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249874&tstart=0"}{"Answers": {"usr-1": ["Hi, Have you had an ETA? We need more information if need to go to my DR site. The Service Health dashboard tells us nothing about ETA or outage Regards,"], "usr-2": ["+1 to this. AWS - PLEASE update the Service Health dashboard to accurately reflect what is going on and provide an ETA if you can. It is absolutely ridiculous that a service this ubiquitous does not have accurate updates on the dashboard, let alone a ball-park ETA to resolution."], "usr-3": ["Hello, Please refer to https://status.aws.amazon.com for any updates on the current event. Unfortunately an ETA cannot be provided here unless the status indicates it. Thanks, Matt J"], "usr-4": ["Hello, I apologize for the inconvenience and confusion. The information posted at the top of the Service Health Dashboard was the only message that could be initially posted, as the outage was impacting our ability to update the Service Health Dashboard as well. That has since been repaired, and you can see the latest accurate information regarding this outage and any ETAs here: http://status.aws.amazon.com/ We're still working on complete recovery. Regards, Nick"], "usr-5": ["Thank you Nick! My customers are very happy to resume normal operations."]}, "Question": "Hi, Have you had an ETA? We need more information if need to go to my DR site. The Service Health dashboard tells us nothing about ETA or outage Regards,", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250333&tstart=0"}{"Answers": {"usr-1": ["I've applied below S3 bucket policy to our bucket : { \"Version\": \"2012-10-17\", \"Id\": \"Policy1415115909152\", \"Statement\": [ { \"Sid\": \"Access-to-specific-VPCE-only\", \"Principal\": \"*\", \"Action\": \"s3:*\", \"Effect\": \"Deny\", \"Resource\": [\"arn:aws:s3:::bucket-name\", \"arn:aws:s3:::bucket-name/*\"], \"Condition\": { \"StringNotEquals\": { \"aws:sourceVpce\": \"vpce-1a2b3c4d\" } } } ] } The problem is that sourceVpce does not exist, therefore I can't neither access the bucke nor change the policy no matter what IAM privileges I have. Is there a way to re-gain control over that bucket?"], "usr-2": ["In order for it to be removed, we had to have the root user login and remove it by CLI. Edited by: akreveve on Jan 30, 2017 1:55 PM"]}, "Question": "I've applied below S3 bucket policy to our bucket : { 'Version': '2012-10-17', 'Id': 'Policy1415115909152', 'Statement': [ { 'Sid': 'Access-to-specific-VPCE-only', 'Principal': '-', 'Action': 's3:-', 'Effect': 'Deny', 'Resource': ['arn:aws:s3:::bucket-name', 'arn:aws:s3:::bucket-name/-'], 'Condition': { 'StringNotEquals': { 'aws:sourceVpce': 'vpce-1a2b3c4d' } } } ] } The problem is that sourceVpce does not exist, therefore I can't neither access the bucke nor change the policy no matter what IAM privileges I have. Is there a way to re-gain control over that bucket?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247998&tstart=0"}{"Answers": {"usr-1": ["I was checking out the new S3 console and wanted to compare it to the old one, so I clicked 'Switch Back to Old Console'. Now I am stuck with the old one - how do I get the new one back? Thanks."], "usr-2": ["Hello, If you go to the properties of any bucket and look on the right under storage management you should see a Opt- In link. Hope this helps!"], "usr-3": ["That worked - glad I asked, though - I never would have found it on my own. Thanks."]}, "Question": "I was checking out the new S3 console and wanted to compare it to the old one, so I clicked 'Switch Back to Old Console'. Now I am stuck with the old one - how do I get the new one back? Thanks.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249865&tstart=0"}{"Answers": {"usr-1": ["I'm getting different results for s3client.ListObjects() in Golang vs the equivalent AWS cli operation. In particular (with the Landsat public data), aws s3 ls s3://landsat-pds/L8/139/ will list all of the 'folders' under that path. Using the Go interface, I only get the index.html entry. The Golang example (gist) does the following: params := &s3.ListObjectsInput{ Bucket: aws.String(bucket), Prefix: aws.String(prefix), Delimiter: aws.String(\"/\"), } \u00a0 resp, err := s3client.ListObjects(params) ... for _, key := range resp.Contents { fmt.Printf(\"%+v\\n\\n\", *key) } \u00a0 Results in the single entry: { ETag: \"\\\"2c889e724e44d3bd7a002371417816d4\\\"\", Key: \"L8/139/index.html\", LastModified: 2015-06-01 18:27:04 +0000 UTC, Owner: { DisplayName: \"landsat-pds\", ID: \"f5f9bd0f2036a9a809aae8e2bfdc5a70ad6bf696695739b71efc1cf5ccdc2718\" }, Size: 3264, StorageClass: \"STANDARD\" } Am I missing an option on the S3 request? Or is there a bucket setting I'm missing? Thanks!"], "usr-2": ["It turns out the in the ListObjectsOutput struct there's a CommonPrefixes array, and the 'folders' are being returned in that field. Not the Contents field as I thought. I've updated the gist with code that works. Edited by: EricBlood on Oct 11, 2016 11:11 AM"]}, "Question": "I'm getting different results for s3client.ListObjects() in Golang vs the equivalent AWS cli operation. In particular (with the Landsat public data), aws s3 ls s3: //landsat-pds/L8/139/ will list all of the 'folders' under that path. Using the Go interface, I only get the index.html entry. The Golang example ( gist ) does the following: params := ands3.ListObjectsInput { Bucket: aws.String(bucket), Prefix: aws.String(prefix), Delimiter: aws.String( '/' ), } \u00a0 resp, err := s3client.ListObjects(params) ... for _, key := range resp.Contents { fmt.Printf( '\\%+v\\n\\n' , -key) } \u00a0 Results in the single entry: { ETag: '\\'2c889e724e44d3bd7a002371417816d4\\'' , Key: 'L8/139/index.html' , LastModified: 2015-06-01 18:27:04 +0000 UTC, Owner: { DisplayName: 'landsat-pds' , ID: 'f5f9bd0f2036a9a809aae8e2bfdc5a70ad6bf696695739b71efc1cf5ccdc2718' } , Size: 3264, StorageClass: 'STANDARD' } Am I missing an option on the S3 request? Or is there a bucket setting I'm missing? Thanks!", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240937&tstart=50"}{"Answers": {"usr-1": ["I've found some similar threads of others with this kind of problem ( here & here ), each usually picked up by a helpful Amazonian who moved the issue into PM/outbound case. Hard to pick up from there on what might have been wrong or what we might be able to do about it. I have the same situation though. An S3 upload should be firing a put event which in this case should be triggering a lambda. I have a prefix and suffix, and no erroneous * wildcard characters or anything that. All details look good. I've checked policies in place and again believe I have it all correct. Has worked in the past with what is there. Except it's not working. Not sure even how to tell if it's: A failure of the S3 event to fire A failure of the successfully fired event to invoke the lambda Just a delay of unknown time and no visibility.. I can turn on S3 logging and try again so I will do that in an attempt to prove that the event fired. But even if it did, I've still got two possibilities up there which I think are beyond my powers to check."], "usr-2": ["S3 logging threw up a potential complication. The file is reasonably large: 84Mb or so. So the aws s3 cp command I used to send it to s3 has done it in parts. Here's a bit of S3 log with names/details/paths redacted: 2016-10-13-01-23-46-6BC498BD2494F054:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 7F54842991086462 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=4&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 588563 699 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-23-46-6BC498BD2494F054:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser DE07BD8414887017 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=3&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 477196 44 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-24-29-F55493585D169BB5:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 289E27072615EA46 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=9&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 613931 44 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:34 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser FFC55CD462773B30 REST.POST.UPLOADS the/path/in/s3/to/theFile.txt \"POST /the/path/in/s3/to/theFile.txt?uploads HTTP/1.1\" 200 - 396 - 28 27 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:35 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 894037D4F930207B REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=10&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 827641 76 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-25-20-5FBE41C7B8B66E7B:3:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser AB35D94F22B6B7F7 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=7&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 823806 28 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-04-3884E0B191B2426C:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser FC8CC39836DBDEA9 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=2&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 794594 27 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-05-D796648F8D7E7606:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 434CDB2D32DF666A REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=6&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 763831 24 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-05-D796648F8D7E7606:2:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser CE72DEA1354888F5 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=5&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 788509 35 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-28-15-0648064B407A4B3E:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 69D76983BA5787F2 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=8&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 829206 271 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-29-57-BF6AC51FDA206167:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:23:37 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 8E2A10EA49A61358 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=1&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 8388608 785924 64 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - 2016-10-13-01-33-08-8CA4DFB56E8669D0:1:4fd6e298d8e82a81ec225a06d03a21dbfa92ecd8a9789d632f18f07a97bd1fcb the-bucket-name [13/Oct/2016:00:31:38 +0000] 255.255.255.255 arn:aws:iam::123456789012:user/myAwsCliUser 7517302CA82E7714 REST.PUT.PART the/path/in/s3/to/theFile.txt \"PUT /the/path/in/s3/to/theFile.txt?partNumber=11&uploadId=ZraRSqlB0mpzNnuA8KqZSDYcgeWJqGvQQ39JtBSJDJTOd76C4e28eNBp8asTzW3gVkkTuHRRn0n9ZKsflAhGR30XmBI7lwZ_O..L8O7GYGd5MiMkPKzOKFf1yhUiK_Bw HTTP/1.1\" 200 - - 3613314 105501 108 \"-\" \"aws-cli/1.10.47 Python/2.7.10 Darwin/15.6.0 botocore/1.4.37\" - That's 11 REST.PUT.PART entries but no clear completion/finale to the put or sign the event fired ? Would that event log ?"], "usr-3": ["I just did another test which told me a lot. I made an empty .txt file and uploaded it. The S3 put event is configured with a prefix for a directory and a suffix of '.txt' in other words the lambda should be invoked for every txt file going into that directory on the S3 bucket. For the 0 byte empty file, the lambda was invoked immediately. For the 84 Mb text file which we're hoping the lambda will process...nothing so far. This means that I'm confident my event is configured right, and that my lambda's permissions are good. I believe I have some kind of infrastructure issue preventing this S3 event from invoking the lambda on the large file. Is there a file size limit I need to be aware of ? Perhaps it's the part breakdown by the S3 upload ?"], "usr-4": ["I made a support request about that one and support indicated that the aws s3 cp command via the CLI does a multipart upload. I think that's a good thing for reliability of a large file transfer...but it does mean I needed my trigger to react to \"ObjectCreated:CompleteMultipartUpload\" rather than (or as well as) \"ObjectCreated:Put\". Seems so obvious now ! Anyway, sharing that tip here in case it helps anybody else. Thank you Austin@AWS."]}, "Question": "I've found some similar threads of others with this kind of problem ( here and here ), each usually picked up by a helpful Amazonian who moved the issue into PM/outbound case. Hard to pick up from there on what might have been wrong or what we might be able to do about it. I have the same situation though. An S3 upload should be firing a put event which in this case should be triggering a lambda. I have a prefix and suffix, and no erroneous - wildcard characters or anything that. All details look good. I've checked policies in place and again believe I have it all correct. Has worked in the past with what is there. Except it's not working. Not sure even how to tell if it's: A failure of the S3 event to fire A failure of the successfully fired event to invoke the lambda Just a delay of unknown time and no visibility.. I can turn on S3 logging and try again so I will do that in an attempt to prove that the event fired. But even if it did, I've still got two possibilities up there which I think are beyond my powers to check.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241088&tstart=25"}{"Answers": {"usr-1": ["Ok, little about what I'm attempting to do: 1) Script the archival of backup files and upload to S3 bucket using the aws CLI. 2) Encryption: Be able to have the backup file easily encrypted/decrypted server-side (preferable, as I'd rather not have to maintain keys and extra code). eg. --sse flags via aws CLI 3) Have the S3 bucket replicate to another region via Cross-Region Replication. A backup for my backup for my backup. Need to meet those options and additionally, this needs to remain simple as it needs to scale. I've managed these things individually but have encountered issues with the cross-region-replication due to the encryption. For encryption, I've first attempted creating & using a kms key, which is pretty standard and how I generally encrypt/decrypt my S3 files. This works great, my backup file gets encrypted server-side with simple flag \"--sse aws:kms --sse-kms-key-id blahblahmy-encryption-key\" Only problem is any files encrypted this way don't show up in my replicated S3 bucket.. The requirements for S3 Cross-Region-Replication state that client & server side encrypted content is ignored for cross-region-replication: \"Objects created with server-side encryption using either customer-provided (SSE-C) or AWS KMS\u2013managed encryption (SSE-KMS) keys are not replicated. \" From here I'm not sure where to go. S3 Replication docs seem to suggest to encrypt using the \"S3 master key\", for replication. However, I don't know for sure what that is or how to specify via the AWS CLI.. Nor do I know what control I have over that key and how secure or any management options for it etc. I've tried changing my aws s3 cp flag to \"--sse aws:kms\". It successfully uploaded my backup file with details specifying \"Server Side Encryption:Using AWS KMS master key: aws/s3 (default)\", which would seem to be correct? However, it was not replicated to my other region's bucket so I'm unsure. I'm sure there are solid reasons but seems silly to have limitations on Cross-Region-Replication like that. Any ideas on how to approach this? I just want a simple encrypted backup which I have, but my other caveat is I must have multiple backups and preferrably with zero additional / minimal effort.."], "usr-2": ["Is it possible at all? Anyone, bueller?"], "usr-3": ["S3 cross region replication currently supports replication of SSE-S3 objects (described at the following link: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html) We suggest using SSE-S3 as the encryption method and cross region replication will replicate those objects. Cross Region Replication does not currently support replication for KMS encrypted S3 objects. We have recorded Cross Region Replication for KMS objects as a feature request."], "usr-4": ["Thanks abhilasha, I think I was having issues utilising SSE-S3 as I was trying to specify it via the s3 cli. If I'm not mistaken I needed to use \"s3api\" eg. aws s3api put-object --bucket xx --key xx --body xx --server-side-encryption AES256 Eitherway, my scripted backup process is now working nicely with cross-region-replication! Cross-Region-Replication support for SSE-KMS or SSE-C encrypted bucket objects would certainly get my +1 as a feature request though! Edited by: ninjada on May 25, 2016 6:31 PM"]}, "Question": "Ok, little about what I'm attempting to do: 1) Script the archival of backup files and upload to S3 bucket using the aws CLI. 2) Encryption: Be able to have the backup file easily encrypted/decrypted server-side (preferable, as I'd rather not have to maintain keys and extra code). eg. --sse flags via aws CLI 3) Have the S3 bucket replicate to another region via Cross-Region Replication. A backup for my backup for my backup. Need to meet those options and additionally, this needs to remain simple as it needs to scale. I've managed these things individually but have encountered issues with the cross-region-replication due to the encryption. For encryption, I've first attempted creating and using a kms key, which is pretty standard and how I generally encrypt/decrypt my S3 files. This works great, my backup file gets encrypted server-side with simple flag '--sse aws:kms --sse-kms-key-id blahblahmy-encryption-key' Only problem is any files encrypted this way don't show up in my replicated S3 bucket.. The requirements for S3 Cross-Region-Replication state that client and server side encrypted content is ignored for cross-region-replication: 'Objects created with server-side encryption using either customer-provided (SSE-C) or AWS KMS-managed encryption (SSE-KMS) keys are not replicated. ' From here I'm not sure where to go. S3 Replication docs seem to suggest to encrypt using the 'S3 master key', for replication. However, I don't know for sure what that is or how to specify via the AWS CLI.. Nor do I know what control I have over that key and how secure or any management options for it etc. I've tried changing my aws s3 cp flag to '--sse aws:kms'. It successfully uploaded my backup file with details specifying 'Server Side Encryption:Using AWS KMS master key: aws/s3 (default)', which would seem to be correct? However, it was not replicated to my other region's bucket so I'm unsure. I'm sure there are solid reasons but seems silly to have limitations on Cross-Region-Replication like that. Any ideas on how to approach this? I just want a simple encrypted backup which I have, but my other caveat is I must have multiple backups and preferrably with zero additional / minimal effort..", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231276&tstart=75"}{"Answers": {"usr-1": ["Hi There, A functionality I appreciated with the old S3 Console, was the fact I could upload an entire folder to S3. This was extremely handy given that the assets folder for websites I deployed, would have various sub folders for CSS, JS,etc. I love the look and feel of the new console, but from what I can see, I cannot upload an entire folder, only selecting specific files. Maybe I'm missing something to the new upload process, but it would be quite annoying if I've to create all the individual subdirectories I need first and then upload the files individually. Any help and clarification would be very much appreciated!"], "usr-2": ["Hi, The new console supports drag and drop of folders in upload for Chrome only. Are you using Chrome or a different browser? Thanks."], "usr-3": ["Ah I see now that the upload is indeed possible. I'm using Chrome, but I think the console is missing some sort of graphical confirmation that would indicate it supports uploading a folder. Like maybe when the folder is over the upload screen, it changes display, like how it did in the old console. I understand that since this is new - probably ironing things out. But thanks for the clarification!"]}, "Question": "Hi There, A functionality I appreciated with the old S3 Console, was the fact I could upload an entire folder to S3. This was extremely handy given that the assets folder for websites I deployed, would have various sub folders for CSS, JS,etc. I love the look and feel of the new console, but from what I can see, I cannot upload an entire folder, only selecting specific files. Maybe I'm missing something to the new upload process, but it would be quite annoying if I've to create all the individual subdirectories I need first and then upload the files individually. Any help and clarification would be very much appreciated!", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=249836&tstart=0"}{"Answers": {"usr-1": ["Hi everyone, It works fine with AWS iOS SDK version 2.4.0, only get 403 when using 2.4.2 (for IPv6). I use Cognito Pool ID in Tokyo, S3 bucket in Singapore. Already tried create S3 Bucket in Tokyo to test but have the same problem. Here is the code: func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -> Bool { let credentialsProvider = AWSCognitoCredentialsProvider(regionType:.APNortheast1, identityPoolId: AppConfig.cognitoPoolID) let configuration = AWSServiceConfiguration(region:.APSoutheast1, credentialsProvider:credentialsProvider) AWSServiceManager.defaultServiceManager().defaultServiceConfiguration = configuration return true } \u00a0 \u00a0 func upload(dataToUpload: NSData) { \u00a0 let transferUtility = AWSS3TransferUtility.defaultS3TransferUtility() let uploadExpression = AWSS3TransferUtilityUploadExpression() uploadExpression.progressBlock = { (task: AWSS3TransferUtilityTask, currentProgress: NSProgress) -> Void in print(currentProgress.fractionCompleted) } transferUtility.uploadData(dataToUpload, bucket: AppConfig.s3bucketName, key: \"temp/\" + file.fileName, contentType: \"image/jpeg\", expression: uploadExpression, completionHander: { (uploadTask, error) -> Void in print(\"error\", error) print(\"uploadTask.request\", uploadTask.request) print(\"uploadTask.response\", uploadTask.response) }) } Here is the response header: { status code: 403, headers { Connection = close; \"Content-Type\" = \"application/xml\"; Date = \"Tue, 24 May 2016 15:15:24 GMT\"; Server = AmazonS3; \"Transfer-Encoding\" = Identity; \"x-amz-id-2\" = \"a8FZ4jEh11pXg0Wmg/Fbz1RdOYDu2px139/IeFx51zl+W/AzBjXK942jBj5Bqo5RbskTp6idLUU=\"; \"x-amz-request-id\" = 54D217A6616C0188; } } Bucket policy { \"Sid\": \"Stmt1464065918500\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\" ], \"Resource\": \"arn:aws:s3:::bucket_name/temp/*\" } Edited by: ngothanhtai on May 24, 2016 9:48 AM Edited by: ngothanhtai on May 24, 2016 9:48 AM"], "usr-2": ["Here is the answer: https://github.com/aws/aws-sdk-ios/pull/389"]}, "Question": "Hi everyone, It works fine with AWS iOS SDK version 2.4.0, only get 403 when using 2.4.2 (for IPv6). I use Cognito Pool ID in Tokyo, S3 bucket in Singapore. Already tried create S3 Bucket in Tokyo to test but have the same problem. Here is the code: func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -> Bool { let credentialsProvider = AWSCognitoCredentialsProvider(regionType:.APNortheast1, identityPoolId: AppConfig.cognitoPoolID) let configuration = AWSServiceConfiguration(region:.APSoutheast1, credentialsProvider:credentialsProvider) AWSServiceManager.defaultServiceManager().defaultServiceConfiguration = configuration return true } \u00a0 \u00a0 func upload(dataToUpload: NSData) { \u00a0 let transferUtility = AWSS3TransferUtility.defaultS3TransferUtility() let uploadExpression = AWSS3TransferUtilityUploadExpression() uploadExpression.progressBlock = { (task: AWSS3TransferUtilityTask, currentProgress: NSProgress) -> Void in print(currentProgress.fractionCompleted) } transferUtility.uploadData(dataToUpload, bucket: AppConfig.s3bucketName, key: 'temp/' + file.fileName, contentType: 'image/jpeg' , expression: uploadExpression, completionHander: { (uploadTask, error) -> Void in print( 'error' , error) print( 'uploadTask.request' , uploadTask.request) print( 'uploadTask.response' , uploadTask.response) } ) } Here is the response header: { status code: 403, headers { Connection = close; 'Content-Type' = 'application/xml' ; Date = 'Tue, 24 May 2016 15:15:24 GMT' ; Server = AmazonS3; 'Transfer-Encoding' = Identity; 'x-amz-id-2' = 'a8FZ4jEh11pXg0Wmg/Fbz1RdOYDu2px139/IeFx51zl+W/AzBjXK942jBj5Bqo5RbskTp6idLUU=' ; 'x-amz-request-id' = 54D217A6616C0188; } } Bucket policy { 'Sid' : 'Stmt1464065918500' , 'Effect' : 'Allow' , 'Principal' : '-' , 'Action' : [ 's3:GetObject' , 's3:PutObject' ], 'Resource' : 'arn:aws:s3:::bucket_name/temp/-' } Edited by: ngothanhtai on May 24, 2016 9:48 AM Edited by: ngothanhtai on May 24, 2016 9:48 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=232244&tstart=75"}{"Answers": {"usr-1": ["My questions are as follows: Why doesn't my CNAME work and Do I need Route 53 or is it making things worse? I am trying to get the subdomain studentaccess.cincinnatisymphony.org to resolve to this bucket: http://studentaccess.cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I already had issues with my current domain registrar. I had to have them manually insert the full AWS S3 address into the Name Record because it was too long for their online form. Because of this, they made the mistake of leaving off the studentaccess part from that AWS URL as the host. So Network Solutions has the CNAME record as such: Alias: studentaccess.cincinnatisymphony.org Other Host: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com They told me they fixed it and I'm waiting for a data cleanse for it to all go through, but in the mean time, the url they made up: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I was able to create a bucket there hoping to get some kind of content to show up. Nothing works. I also have studentaccess.cincinnatisymphony.org set up in Route 53 to point to the correct S3 bucket through an A record. This may be unnecessary, I don't know. I have other subdomains pointing to EC2 servers, but that was way too much power for what we need here. Help? Edited by: cso-snow on Jun 10, 2016 7:42 AM"], "usr-2": ["it seems to be working for me."], "usr-3": ["It is all working for me now. I did not need Route 53 at all. My DNS provider finally spelled everything correctly. The biggest problems I had was an internal firewall issue with the subdomain not resolving in house, but working outside. Checked with a free proxy server."]}, "Question": "My questions are as follows: Why doesn't my CNAME work and Do I need Route 53 or is it making things worse? I am trying to get the subdomain studentaccess.cincinnatisymphony.org to resolve to this bucket: http://studentaccess.cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I already had issues with my current domain registrar. I had to have them manually insert the full AWS S3 address into the Name Record because it was too long for their online form. Because of this, they made the mistake of leaving off the studentaccess part from that AWS URL as the host. So Network Solutions has the CNAME record as such: Alias: studentaccess.cincinnatisymphony.org Other Host: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com They told me they fixed it and I'm waiting for a data cleanse for it to all go through, but in the mean time, the url they made up: http://cincinnatisymphony.org.s3-website-us-east-1.amazonaws.com/ I was able to create a bucket there hoping to get some kind of content to show up. Nothing works. I also have studentaccess.cincinnatisymphony.org set up in Route 53 to point to the correct S3 bucket through an A record. This may be unnecessary, I don't know. I have other subdomains pointing to EC2 servers, but that was way too much power for what we need here. Help? Edited by: cso-snow on Jun 10, 2016 7:42 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233308&tstart=75"}{"Answers": {"usr-1": ["I have just started trying out the S3 service. I created a single bucket (using an administrator level IAM user) and I uploaded a single image file. I then enabled logging for that bucket and when I checked the log \"folder\" a little while later, I found that it was filled up with \"access denied\" logs. During the day, these logs were coming in on average almost one per minute, and continued through the night though at a reduced rate. Has anyone else experienced this? Other than browsing to that image myself a couple of times, and also connecting to it through a web application (using the access keys of a restricted IAM user) there should be no one and no service trying to access that bucket. Thank you in advance for any insight."]}, "Question": "I have just started trying out the S3 service. I created a single bucket (using an administrator level IAM user) and I uploaded a single image file. I then enabled logging for that bucket and when I checked the log 'folder' a little while later, I found that it was filled up with 'access denied' logs. During the day, these logs were coming in on average almost one per minute, and continued through the night though at a reduced rate. Has anyone else experienced this? Other than browsing to that image myself a couple of times, and also connecting to it through a web application (using the access keys of a restricted IAM user) there should be no one and no service trying to access that bucket. Thank you in advance for any insight.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233344&tstart=75"}{"Answers": {"usr-1": ["I have a lifecycle rule set for an entire bucket. it doesn't appear to be clearing out the bucket (set to delete older than 1 day). Files are months old btw. Its been set since last night with no change region: us-west-2 Edited by: jeffreysteinmetz on May 20, 2016 2:41 PM"], "usr-2": ["Hello, I would like to assist you. Please share your bucket name with me via a private message. Thanks and Regards, Abhilasha"], "usr-3": ["looks like it finally emptied after a few days, but now I have another bucket with the same story. Will direct message you with bucket name. Edited by: jeffreysteinmetz on May 24, 2016 1:03 PM"], "usr-4": ["Part of the challenge is quickly finding the cases where lifecycles were missed or are missing files. I solved similar storage tracking issues by watching the daily trend of standard storage and versions per bucket and path prefix. The webservice i have used is Insight4Storage on AWS marketplace, it crawls storage and saves total size by path/prefix , bucket and storage class in elasticsearch, and has trend charts by storage class, bucket, and total size by path prefix."]}, "Question": "I have a lifecycle rule set for an entire bucket. it doesn't appear to be clearing out the bucket (set to delete older than 1 day). Files are months old btw. Its been set since last night with no change region: us-west-2 Edited by: jeffreysteinmetz on May 20, 2016 2:41 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=231979&tstart=75"}{"Answers": {"usr-1": ["i create a bucket and website and it works ok. i created another but it is getting 403 Forbidden and Code: AccessDenied, etc., on all access attempts. both site are configured alike as far as i can see. when i delete error.html then the message below An Error Occurred While Attempting to Retrieve a Custom Error Document changes from Code: AccessDenied to Code: NoSuchKey ... so i know i am getting to the correct bucket. the 2nd website is a hostname under the 1st which is the domain apex name (e.g like www.example.com but a different name). both are DNS served by Route 53. any idea what i might be missing? permissions are the same."], "usr-2": ["i found a typo in the policy which i should have posted for maybe faster resolution"]}, "Question": "i create a bucket and website and it works ok. i created another but it is getting 403 Forbidden and Code: AccessDenied , etc., on all access attempts. both site are configured alike as far as i can see. when i delete error.html then the message below An Error Occurred While Attempting to Retrieve a Custom Error Document changes from Code: AccessDenied to Code: NoSuchKey ... so i know i am getting to the correct bucket. the 2nd website is a hostname under the 1st which is the domain apex name (e.g like www.example.com but a different name). both are DNS served by Route 53 . any idea what i might be missing? permissions are the same.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233288&tstart=75"}{"Answers": {"usr-1": ["Hi, Last week I created a simple application that takes a file prefix as an argument, and does the following: list 1000 keys, download all of them and delete them from S3, use the last key as the marker for the next list and so on. It ran for days without problems and processed ~184 million files so far. But this week I started seeing a large number (hundreds of thousands) of \"NoSuchKey\" errors when sending GET requests to the keys received from LIST. This doesn't happen for each LIST request, and the same GET request can succeed once and fail the second time. The files have been in this S3 bucket for several months, none of them are new. Any suggestions? The only information I found about these errors is that they can happen for new files immediately after uploading, which isn't the case here."], "usr-2": ["Update: I re-ran the application again (same way I did multiple times today and yesterday), and this time it processed the remaining couple hundred thousands files without getting a single error. This seemed to have been either a temporary issue or a strange bug in the app."]}, "Question": "Hi, Last week I created a simple application that takes a file prefix as an argument, and does the following: list 1000 keys, download all of them and delete them from S3, use the last key as the marker for the next list and so on. It ran for days without problems and processed ~184 million files so far. But this week I started seeing a large number (hundreds of thousands) of 'NoSuchKey' errors when sending GET requests to the keys received from LIST. This doesn't happen for each LIST request, and the same GET request can succeed once and fail the second time. The files have been in this S3 bucket for several months, none of them are new. Any suggestions? The only information I found about these errors is that they can happen for new files immediately after uploading, which isn't the case here.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233364&tstart=75"}{"Answers": {"usr-1": ["Not long ago all my S3 API calls started timing out. Is anyone else seeing this? Thanks in advance"], "usr-10": ["Like wise. Seems to be affecting all US-EAST-1 region. Was able to access the frontend momentarily and saw the new Dashboard view but all my buckets were gone, when I tried to change to old view I get: { \"errorCode\" : \"InternalError\" }"], "usr-11": ["Unable to execute HTTP request: Connect to s3.amazonaws.com:443 http://s3.amazonaws.com/10.188.125.192 failed: connect timed out"], "usr-12": ["{ \"errorCode\" : \"InternalError\" } Same error"], "usr-13": ["https://status.aws.amazon.com Amazon is looking into it."], "usr-14": ["we're also experiencing the same issues but not only in us-east-1, but also sa-east-1 and ap-southeast-1 status.aws.amazon.com doesn't give any info about the outage yet"], "usr-15": ["Same issue. An issue appeared on my AWS Health Dashboard for about a minute. Then it disappeared . Not in Event Log either... That renders AWS Health pretty useless for outages (what else is it useful then?)."], "usr-16": ["Yes, I have the same problem. Even my S3 dashboard returns: \"errorCode\" : \"InternalError\""], "usr-17": ["Quoted from top of AWS health page (https://status.aws.amazon.com): Increased Error Rates We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue."], "usr-18": ["Hello, We've identified the issue as high error rates with S3 in US-EAST-1, which is also impacting applications and services dependent on S3. We are actively working on remediating the issue. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. We apologize for the inconvenience caused."], "usr-19": ["It has been 1.5 hours now. What is the ETA for a fix, or some sort of recovery?"], "usr-2": ["I'm experiencing the same on my end."], "usr-20": ["Still not working. Can't call API. Public files are not accessible. Can access s3 console, but shows 0 buckets 0 regions."], "usr-21": ["Hi, Update at 2:08 PM PST: As of 1:49 PM PST, we are fully recovered for operations for adding new objects in S3, which was our last operation showing a high error rate. The Amazon S3 service is operating normally. Update at 1:12 PM PST: S3 object retrieval, listing and deletion are fully recovered now. We are still working to recover normal operations for adding new objects to S3. Please keep an eye on the AWS Service Health Dashboard at http://status.aws.amazon.com/. Again, we apologize for the inconvenience caused. Thanks!!!"], "usr-3": ["I'm getting an InternalError when trying to access the S3 via the web console."], "usr-4": ["Yes, same here. And, trying to view S3 buckets via old S3 console page am receiving \"InternalError\" message."], "usr-5": ["Yes same here."], "usr-6": ["Yes calls to S3 are timing out from our servers in multiple locations."], "usr-7": ["Yup totally broken here as well, public files aren't reachable either. US-East if it matters."], "usr-8": ["{ \"errorCode\" : \"InternalError\" } . when clicking on s3 from the dashboard"], "usr-9": ["Add me to the \"me too list\" entire site is has no images"]}, "Question": "Not long ago all my S3 API calls started timing out. Is anyone else seeing this? Thanks in advance", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=250319&tstart=0"}{"Answers": {"usr-1": ["Is there an App to put Amazon S3 as a folder in windows File Explorer, in the same way that OneDrive and Dropbox are folders?"], "usr-2": ["Hi We have a separate product called CloudBerry Drive that helps you map cloud storage as a local disk. Check out the link below on how to get started with Amazon S3 http://www.cloudberrylab.com/blog/how-to-use-amazon-s3-with-cloudberry-drive/ Thanks Andy"], "usr-3": ["Thank you sounds good. Tell me is there any amazon cost involved? For example if there are no uploads or downloads of files in a period but windows file explorer is open all the time and regularly used would there be a cost from Amazon?"]}, "Question": "Is there an App to put Amazon S3 as a folder in windows File Explorer, in the same way that OneDrive and Dropbox are folders?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233374&tstart=75"}{"Answers": {"usr-1": ["I just recently purchased a domain name (my-main-page.com) from amazon and need to point it to the corresponding S3 bucket. There are some instructions on set-up, like making two buckets (my-main-page.com & www.my-main-page.com), etc which i was able to follow, but the remaining process is a complete mystery to me. I need a step-by-step explanation on how to fill in the blanks (i guess) on the Route 53 page. I need a simple and effective way to make the connection to my S3 bucket. The terms associated with DNS are totally not understandable by me. Please see attached picture. Thanks for any help. ~gg"], "usr-2": ["Hello Gary, thank you for posting on the AWS forums! I believe you have followed the tutorial at the following link: https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html I have checked your buckets and I can see that you have already completed the Static Website Hosting configuration, put the correct bucket policy for public access on my-main-page.com and set up requests redirection from www.my-main-page.com. According to your screenshot, you should already have the Hosted Zones in Route53, so you would just need to perform the configuration outlined in step 3.2 from the page linked above. For your convenience, below are the steps to complete the configuration as outlined in the tutorial, starting from the Route53 dashboard in your screenshot: Click on the \"Hosted zones\" link Click on \"my-main-page.com\" Click on \"Create Record Set\" Fill in the details on the \"Create Record Set panel\" as per the attached picture, replacing \"example.com\" with \"my-main-page.com\" Click on \"Create\" Repeat the same steps for the \"www.my-main-page.com\" Hosted Zone, replacing \"example.com\" with \"www.my-main-page.com\". Everything should be correctly set up at this point, and you should be able to browse to your website. I hope this information helps you! Have a great day! Andrea S."], "usr-3": ["Dear Andrea, Thanks very much for answering my question and for your help. Using the information you provided i was able to complete the process of pointing my domain name to the correct bucket. In fact it works very well: The www. redirects to the DN \"my-main-page.com\" very quickly and everything seems to work fine. I have attached a picture of the final set-up: Please note the \"www\" in the \"edit record set\" name field. I had to experiment some to get it to come out right - I don't want to be too critical because i have a tendency to not update things on my own websites as i should, and as Amazon should. So i am very happy about getting this working correctly. If you have other people needing some instruction with setting up their own Route 53 with the same problems i have had please feel free to use my pictures for reference. Sincerely, gary green Edited by: gary green on Jun 10, 2016 9:37 PM"], "usr-4": ["FYI your .jpg files are really in PNG format. PNG is better, but the names have the wrong endings."], "usr-5": ["Hi Andrea. Now when i click on a link on the webpage i get this: http://my-main-page.com/shedandshelter.s3.amazonaws.com/index.html The link is not opening up into a window of it's own. Pease help correct this. Thanks, ~gg"], "usr-6": ["Hello, I checked your website and found that the URL on the page is pointing to \"http://my-main-page.com/shedandshelter.s3.amazonaws.com/index.html\". It means that the requested page exists in \"my-main-page.com\" bucket. If the page does not exist in the bucket, it will return 404 error as it is doing currently. When I separately browsed \"shedandshelter.s3.amazonaws.com/index.html\" or \"http://s3.amazonaws.com/shedandshelter/index.html\", I was able to get the page. It means that the desired page exist in a separate bucket named \"shedandshelter\". If you want to access the page at using 'my-main-page.com' domain, you should create a folder named \"shedandshelter\" in bucket 'my-main-page.com' and have \"index.html\" inside the folder \"shedandshelter\". Once you do that, you should change your URL to \"http://my-main-page.com/shedandshelter/index.html\". Thanks, Jay"], "usr-7": ["Hi Jay. (I have found the source of the problem. Now everything works great! 14 Jun 16) Thank you for your detailed response. The solution you outlined will not work for my situation because i cannot change the URL of my many websites because people bookmark these current URL's and i am able to make some income from the websites. Also, i want to mention that my website: SHED & SHELTER (shedandshelter.s3.amazonaws.com/index.html), for example has many links to my external S3 buckets. I have never had a problem like this. I have always been able to link to my other buckets in the S3 system. Maybe what i will have to do is point the domain name: my-main-page.com - to another folder named: my-main-page and leave off the .com? That is probably what i have to do. my-main-page.com needs to point to: my-main-page.s3.amazonaws.com/index.html The above will work and if people bookmark that URL that is OK. What do you think? Thanks for your help. ~gary green Edited by: gary green on Jun 14, 2016 12:00 AM"], "usr-8": ["Thanks Jay for your response. I just found out what the problem is/was: I needed to add \"http://\" in front of the rest of the URL. After doing that everything worked fine. Sorry to bother you. ~gg"], "usr-9": ["Thank you for your comment about \"FYI your .jpg files are really in PNG format. PNG is better, but the names have the wrong endings.\" Do you mean that i have used .jpg in place of .png? How can i tell if my jpg file is in the png format? (I just answered this question myself: I guess you right click on the picture and select \"Properties\" from the drop-down menu. Thanks for pointing this out to me, much appreciated. ~gg) Sometimes i try and save a picture in jpg even though it says png in the name field, since the picture was/is viewable in my xnview software i thought maybe it was converted to jpg but apparently not. If you could explain a little i would appreciate it. Thanks, ~gg Edited by: gary green on Jun 14, 2016 12:44 AM"]}, "Question": "I just recently purchased a domain name (my-main-page.com) from amazon and need to point it to the corresponding S3 bucket. There are some instructions on set-up, like making two buckets (my-main-page.com and www.my-main-page.com), etc which i was able to follow, but the remaining process is a complete mystery to me. I need a step-by-step explanation on how to fill in the blanks (i guess) on the Route 53 page. I need a simple and effective way to make the connection to my S3 bucket. The terms associated with DNS are totally not understandable by me. Please see attached picture. Thanks for any help. ~gg", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=232961&tstart=75"}{"Answers": {"usr-1": ["newpost: this issue is a bug in the web console which caused me to not save the bucket policy. see: https://forums.aws.amazon.com/thread.jspa?threadID=235937&tstart=0 oldpost: i am getting different responses from different buckets as if different software was involved: ==> lt1/sr_use1 /home2/sr_use1 1> lynx -mime_header http://stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> Last-Modified: Sun, 12 Jun 2016 05:04:54 GMT ==> ETag: \"d41d8cd98f00b204e9800998ecf8427e\" ==> x-amz-error-code: NoSuchKey ==> x-amz-error-message: The specified key does not exist. ==> x-amz-error-detail-Key: thisdoesnotexist.html ==> x-amz-request-id: 79DF161EA4B9D9E8 ==> x-amz-id-2: gDvVCd92l04suxYkfFUFsjzUFMy4vxN1kAEf51rDkA4XqGvSDtGHOReq70e9Z3fND/G/il7QvPI= ==> Content-Type: text/html ==> Date: Sun, 12 Jun 2016 05:13:38 GMT ==> Connection: close ==> Server: AmazonS3 ==> ==> lt1/sr_use1 /home2/sr_use1 2> lynx -mime_header http://work.stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> x-amz-request-id: 4B6656B4C73FE964 ==> x-amz-id-2: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> Content-Type: text/html; charset=utf-8 ==> Content-Length: 549 ==> Date: Sun, 12 Jun 2016 05:13:49 GMT ==> Server: AmazonS3 ==> Connection: close ==> ==> <html> ==> <head><title>404 Not Found</title></head> ==> <body> ==> <h1>404 Not Found</h1> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: thisdoesnotexist.html ==> RequestId: 4B6656B4C73FE964 ==> HostId: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> ==> <h3>An Error Occurred While Attempting to Retrieve a Custom Error Document</h3> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: error.html ==> ==> <hr/> ==> </body> ==> </html> ==> lt1/sr_use1 /home2/sr_use1 3> anyone know why? since i am getting other different behavior for the same configuration i really am suspecting software inconsistency. Edited by: stratusrelay on Jul 24, 2016 10:48 PM"]}, "Question": "newpost: this issue is a bug in the web console which caused me to not save the bucket policy. see: https://forums.aws.amazon.com/thread.jspa?threadID=235937andtstart=0 oldpost: i am getting different responses from different buckets as if different software was involved: ==> lt1/sr_use1 /home2/sr_use1 1> lynx -mime_header http://stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> Last-Modified: Sun, 12 Jun 2016 05:04:54 GMT ==> ETag: 'd41d8cd98f00b204e9800998ecf8427e' ==> x-amz-error-code: NoSuchKey ==> x-amz-error-message: The specified key does not exist. ==> x-amz-error-detail-Key: thisdoesnotexist.html ==> x-amz-request-id: 79DF161EA4B9D9E8 ==> x-amz-id-2: gDvVCd92l04suxYkfFUFsjzUFMy4vxN1kAEf51rDkA4XqGvSDtGHOReq70e9Z3fND/G/il7QvPI= ==> Content-Type: text/html ==> Date: Sun, 12 Jun 2016 05:13:38 GMT ==> Connection: close ==> Server: AmazonS3 ==> ==> lt1/sr_use1 /home2/sr_use1 2> lynx -mime_header http://work.stratusrelay.com/thisdoesnotexist.html ==> HTTP/1.1 404 Not Found ==> x-amz-request-id: 4B6656B4C73FE964 ==> x-amz-id-2: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> Content-Type: text/html; charset=utf-8 ==> Content-Length: 549 ==> Date: Sun, 12 Jun 2016 05:13:49 GMT ==> Server: AmazonS3 ==> Connection: close ==> ==> <html> ==> <head><title>404 Not Found</title></head> ==> <body> ==> <h1>404 Not Found</h1> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: thisdoesnotexist.html ==> RequestId: 4B6656B4C73FE964 ==> HostId: NAwnQu4mO97jfsnBADkEM9bD6mMsmHGjAVbSm1izurvfc6G3DvPFJezmwW6K6cJzGvrwVG4r6V8= ==> ==> <h3>An Error Occurred While Attempting to Retrieve a Custom Error Document</h3> ==> ==> Code: NoSuchKey ==> Message: The specified key does not exist. ==> Key: error.html ==> ==> <hr/> ==> </body> ==> </html> ==> lt1/sr_use1 /home2/sr_use1 3> anyone know why? since i am getting other different behavior for the same configuration i really am suspecting software inconsistency. Edited by: stratusrelay on Jul 24, 2016 10:48 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233358&tstart=75"}{"Answers": {"usr-1": ["Hi, We stumbled upon EntityTooSmall errors with uploads that did not seem to have problems with part sizes. By trial and error we found out that by removing earlier parts the problem went away. Can earlier parts with different UploadId affect future uploads? Or was this a malfunction in the service? --hajautettu (Some further details: 6500 parts, 863 files, approximate time range: 2016-04-28T11:16:47.000Z\", \"2016-05-30T20:02:08.000Z\". AWS SDK for Javascript, version available in Lambda nodejs4.3 runtime)"], "usr-2": ["Deleting parts did not seem to be the solution. Some files still refuse to upload. Will continue to diagnose this."]}, "Question": "Hi, We stumbled upon EntityTooSmall errors with uploads that did not seem to have problems with part sizes. By trial and error we found out that by removing earlier parts the problem went away. Can earlier parts with different UploadId affect future uploads? Or was this a malfunction in the service? --hajautettu (Some further details: 6500 parts, 863 files, approximate time range: 2016-04-28T11:16:47.000Z', '2016-05-30T20:02:08.000Z'. AWS SDK for Javascript, version available in Lambda nodejs4.3 runtime)", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233520&tstart=75"}{"Answers": {"usr-1": ["Hello! Yesterday our goverment blocked s3.amazonaws.com url. It still remains blocked by some ISP in Russia. We use s3 for storing files in our projects. We use browser upload with CORS and direct download with presigned URLs. Upload is not blocked since we can use endpoint *.s3-eu-west-1.amazonaws.com. But presigned URLs are formed like bucket-name.s3.amazonaws.com/... so it is partially blocked in Russia. Can some advise how to cope with this? Is there a way work with S3, but not so use bucket-name.s3.amazonaws.com address? Best regards, Nemtsov Georgiy Edited by: gnemtsov on Jun 22, 2016 11:59 PM"], "usr-2": ["Hello Georgiy, thanks for bring this into our atention. I'm sorry to hear you are in this kind of trouble. I have already requested the opinion of an S3 Engineer, to see if there is possible to workaround this issue and allow you to use our service. We will let you know as soon as I got any new on this Rgrds Javier C#"], "usr-3": ["Hello Georgiy, S3 has a list of all regional endpoints at https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region that you can use. You can also create presigned URLs using other endpoints (presigned url feature is not limited to the s3.amazonaws.com endpoint). Please refer to the following link for additional information: http://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html Best Regards, Pavithra K Edited by: Pavithra@AWS on Jun 23, 2016 10:56 AM"], "usr-4": ["Thanks for information. Today Amazon S3 was unblocked in Russia, because Amazon fulfilled our goverments' request to block some prohibited content on S3. Hope this won't repeat in our country."]}, "Question": "Hello! Yesterday our goverment blocked s3.amazonaws.com url. It still remains blocked by some ISP in Russia. We use s3 for storing files in our projects. We use browser upload with CORS and direct download with presigned URLs. Upload is not blocked since we can use endpoint -.s3-eu-west-1.amazonaws.com. But presigned URLs are formed like bucket-name.s3.amazonaws.com/... so it is partially blocked in Russia. Can some advise how to cope with this? Is there a way work with S3, but not so use bucket-name.s3.amazonaws.com address? Best regards, Nemtsov Georgiy Edited by: gnemtsov on Jun 22, 2016 11:59 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234105&tstart=75"}{"Answers": {"usr-1": ["I am trying to archive old logs to glacier from an S3 bucket and it's simply not working. Basically I have an s3 bucket admin.logs in which there is a folder api. (this is phase 1). So in that bucket I create a life-cycle rule. Step1 Apply the Rule to: I select A prefix api/ Step2 Configure Rule: the 2nd box Archive to the Glacier Storage Class 30 days after the objects creation date is setup. Step 3 is just the info, re-confirming I see; This rule will apply to Objects with the prefix: api/ in the admin.logs bucket Archive to the Glacier Storage Class 30 days after the object's creation date. Now, I still see things from 2+ months ago in the S3 folder. Also, I don't see a place where you set the glacier vault info. I created a vault for this and see no data. So, I'm not sure if this is right OR do I really need to bash script some aws s3 ls | find mtime +30, etc. Thanks for all read/replies."], "usr-2": ["If you set up the rule very recently, give it a couple of days since the transition isn't immediate. In the S3 Management Console, files that have been moved to Glacier storage will have the \"Storage Class\" listed as \"Glacier\". Note that you will still see the files in S3. They are technically stored in Glacier if they have a storage class saying so, but all files are still managed through the S3 web interface or client as usual. Even though they are in Glacier, you won't see them through the Glacier service when you use the S3/Glacier integration."], "usr-3": ["Ah, funny I didn't see that \"storage class\" column, I just expected it to just disappear from the S3 bucket and appear in a Glacier vault. Then when I went to check, I realized I never gave a vault name, and then wondered even more. So, I guess the reference is still there with the same glacier rules/rates, but I have the luxury of using the S3 CLI commands if I ever need to copy back (knowing there will be a delay), but just surprised I didn't see that class column (sigh). Thanks for the simple and correct answer."]}, "Question": "I am trying to archive old logs to glacier from an S3 bucket and it's simply not working. Basically I have an s3 bucket admin.logs in which there is a folder api. (this is phase 1). So in that bucket I create a life-cycle rule. Step1 Apply the Rule to: I select A prefix api/ Step2 Configure Rule: the 2nd box Archive to the Glacier Storage Class 30 days after the objects creation date is setup. Step 3 is just the info, re-confirming I see; This rule will apply to Objects with the prefix: api/ in the admin.logs bucket Archive to the Glacier Storage Class 30 days after the object's creation date. Now, I still see things from 2+ months ago in the S3 folder. Also, I don't see a place where you set the glacier vault info. I created a vault for this and see no data. So, I'm not sure if this is right OR do I really need to bash script some aws s3 ls | find mtime +30, etc. Thanks for all read/replies.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234434&tstart=75"}{"Answers": {"usr-1": ["I'm trying to work through the S3 signing example. But I'm not getting the interim answers in their docs. I started with: policy = %{\"conditions\"=> [ %{\"bucket\"=> \"sigv4examplebucket\"}, [\"starts-with\", \"$key\", \"user/user1/\"], %{\"acl\"=> \"public-read\"}, %{\"success_action_redirect\"=> \"http=>//sigv4examplebucket.s3.amazonaws.com/successful_upload.html\"}, [\"starts-with\", \"$Content-Type\", \"image/\"], %{\"x-amz-meta-uuid\"=> \"14365123651274\"}, %{\"x-amz-server-side-encryption\"=> \"AES256\"}, [\"starts-with\", \"$x-amz-meta-tag\", \"\"], %{\"x-amz-credential\"=> \"AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request\"}, %{\"x-amz-algorithm\"=> \"AWS4-HMAC-SHA256\"}, %{\"x-amz-date\"=> \"20151229T000000Z\"} ] } \u00a0 stringToSign = policy |> Poison.encode! |> Base.encode64 But the `stringToSign` comes out rather shorter than the Amazon example. For the signature I have these helpers: @secret_key \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" def signing_key2(secret, date, region, service) do hash_sha256(\"AWS4\" \\<\\> secret, date) |> hash_sha256(region) |> hash_sha256(service) |> hash_sha256(\"aws4_request\") end \u00a0 def hash_sha256(secret, msg) do hash_sha256_bis(secret, msg) |> Base.url_encode64 end \u00a0 def hash_sha256_bis(secret, msg) do :crypto.hmac(:sha256, secret, msg) end and ran signing_key2(@secret_key, \"20151229\", \"us-east-1\", \"s3\") but this came out shorter than the test answer. Would welcome some pointers. Edited by: hotbelgo on Jul 3, 2016 1:23 AM Edited by: hotbelgo on Jul 3, 2016 1:25 AM Edited by: hotbelgo on Jul 3, 2016 1:26 AM"]}, "Question": "I'm trying to work through the S3 signing example . But I'm not getting the interim answers in their docs. I started with: policy = \\% { 'conditions' => [ \\% { 'bucket' => 'sigv4examplebucket' } , [ 'starts-with' , '$key' , 'user/user1/' ], \\% { 'acl' => 'public-read' } , \\% { 'success_action_redirect' => 'http=>//sigv4examplebucket.s3.amazonaws.com/successful_upload.html' } , [ 'starts-with' , '$Content-Type' , 'image/' ], \\% { 'x-amz-meta-uuid' => '14365123651274' } , \\% { 'x-amz-server-side-encryption' => 'AES256' } , [ 'starts-with' , '$x-amz-meta-tag' , '' ], \\% { 'x-amz-credential' => 'AKIAIOSFODNN7EXAMPLE/20151229/us-east-1/s3/aws4_request' } , \\% { 'x-amz-algorithm' => 'AWS4-HMAC-SHA256' } , \\% { 'x-amz-date' => '20151229T000000Z' } ] } \u00a0 stringToSign = policy |> Poison.encode! |> Base.encode64 But the `stringToSign` comes out rather shorter than the Amazon example. For the signature I have these helpers: @secret_key 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' def signing_key2(secret, date, region, service) do hash_sha256( 'AWS4' \\<\\> secret, date) |> hash_sha256(region) |> hash_sha256(service) |> hash_sha256( 'aws4_request' ) end \u00a0 def hash_sha256(secret, msg) do hash_sha256_bis(secret, msg) |> Base.url_encode64 end \u00a0 def hash_sha256_bis(secret, msg) do :crypto.hmac(:sha256, secret, msg) end and ran signing_key2(@secret_key, '20151229' , 'us-east-1' , 's3' ) but this came out shorter than the test answer. Would welcome some pointers. Edited by: hotbelgo on Jul 3, 2016 1:23 AM Edited by: hotbelgo on Jul 3, 2016 1:25 AM Edited by: hotbelgo on Jul 3, 2016 1:26 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234588&tstart=75"}{"Answers": {"usr-1": ["Hello, I followed the steps shown on this walkthrough essentially word-for-word to create a website that can be accessed at http://mattchen.co. I bought the domain name through Route 53, which I'm also using to handle the DNS aspect of the website hosting. The problem: If I access http://www.mattchen.co, then I am not automatically redirected to http://mattchen.co (although according to the article, I should be.) What did I do wrong?"], "usr-2": ["You are missing a DNS record for www as discussed under \"Step 3.2: Add Alias Records for example.com and www.example.com\"."], "usr-3": ["Hi D. Svanlund, I thought I had followed that step correctly\u2014in fact, I even undid it and then did it again just to be sure. I've attached a screenshot which might provide more detail. Can you clarify why pursuing this step did not seem to be successful? Thanks, M"], "usr-4": ["The screenshot shows that you have a separate Route 53 hosted zone for the www domain. That's not necessary (and it won't work out of the box). Instead, create a new record set (type A) for www inside your example.com hosted zone."]}, "Question": "Hello, I followed the steps shown on this walkthrough essentially word-for-word to create a website that can be accessed at http://mattchen.co . I bought the domain name through Route 53, which I'm also using to handle the DNS aspect of the website hosting. The problem: If I access http://www.mattchen.co , then I am not automatically redirected to http://mattchen.co (although according to the article, I should be.) What did I do wrong?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234509&tstart=75"}{"Answers": {"usr-1": ["let say i sync a directory whit 1000 files to s3 the first time (so no file on s3 yet) how many put requests are that ? 1 or 1000 if all files are on s3 how many then ? does the command GET each file one by one checks the state and then and PUT them on if needed mean could doing a sync on large directorys end up with tens of thousand of put and get requests ?"]}, "Question": "let say i sync a directory whit 1000 files to s3 the first time (so no file on s3 yet) how many put requests are that ? 1 or 1000 if all files are on s3 how many then ? does the command GET each file one by one checks the state and then and PUT them on if needed mean could doing a sync on large directorys end up with tens of thousand of put and get requests ?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=233864&tstart=75"}{"Answers": {"usr-1": ["I have 4 S3 buckets. They are all \"standard\" type. In the list of available metrics in CloudWatch, I only see metrics for 1 bucket. I thought maybe the metrics need to be enabled on the bucket in the S3 console, but I can't find any settings related to CloudWatch. What am I missing?"], "usr-2": ["Are any of your S3 buckets with \"missing\" CloudWatch metrics empty? I don't believe that CloudWatch generates NumberOfObjects and BucketSizeBytes metrics for empty buckets..."], "usr-3": ["Good idea, but no, they're not empty."], "usr-4": ["I figured it out. S3 shows \"Global\" as the region, and when the region selector is clicked, the message \"S3 does not require a region\". However each bucket can be associated with a different region. The 3 buckets that were missing in CloudWatch were in a different region, and I had to switch to that region in CloudWatch to see their metrics."]}, "Question": "I have 4 S3 buckets. They are all 'standard' type. In the list of available metrics in CloudWatch, I only see metrics for 1 bucket. I thought maybe the metrics need to be enabled on the bucket in the S3 console, but I can't find any settings related to CloudWatch. What am I missing?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235254&tstart=75"}{"Answers": {"usr-1": ["Hi, I encountered a strange issue that seems to relate to special-chars being used in object-keys. I am listing objects the way that it is explained at https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingJava.html. The bucket contains 40.000+ objects, some with special chars in their name, e.g. \"Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac\". I noticed, that, although the continuation-token changes, the keys that I get back from AWS were repeated sometimes and I was stuck in an endless loop (truncated == true but the same keys are returned over and over again). I then decreased the maxObjectSize to 1 to try to find the object-key where the looping starts. This was an object-key that contained a special-char: \"Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac\". So the trigger to start this looping-behavior seems to be, that the last object in a fetched batch contains special-chars which seems like a serious bug to me. Loop output: Alabama Shakes_Gimme All Your Love.flac Alexandrina_Flori De Spin.flac Alexandrina_La Ville Ombres.flac Alexandrina_Muguri Florali.flac Alexandrina_Noaptea.flac Alexandrina_Orasul Umbre.flac Alexandrina_Pijamale Reci.flac Alexandrina_Spre Sud.flac Alexandrina_Tu Cu Soarele.flac Alexandrina_Turturica.flac Alogte Oho Jonas And His Sounds Of Joy_Zota Yinne.flac AlunaGeorge_I'm In Control (The Magician Remix).flac Amason_California Dreamin'.flac Amason_I Want To Know What Love Is.flac Anchorsong_Butterflies.flac Anchorsong_Ceremony.flac Anchorsong_Expo.flac Anchorsong_Kajo.flac Anchorsong_Last Feast.flac Anchorsong_Oriental Suite.flac Anchorsong_Rendezvous.flac Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Thanks! Clemens"], "usr-2": ["Hi Clemens, Thanks for bringing this to our attention. We are currently investigating this and we will post an update soon. Thanks, Vik."], "usr-3": ["Hi Clemens, This issue is now resolved, let us know if you still face any problems. Thanks again for reporting and contributing to the forums. Best, Vik."], "usr-4": ["We've got the same problem with one of our customser! There's an endless loop that is split across 10 responses of ListBucketResult (1000 objects in each). There're about 2 million objects in total. The last object returned in the 10-th subsequent list response is the marker that was used for the 1-st request. The problem discovered for 2 buckets of the account. The problem is reproducible with very high rate. The buckets are in eu-west-1 region,and EC2 instance is in us-east-1b region. The bucket name, account id and the responses are not for public, but I can provide it to AWS support contact. Here're the response headers returned by AWS to subsequent bucket listing requests. Appropriate GET request contains sensitive information and can be provided to someone from AWS team if needed. 1. x-amz-id-2: rkO5vBQ789qsn4hIfX6KSjONtkr+a/sZxJIixx4ukpkhlAg54Zf8I2cWfOBlQB4/xKaFkJ/iI80= x-amz-request-id: 461B1DACBBF7BD30 \u00a0 2. x-amz-id-2: 8IOZfjuU+08qqlGvzhRn7Le1nOKwUMmRImFzMD4cuji/NDkCWWOyA91tBnyaI8s9qHkV4dAqTC8= x-amz-request-id: C6BCE9367D02763A \u00a0 3. x-amz-id-2: YCmb+KqRgNKe2ZRm8Mdp7gDgBD0c0NGA6EJ98GUKnHyTPcZ4zE6rQgrt+m4yvFxhGi43sUQEzQs= x-amz-request-id: 46DEFC7BC2E162B3 \u00a0 4. x-amz-id-2: dkXNlWz8So41qh9y/rs4K84M+L0Lyb953Mhohch3CSb7TGs+VOGTzqRH+b6TbbJnduEBt7TeeIo= x-amz-request-id: 1A2A09356474340A \u00a0 5. x-amz-id-2: +pVvGbA6k1LDJbqS9mT6KZtvX0fxhujxenMEom8gRs0eT6hUYpey9zZCADDx5WQfEa4P2DBhz3Y= x-amz-request-id: 0D3479695E4C9DD8 \u00a0 6. x-amz-id-2: pVvoeKSSAg+ytcJG1flXvDrQHqs9pGOglPlZPMIs+pW0ggmcbLCMIBLYK1X4qIBjHea+/3Oyaeo= x-amz-request-id: 3D6C22F4FD749CC6 \u00a0 7. x-amz-id-2: oFZ+kQG1fx+lyXkmrWSrr4ED5UFOGwodsJH2mgG+Wj3MQzQwRBVwuODr+xqCnJ3BhDRZeqiH9TQ= x-amz-request-id: 279AEEAF1C298D12 \u00a0 8. x-amz-id-2: uFuukh36N4DPc0Yp/4z/mfsoc5vulavg8WzhLjDuG4iVZAJJvw6FZtIE9H2i5RlUcAMe5lCtEg8= x-amz-request-id: D257A9EC48C4DCA4 \u00a0 9. x-amz-id-2: +axYUYEtx4czwHhnALTUG+CvbqeqonOE/7P1irvJs7n8NuAaQMaiNqG93s82HtB3rsTDz4qnc68= x-amz-request-id: 434B6D164840B28D \u00a0 10. !!! START LOOP. The Marker is the same as in the first request.!!! x-amz-id-2: uhJar8UMTeGjFASAFBVJgJ83ZoCWF1K0fOfdt3V10rke8Y5dN2Lhpv1nN87cqt0JuR5iwYSTf6k= x-amz-request-id: 32E28E5E47D8C8BB \u00a0 11. !!! CONTINUE LOOP !!! x-amz-id-2: kTw23GENbj2+9ThJm8HdFZUVbtzMkFYuG/hO5X2yHNWIgghQNwAKpSfeAukNH/1XNnDCzUrEbB0= x-amz-request-id: 85DF9AD7D5F1E3C2 \u00a0 ... In this case the repeated marker appeared exactly at ListBucketResult boarder. However it can appear somewhere in the middle that makes detecting the issue very hard. Sincerely, IP"], "usr-5": ["Figured out the issue. It was MS .Net framework that removes some unprintable control characters from an URI (particularly, \\u200f) when sending a request"]}, "Question": "Hi, I encountered a strange issue that seems to relate to special-chars being used in object-keys. I am listing objects the way that it is explained at https://docs.aws.amazon.com/AmazonS3/latest/dev/ListingObjectKeysUsingJava.html . The bucket contains 40.000+ objects, some with special chars in their name, e.g. 'Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac'. I noticed, that, although the continuation-token changes, the keys that I get back from AWS were repeated sometimes and I was stuck in an endless loop (truncated == true but the same keys are returned over and over again). I then decreased the maxObjectSize to 1 to try to find the object-key where the looping starts. This was an object-key that contained a special-char: 'Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac'. So the trigger to start this looping-behavior seems to be, that the last object in a fetched batch contains special-chars which seems like a serious bug to me. Loop output: Alabama Shakes_Gimme All Your Love.flac Alexandrina_Flori De Spin.flac Alexandrina_La Ville Ombres.flac Alexandrina_Muguri Florali.flac Alexandrina_Noaptea.flac Alexandrina_Orasul Umbre.flac Alexandrina_Pijamale Reci.flac Alexandrina_Spre Sud.flac Alexandrina_Tu Cu Soarele.flac Alexandrina_Turturica.flac Alogte Oho Jonas And His Sounds Of Joy_Zota Yinne.flac AlunaGeorge_I'm In Control (The Magician Remix).flac Amason_California Dreamin'.flac Amason_I Want To Know What Love Is.flac Anchorsong_Butterflies.flac Anchorsong_Ceremony.flac Anchorsong_Expo.flac Anchorsong_Kajo.flac Anchorsong_Last Feast.flac Anchorsong_Oriental Suite.flac Anchorsong_Rendezvous.flac Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Andrew Bird_Are You Serious.flac Andrew Bird_Capsized.flac Andrew Bird_Pulaski.flac Andrew Bird_Roma Fade.flac Andrew Bird_Saints Preservus.flac Andrew Bird_Shoulder Mountain.flac Andrew Bird_Truth Lies Low.flac Andr\u00e1s_Gold Coast (Surfer's Paradise Mix).flac <---- Thanks! Clemens", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=234026&tstart=75"}{"Answers": {"usr-1": ["The response-content-disposition parameter supported by the GET request (http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) is no longer working. It ignores the value specified in the parameter and just returns the file with the content-disposition information in the stored resource. For example, the following request should cause the file to be returned with a content disposition of: inline;filename=\"foobar.pdf\";filename*=UTF-8''foobar.pdf GET https://s3.amazonaws.com/<bucket>/<docID>?response-content-disposition=inline%3Bfilename%3D%22foobar.pdf%22%3Bfilename*%3DUTF-8%27%27foobar.pdf&Signature=<signature>&Expires=<expires>&AWSAccessKeyId=<key> See RFC 5987 (https://tools.ietf.org/html/rfc5987) for an explanation of the content-disposition format, though I don't think that matters for this bug. When I use this parameter, the filename comes back with the original content disposition stored with the file, which in my case causes the file to be downloaded. This was working earlier this year and perhaps as recently as a few weeks ago - I have no way to verify. Has something changed? Edited by: BenGilbert on Jul 8, 2016 5:02 PM"], "usr-2": ["I am also experiencing this issue. Has anyone found a workaround? For now I am manually setting the ContentDisposition by copying the object into itself with the new ContentDisposition in the metadata. I also had to set MetadataDirective=true. It works, but... meh. Edited by: lucashansen on Jul 10, 2016 4:31 PM"], "usr-3": ["We're also experiencing the same thing but it's an intermittent problem for us. We have a test in place which checks the Content-Disposition header has the expected value from a signed S3 URL with the response-content-disposition specified. The test is failing about 50% of the time (with no code changes in between runs). I have an example independent of our application. If you run the following curl command repeatedly then sometimes the content disposition is as expected (inline; filename=\"filename.txt\" ...) and the sometimes it simply returns the content disposition from the object metadata (attachment; filename=\"file\"; ...) curl -s -D - 'https://kev-test.s3.amazonaws.com/449/UffU47CEyYekTRVaG1V0BjlaGjPP8ZEo?response-content-disposition=inline%3B%20filename%3D%22filename.txt%22%3B%20filename%2A%3DUTF-8%27%27filename%252Etxt&Signature=NnqjRWGRHuuZV8m2KuFdHRLrGA0%3D&AWSAccessKeyId=&ltAccess Key Removed&gt&Expires=1483228800&response-content-type=image/gif' -o /dev/null"], "usr-4": ["Thank you for the report. S3 is aware of this issue and we are working to resolve it."], "usr-5": ["Hello, Our service team has finished the deployment of a fix. In case you still encounter any issues, let us know the requestid and x-amz-id2s for some failed requests and we'll look into it. Eduardo C"]}, "Question": "The response-content-disposition parameter supported by the GET request ( http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html ) is no longer working. It ignores the value specified in the parameter and just returns the file with the content-disposition information in the stored resource. For example, the following request should cause the file to be returned with a content disposition of: inline;filename='foobar.pdf';filename-=UTF-8''foobar.pdf GET https://s3.amazonaws.com/ <bucket>/<docID>?response-content-disposition=inline\\%3Bfilename\\%3D\\%22foobar.pdf\\%22\\%3Bfilename-\\%3DUTF-8\\%27\\%27foobar.pdfandSignature=<signature>andExpires=<expires>andAWSAccessKeyId=<key> See RFC 5987 ( https://tools.ietf.org/html/rfc5987 ) for an explanation of the content-disposition format, though I don't think that matters for this bug. When I use this parameter, the filename comes back with the original content disposition stored with the file, which in my case causes the file to be downloaded. This was working earlier this year and perhaps as recently as a few weeks ago - I have no way to verify. Has something changed? Edited by: BenGilbert on Jul 8, 2016 5:02 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235006&tstart=75"}{"Answers": {"usr-1": ["I am working with other vendors that deliver content to our S3 account. Their system automatically creates the buckets to store the images delivered. The Buckets they have created are in Virginia (us-east-1). All of our services and other buckets are in Oregon (us-west-2). What costs could I expect for users accessing images in a different region? Web site visitors will be provided a url to the image i.e. https://s3.amazonaws.com/glvar-a/IMG-58412203_1.jpg I have PHP processes that will have to connect cross region to search for files containing specific strings in the file names. Is there an additional cost to poll a bucket in a different region? The result should contain information for 50 files or less. NOTE: We are not transferring the files between regions, just obtaining file information. Thank you! Steven"], "usr-2": ["You can find S3's pricing here: https://aws.amazon.com/s3/pricing/ Pricing for requests to each region varies depending on the region being accessed and the request being submitted. This includes differences in the cost for PUT/COPY/POST/LIST in addition GET or other requests. Additionally there is a data transfer pricing to also consider for data transferred out of each region to the internet (or to EC2 instances in a different region) and that cost also varies by region."]}, "Question": "I am working with other vendors that deliver content to our S3 account. Their system automatically creates the buckets to store the images delivered. The Buckets they have created are in Virginia (us-east-1). All of our services and other buckets are in Oregon (us-west-2). What costs could I expect for users accessing images in a different region? Web site visitors will be provided a url to the image i.e. https://s3.amazonaws.com/glvar-a/IMG-58412203_1.jpg I have PHP processes that will have to connect cross region to search for files containing specific strings in the file names. Is there an additional cost to poll a bucket in a different region? The result should contain information for 50 files or less. NOTE: We are not transferring the files between regions, just obtaining file information. Thank you! Steven", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235287&tstart=75"}{"Answers": {"usr-1": ["I've had a support request for this open for five days, but not getting much joy (nothing besides \"looking into it\"). downloading objects from s3.amazonaws.com/bucket/key is behaving very oddly - sometimes it works. sometimes it fails (403) for the same object. Just using wget/curl here - nothing fancy. The DNS is being resolved to the same IP address. We have recently changed the bucket policy here, however there are only allow rules (no deny) and even then, the indeterministic nature of the problem seems highly unusual. Just wondering if anyone has seen this before."], "usr-2": ["Hi, We are currently investigating and our team will follow up as soon as we have something. Thanks."], "usr-3": ["To wrap this up, eventually (took 13 days!) support came back - there were two issues: a) I took the example from http://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html - but it's a user policy (not a bucket policy) and the ${aws.username} caused the policy to fail evaluation (http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html says that anonymous = not present = means that the value is not in the current request information, and any attempt to match it fails and causes the request to be denied.) b) There was an issue with S3 where because of the fail above (but also an object ACL that said \"pass\"), S3 would occasionally return \"pass\". This has now been fixed so that it always returns fail."]}, "Question": "I've had a support request for this open for five days, but not getting much joy (nothing besides 'looking into it'). downloading objects from s3.amazonaws.com/bucket/key is behaving very oddly - sometimes it works. sometimes it fails (403) for the same object. Just using wget/curl here - nothing fancy. The DNS is being resolved to the same IP address. We have recently changed the bucket policy here, however there are only allow rules (no deny) and even then, the indeterministic nature of the problem seems highly unusual. Just wondering if anyone has seen this before.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235265&tstart=75"}{"Answers": {"usr-1": ["I'm struggling to emulate the test signatures at http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html. I can eventually get these answers but only by very careful attention to \\r\\n sequences in my test examples (e.g. note the double line break in the amazon example). I can't hope to get an exact amount of line spaces when I start to work with Elixir data structures and their json encoders, and so far at least I'm getting a series of SignatureDoesNotMatch errors (which are of course hard to debug, see also see http://stackoverflow.com/questions/38525846/s3-pre-signed-upload-signaturedoesnotmatch). Edited by: hotbelgo on Jul 23, 2016 1:26 AM"], "usr-2": ["Turns out that the \\r\\n are only added when there in your html, as there is the AWS example code. Without that, everything behaves more normally."]}, "Question": "I'm struggling to emulate the test signatures at http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html . I can eventually get these answers but only by very careful attention to \\r\\n sequences in my test examples (e.g. note the double line break in the amazon example). I can't hope to get an exact amount of line spaces when I start to work with Elixir data structures and their json encoders, and so far at least I'm getting a series of SignatureDoesNotMatch errors (which are of course hard to debug, see also see http://stackoverflow.com/questions/38525846/s3-pre-signed-upload-signaturedoesnotmatch ). Edited by: hotbelgo on Jul 23, 2016 1:26 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235868&tstart=75"}{"Answers": {"usr-1": ["Hello, Is there a way to get event notifications (i.e. to SQS) for objects that transition to Glacier? I saw in another forum post that with bucket logging enabled, there are S3.EXPIRE.OBJECT and S3.TRANSITION.OBJECT log entries, but what I really need is an SQS notification on these actions. Thanks, Joe"], "usr-2": ["I opened a support case and was informed that this feature does not yet exist, but that a feature request is being submitted: \"Unfortunately, Expiration and Transition of objects from lifecycle rules do not trigger event notifications. \"You will not receive event notifications from automatic deletes from lifecycle policies or from failed operations.\" http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html#notification-how-to-event-types-and-destinations However, as mentioned, I will be putting in a feature request to have this added to S3 on your behalf. I can't speak to when or if this feature will be added, but I suggest following our S3 blog for news about our latest releases: S3 Blog: https://aws.amazon.com/blogs/aws/category/amazon-s3/ \""]}, "Question": "Hello, Is there a way to get event notifications (i.e. to SQS) for objects that transition to Glacier? I saw in another forum post that with bucket logging enabled, there are S3.EXPIRE.OBJECT and S3.TRANSITION.OBJECT log entries, but what I really need is an SQS notification on these actions. Thanks, Joe", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=235364&tstart=75"}{"Answers": {"usr-1": ["Well, I don't know what was wrong, but I was able to fix it. I moved my DNS zone over to Route53 (I had it at my registrar, where I'd put it while migrating things). Now that the FQDN is actually an alias to the AWS bucket directly (instead of cname shenanigans) it Just Works. I did have to wipe out my redirect config as that was causing a loop, but the bucket policy, index and error pages, etc seems to be working now. Original issue follows, so as to help anyone else who comes along this same path. So, I've no idea what I've done wrong, so I'll include what I've got set up currently with a minimum of redaction, so that you can see for yourself how it's behaving. Theoretically I have my account set up such that there's no harm in the public seeing any of this (MFA etc). I've swapped out part of the URL (look for 'EXAMPLE.COM, replace with draeath.net) to stop indexer bots and the casually malicious, so just swap that text if you'd like to have a peek yourself. TL;DR: my specified index and error pages are not used, yet you are able to directly request the pages I've specified (or indeed any object, as is my intent). Clearly, I've done something wrong or massively misunderstood how this is supposed to function? Any help you might offer would be helpful! Again, replace any occurrences of \"EXAMPLE.COM\" in the below with \"draeath.net\" Bucket name in question: video-archive.EXAMPLE.COM URL that I'm using: http://video-archive.EXAMPLE.COM CNAME configuration for the above: \"video-archive IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com.\" DNS resolution test looks OK to me: $ dig video-archive.EXAMPLE.COM CNAME | grep -A1 \"ANSWER SECTION\" ;; ANSWER SECTION: video-archive.EXAMPLE.COM. 1798 IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com. Bucket policy: { \"Version\": \"2012-10-17\", \"Id\": \"Policy1469943355310\", \"Statement\": [ { \"Sid\": \"Stmt1469943336200\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"MyPrincipalWasHereButSnippedForObviousReasons\" }, \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::video-archive.EXAMPLE.COM/*\" }, { \"Sid\": \"Stmt1469943353219\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::video-archive.EXAMPLE.COM/*\" } ] } CORS config: (not that I'm using any scripting) <?xml version=\"1.0\" encoding=\"UTF-8\"?> <CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"> <CORSRule> <AllowedOrigin>*</AllowedOrigin> <AllowedMethod>GET</AllowedMethod> <AllowedMethod>HEAD</AllowedMethod> <MaxAgeSeconds>3000</MaxAgeSeconds> <AllowedHeader>Authorization</AllowedHeader> </CORSRule> </CORSConfiguration> I've also added custom redirect rules after trying things as above, but this seems to have done exactly nothing: <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/index.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>401</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>403</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>405</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>406</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> </RoutingRules> I have \"enable website hosting\" enabled on the bucket, with \"index.html\" for index page and \"error.html\" for the error page. Both of these objects exist in the root of the bucket with those names exactly - they are tiny HTML documents with appropriate Content-Type metadata. They are accessible when directly accessed, eg like so: index.html error.html Yet, finally, if you try to access the root of the bucket you get the standard AccessDenied XML back. If you try to access a folder in the bucket without a trailing slash, you the same, and if you include the slash you get a 0-byte octet-stream. Edited by: draeath on Jul 31, 2016 9:44 AM - adding routing config XML Edited by: draeath on Jul 31, 2016 10:36 AM - solved my issue"]}, "Question": "Well, I don't know what was wrong, but I was able to fix it. I moved my DNS zone over to Route53 (I had it at my registrar, where I'd put it while migrating things). Now that the FQDN is actually an alias to the AWS bucket directly (instead of cname shenanigans) it Just Works. I did have to wipe out my redirect config as that was causing a loop, but the bucket policy, index and error pages, etc seems to be working now. Original issue follows, so as to help anyone else who comes along this same path. So, I've no idea what I've done wrong, so I'll include what I've got set up currently with a minimum of redaction, so that you can see for yourself how it's behaving. Theoretically I have my account set up such that there's no harm in the public seeing any of this (MFA etc). I've swapped out part of the URL (look for 'EXAMPLE.COM, replace with draeath.net) to stop indexer bots and the casually malicious, so just swap that text if you'd like to have a peek yourself. TL;DR: my specified index and error pages are not used, yet you are able to directly request the pages I've specified (or indeed any object, as is my intent). Clearly, I've done something wrong or massively misunderstood how this is supposed to function? Any help you might offer would be helpful! Again, replace any occurrences of 'EXAMPLE.COM' in the below with 'draeath.net' Bucket name in question: video-archive.EXAMPLE.COM URL that I'm using: http://video-archive.EXAMPLE.COM CNAME configuration for the above: 'video-archive IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com.' DNS resolution test looks OK to me: $ dig video-archive.EXAMPLE.COM CNAME | grep -A1 'ANSWER SECTION' ;; ANSWER SECTION: video-archive.EXAMPLE.COM. 1798 IN CNAME video-archive.EXAMPLE.COM.s3.amazonaws.com. Bucket policy: { 'Version' : '2012-10-17' , 'Id' : 'Policy1469943355310' , 'Statement' : [ { 'Sid' : 'Stmt1469943336200' , 'Effect' : 'Allow' , 'Principal' : { 'AWS' : 'MyPrincipalWasHereButSnippedForObviousReasons' } , 'Action' : 's3:-' , 'Resource' : 'arn:aws:s3:::video-archive.EXAMPLE.COM/-' } , { 'Sid' : 'Stmt1469943353219' , 'Effect' : 'Allow' , 'Principal' : '-' , 'Action' : 's3:GetObject' , 'Resource' : 'arn:aws:s3:::video-archive.EXAMPLE.COM/-' } ] } CORS config: (not that I'm using any scripting) <?xml version= '1.0' encoding= 'UTF-8' ?> <CORSConfiguration xmlns= 'http://s3.amazonaws.com/doc/2006-03-01/' > <CORSRule> <AllowedOrigin>-</AllowedOrigin> <AllowedMethod>GET</AllowedMethod> <AllowedMethod>HEAD</AllowedMethod> <MaxAgeSeconds>3000</MaxAgeSeconds> <AllowedHeader>Authorization</AllowedHeader> </CORSRule> </CORSConfiguration> I've also added custom redirect rules after trying things as above, but this seems to have done exactly nothing: <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/index.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>401</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>403</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>405</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals>406</HttpErrorCodeReturnedEquals> </Condition> <Redirect><ReplaceKeyWith>/error.html</ReplaceKeyWith></Redirect> </RoutingRule> </RoutingRules> I have 'enable website hosting' enabled on the bucket, with 'index.html' for index page and 'error.html' for the error page. Both of these objects exist in the root of the bucket with those names exactly - they are tiny HTML documents with appropriate Content-Type metadata. They are accessible when directly accessed, eg like so: index.html error.html Yet, finally, if you try to access the root of the bucket you get the standard AccessDenied XML back. If you try to access a folder in the bucket without a trailing slash, you the same, and if you include the slash you get a 0-byte octet-stream. Edited by: draeath on Jul 31, 2016 9:44 AM - adding routing config XML Edited by: draeath on Jul 31, 2016 10:36 AM - solved my issue", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236380&tstart=75"}{"Answers": {"usr-1": ["I added this policy to one of our buckets, and now I can't get access even from root. How I can remove or reset this policy? { \"Sid\": \"Access-to-specific-VPC-only\", \"Action\": \"s3:*\", \"Effect\": \"Deny\", \"Resource\": [\"arn:aws:s3:::examplebucket\", \"arn:aws:s3:::examplebucket/*\"], \"Condition\": { \"StringNotEquals\": { \"aws:sourceVpc\": \"vpc-********\" } }, \"Principal\": \"*\" }"], "usr-2": ["I solved this issue by myself"]}, "Question": "I added this policy to one of our buckets, and now I can't get access even from root. How I can remove or reset this policy? { 'Sid' : 'Access-to-specific-VPC-only' , 'Action' : 's3:-' , 'Effect' : 'Deny' , 'Resource' : [ 'arn:aws:s3:::examplebucket' , 'arn:aws:s3:::examplebucket/-' ], 'Condition' : { 'StringNotEquals' : { 'aws:sourceVpc' : 'vpc---------' } } , 'Principal' : '-' }", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241175&tstart=25"}{"Answers": {"usr-1": ["Hello, I am attempting to access an S3 Bucket that's in my Dev Account from my Prod Account using IAM Roles. I have a server role on the Instance in the Prod account and a policy to allow the role to access the S3 Bucket in the Dev Account. I then have a bucket policy allowing the Prod 'IAM root' access to the bucket. When I login to the instance and attempt to curl or use AWS cli I get Access Denied. I've checked to make sure the instance was getting Credentials and sure enough it is. I'm a bit lost on where else to go to check. Please any help would be greatly appreciated! Here is my IAM Policy in my Prod Account attached to my role: Show Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1476454325248\", \"Action\": [ \"s3:Get*\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::my-bucket/*\" }, { \"Sid\": \"Stmt1476454375685\", \"Action\": [ \"s3:List*\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::my-bucket/\" } ] } Here is my S3 bucket Policy in my Dev Account: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::my-account-number:root\" ] }, \"Action\": \"s3:Get*\", \"Resource\": \"arn:aws:s3:::my-bucket/*\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::my-account:root\" ] }, \"Action\": \"s3:List*\", \"Resource\": \"arn:aws:s3:::my-bucket/*\" } ] }"], "usr-2": ["Nvm. I figured it out."]}, "Question": "Hello, I am attempting to access an S3 Bucket that's in my Dev Account from my Prod Account using IAM Roles. I have a server role on the Instance in the Prod account and a policy to allow the role to access the S3 Bucket in the Dev Account. I then have a bucket policy allowing the Prod 'IAM root' access to the bucket. When I login to the instance and attempt to curl or use AWS cli I get Access Denied. I've checked to make sure the instance was getting Credentials and sure enough it is. I'm a bit lost on where else to go to check. Please any help would be greatly appreciated! Here is my IAM Policy in my Prod Account attached to my role: Show Policy { 'Version' : '2012-10-17' , 'Statement' : [ { 'Sid' : 'Stmt1476454325248' , 'Action' : [ 's3:Get-' ], 'Effect' : 'Allow' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } , { 'Sid' : 'Stmt1476454375685' , 'Action' : [ 's3:List-' ], 'Effect' : 'Allow' , 'Resource' : 'arn:aws:s3:::my-bucket/' } ] } Here is my S3 bucket Policy in my Dev Account: { 'Version' : '2012-10-17' , 'Statement' : [ { 'Effect' : 'Allow' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::my-account-number:root' ] } , 'Action' : 's3:Get-' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } , { 'Effect' : 'Allow' , 'Principal' : { 'AWS' : [ 'arn:aws:iam::my-account:root' ] } , 'Action' : 's3:List-' , 'Resource' : 'arn:aws:s3:::my-bucket/-' } ] }", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241204&tstart=25"}{"Answers": {"usr-1": ["Here are links to four files that I uploaded in the last week, but have now disappeared from my bucket: https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-naypyidaw.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-london.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-brussels.jpg https://s3.amazonaws.com/gh-resource/ghImage/SWjqzgXy9rGCYvpRF-ottawa.jpg I know they successfully uploaded because I saw them on my website multiple times before they disappeared. The last file above (ottawa), I just now re-uploaded, so that I could look at the permissions and see if there was an expiry date or expiry rule. Permissions give everyone read/download permission and all permissions to myself. Expiry date is None, expiry rule is N/A. This has been happening regularly for the last year or so. What could be causing this?"], "usr-2": ["OK, I got some good advice on StackOverflow. If you ever have this same experience that I had, enable logging on your bucket. Then you can see who's deleting the files and when they are being deleted. In my case, I enabled the logging, but then never even had to look at the log to figure out the problem. Since I was the only one with access to the files, I knew it had to be me. And it was, some old code that I meant to go back and fix, but never did. Once I realized that it had to be me, I was able to find the problem. Lesson learned -- files with no expiry date and no expiry rule don't just disappear. If they seem to, then there's more to the story."]}, "Question": "Here are links to four files that I uploaded in the last week, but have now disappeared from my bucket: https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-naypyidaw.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-london.jpg https://gh-resource.s3.amazonaws.com/ghImage/SWjqzgXy9rGCYvpRF-brussels.jpg https://s3.amazonaws.com/gh-resource/ghImage/SWjqzgXy9rGCYvpRF-ottawa.jpg I know they successfully uploaded because I saw them on my website multiple times before they disappeared. The last file above (ottawa), I just now re-uploaded, so that I could look at the permissions and see if there was an expiry date or expiry rule. Permissions give everyone read/download permission and all permissions to myself. Expiry date is None, expiry rule is N/A. This has been happening regularly for the last year or so. What could be causing this?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241229&tstart=25"}{"Answers": {"usr-1": ["AWS CLI: creating a new bucket in ap-southeast-2 always returns error: data must be a byte string. Same command works perfectly fine for other regions. Any idea why I can not create S3 buckets in Sydney? # aws s3 mb s3://test-1476747511 --region ap-southeast-2 make_bucket failed: s3://test-1476747511/ data must be a byte string # aws s3 mb s3://test-1476747511 --region us-east-1 make_bucket: s3://test-1476747511/ # aws --region us-east-1 s3 ls | grep test 2016-10-18 10:48:07 test-1476747511 # aws --version aws-cli/1.7.18 Python/2.7.9 Linux/3.16.0-4-amd64 \u00a0 # dpkg -s python-openssl | grep 'Version' Version: 0.14-1 Any suggestions welcome"], "usr-2": ["...and update to the latest version of awscli fixed the issue."]}, "Question": "AWS CLI: creating a new bucket in ap-southeast-2 always returns error: data must be a byte string . Same command works perfectly fine for other regions. Any idea why I can not create S3 buckets in Sydney? # aws s3 mb s3: //test-1476747511 --region ap-southeast-2 make_bucket failed: s3: //test-1476747511/ data must be a byte string # aws s3 mb s3: //test-1476747511 --region us-east-1 make_bucket: s3: //test-1476747511/ # aws --region us-east-1 s3 ls | grep test 2016-10-18 10:48:07 test-1476747511 # aws --version aws-cli/1.7.18 Python/2.7.9 Linux/3.16.0-4-amd64 \u00a0 # dpkg -s python-openssl | grep 'Version' Version: 0.14-1 Any suggestions welcome", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241310&tstart=25"}{"Answers": {"usr-1": ["I created a new S3 bucket in the also new Ohio region. This announcement from AWS says you can use S3 in Ohio now. So.... ...I use a Gulp build script to compress and upload to S3. I have used this for years. I gave my code upload user IAM permissions to the new bucket and tried to upload some code. NOPE Just to make sure I wasn't doing something dumb I created another new S3 bucket, only this time marking the region as US Standard instead of Ohio. IAM permissions, tried upload. YUP! So the only difference is the region specified when creating the bucket. Two questions Q1) I don't know why S3 buckets are tagged by location in the first place because aren't they supposed to be global? Q2) Why isn't Ohio up and running like the announcement says?"], "usr-2": ["I'm sorry you are running into trouble. I suspect that the reason you are having issues is that all new AWS regions require Sigv4 signing for S3 requests - http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html You will notice the note on this page: Amazon S3 supports Signature Version 4, a protocol for authenticating inbound API requests to AWS services, in all AWS regions. At this time, AWS regions created before January 30, 2014 will continue to support the previous protocol, Signature Version 2. Any new regions after January 30, 2014 will support only Signature Version 4 and therefore all requests to those regions must be made with Signature Version 4. For more information about AWS Signature Version 2, see Signing and Authenticating REST Requests in the Amazon Simple Storage Service Developer Guide. Also, while the bucket namespace is Global, AWS regions allow your data to be stored where you want it, close to your customers or close to your compute resources in AWS in order to provide the least latency for your applications. I hope this helps."], "usr-3": ["LeeK@AWS, Thanks for the heads up - I missed the nuance of how SigV4 application differed from region to region. Its confusing and not the ideal, but anyway, we are through it. Thank you. For others in a similar situation using Gulp to upload, the easy fix to this issue is to switch to a different Gulp module. This is the one we switched to and we now have no SigV4 issues. The other NPM S3 module referenced above is not being kept current."]}, "Question": "I created a new S3 bucket in the also new Ohio region. This announcement from AWS says you can use S3 in Ohio now. So.... ...I use a Gulp build script to compress and upload to S3. I have used this for years. I gave my code upload user IAM permissions to the new bucket and tried to upload some code. NOPE Just to make sure I wasn't doing something dumb I created another new S3 bucket, only this time marking the region as US Standard instead of Ohio. IAM permissions, tried upload. YUP! So the only difference is the region specified when creating the bucket. Two questions Q1) I don't know why S3 buckets are tagged by location in the first place because aren't they supposed to be global? Q2) Why isn't Ohio up and running like the announcement says?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242047&tstart=25"}{"Answers": {"usr-1": ["Hey, since 2 weeks my import job is pending and I'm waiting for AWS to complete the job but as of today my status changed to: JOB FOUND JobId: EU-E2KUH CreationDate: Fri Jul 08 10:26:10 CEST 2016 JobType: Import LocationCode: AtAWS LocationMessage: Your device is at AWS. ProgressCode: PendingCustomer ProgressMessage: The specified job requires action from customr's end. ErrorCount: 0 LogBucket: null LogKey: null Carrier: null TrackingNumber: null I didn't get any notifications so I'm wondering what the next steps are? Which actions do I have to perform? thx"], "usr-2": ["Hi ta2002. We have reached out to you for some information. If you have any questions or have not received our reach out email, please PM me and I can help you with this. Frank"]}, "Question": "Hey, since 2 weeks my import job is pending and I'm waiting for AWS to complete the job but as of today my status changed to: JOB FOUND JobId: EU-E2KUH CreationDate: Fri Jul 08 10:26:10 CEST 2016 JobType: Import LocationCode: AtAWS LocationMessage: Your device is at AWS. ProgressCode: PendingCustomer ProgressMessage: The specified job requires action from customr's end. ErrorCount: 0 LogBucket: null LogKey: null Carrier: null TrackingNumber: null I didn't get any notifications so I'm wondering what the next steps are? Which actions do I have to perform? thx", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236125&tstart=75"}{"Answers": {"usr-1": ["Hi, Are there any suggested strategies for blue-green deployment for a site that is hosted on s3? Ideally one that is behind cloud front. Thanks, Michal. Edited by: michalc18 on Oct 28, 2016 11:22 PM"], "usr-2": ["I've come up with a way where you put the HTML of the site behind an API Gateway. Details at http://charemza.name/blog/posts/devops/aws/blue-green-deployment-static-site-s3/ ."]}, "Question": "Hi, Are there any suggested strategies for blue-green deployment for a site that is hosted on s3? Ideally one that is behind cloud front. Thanks, Michal. Edited by: michalc18 on Oct 28, 2016 11:22 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242153&tstart=25"}{"Answers": {"usr-1": ["Hello there! I'm building a mobile app (iOS / Android) which connects to an S3 bucket in order to download files. I would also like to replicate the bucket to another region, but I was wondering: how does the code in my app know which region to access? If two buckets with the same content were accessible, would the AWS sdk code connect to the fastest region? Should I be the one who specifies which region to use inside the app? If so, how? Please advice! Thanks in advance."], "usr-10": ["If you're using CloudFront and have your custom cert there, just set up your API Gateway endpoint as an additional origin server in your CloudFront distribution, then create a cache behavior with a path pattern like /api* and route it to the new origin -- one stone, two birds -- you have your custom domain and SSL with API Gateway and you have eliminated some potential cross-origin issues. However... counting objects in a bucket isn't something you will want to be doing in a high performance/high traffic path in your application. It will get expensive and will not scale well. Storing a count somewhere else, whether RDS, DynamoDB, SimpleDB, or even as a JSON object in S3 is likely to be a better idea. S3 event notifications could be used to trigger code that maintains such a count."], "usr-2": ["colto wrote: If two buckets with the same content were accessible, would the AWS sdk code connect to the fastest region? Bucket names are globally unique meaning that these would have different names. The SDK uses the bucket name you specify. Should I be the one who specifies which region to use inside the app? If so, how? I can probably come up with a way or two to figure out the best region to use. But first, have you considered CloudFront? It's a content delivery network that will cache the content closer to the users. You would basically set up a single S3 bucket and point CloudFront there. When your app requests some content, the traffic will automatically go through the best CloudFront edge location. These are spread all over the world. You get an optimal connection to S3 and your content is cached at edge locations when it is being accessed, meaning that you often don't need to go the whole way back to S3."], "usr-3": ["I did not know about CloudFront, but it looks like it could work in my scenario. I'll try to use it and report back if I have issues. Edit: do I need more code to instruct the app to work with CloudFront or does AWS automatically understand that there's a CloudFront interface in between? Thanks for your help! Edited by: colto on Oct 26, 2016 12:39 AM Edited by: colto on Oct 26, 2016 1:15 AM"], "usr-4": ["colto wrote: do I need more code to instruct the app to work with CloudFront or does AWS automatically understand that there's a CloudFront interface in between? When you set up CloudFront, you get an address looking something like \"d123456.cloudfront.net\" (you can optionally connect your own custom domain). Let's say that you have a file in your S3 bucket called \"example.png\" and that CloudFront has been configured to get files from that bucket. Your app would then call http://d123456.cloudfront.net/example.png. The request is routed to the nearest CloudFront server location automatically and CloudFront then fetches example.png from S3 or the local cache at that location. The AWS SDK is not involved."], "usr-5": ["+1 for Cloudfront. Its the bomb. Also the reporting Cloudfront produces is super helpful."], "usr-6": ["Thank you so much guys, it worked! AWS is truly amazing. Just one last question: for app internal logic reasons, I need to know the number of items in the bucket when I open the app. Right now I use the AWS SDK which has a specific function to do so, but I would like to get rid of it if possibile and replace it with CloudFront links (thus, allowing me to remove the AWS SDK altogether). How would you proceed? I was thinking of a 'lambda function' made available using 'API gateway' that reads the number of items and reports back. Is that possible? What's your advice? I'm pretty new to AWS! Thanks in advance!"], "usr-7": ["The question was answered but I asked for further clarification on the same subject and did not want to open a new thread"], "usr-8": ["colto wrote: I was thinking of a 'lambda function' made available using 'API gateway' that reads the number of items and reports back. That sounds like a nice approach in my opinion. It's a relatively lightweight solution. I have found the API Gateway to be unnecessarily complicated to work with, but a recent announcement seem to have changed that. As described in the blog post API Gateway Update \u2013 New Features Simplify API Development you can now basically map everything to Lambda and have the function look at the request and act accordingly. It's mostly about your Lambda function - very little work on the API Gateway side."], "usr-9": ["Appreciate that link - will mos def catch up on recent API Gateway changes. We abandoned API Gateway over the issue of custom SSL's via Certificate Manager not being able to work with API Gateway as they do with Cloudfront. Also, the complexity issue is (was?) steep."]}, "Question": "Hello there! I'm building a mobile app (iOS / Android) which connects to an S3 bucket in order to download files. I would also like to replicate the bucket to another region, but I was wondering: how does the code in my app know which region to access? If two buckets with the same content were accessible, would the AWS sdk code connect to the fastest region? Should I be the one who specifies which region to use inside the app? If so, how? Please advice! Thanks in advance.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=241824&tstart=25"}{"Answers": {"usr-1": ["I just created a bucket called \"minio-test\" and haven't changed any default settings. I then created a new user in IAM called \"miniotester\" and attached the following policy to give that user full access to the bucket: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::minio-test/*\" ] } ] } I then wanted to test if that worked as expected so I entered \"miniotester\"'s credentials under \"default\" in ~/.aws/credentials and tried the following command: $ aws s3 ls minio-test/\u00a0 \u00a0 An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I suppose my policy is incorrect? I recall using that same policy for other buckets in the past and it worked. Maybe it takes some time for the policy to take effect? Anyone knows what I'm doing wrong? Edited by: olalondee on Oct 30, 2016 5:53 PM Edited by: olalondee on Oct 30, 2016 5:53 PM"], "usr-2": ["Ok, the following policy seemed to fix the problem: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::minio-test\", \"arn:aws:s3:::minio-test/*\" ] } ] } Edited by: olalondee on Oct 30, 2016 6:10 PM"]}, "Question": "I just created a bucket called 'minio-test' and haven't changed any default settings. I then created a new user in IAM called 'miniotester' and attached the following policy to give that user full access to the bucket: { 'Version' : '2012-10-17' , 'Statement' : [ { 'Effect' : 'Allow' , 'Action' : [ 's3:-' ], 'Resource' : [ 'arn:aws:s3:::minio-test/-' ] } ] } I then wanted to test if that worked as expected so I entered 'miniotester''s credentials under 'default' in ~/.aws/credentials and tried the following command: $ aws s3 ls minio-test/\u00a0 \u00a0 An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I suppose my policy is incorrect? I recall using that same policy for other buckets in the past and it worked. Maybe it takes some time for the policy to take effect? Anyone knows what I'm doing wrong? Edited by: olalondee on Oct 30, 2016 5:53 PM Edited by: olalondee on Oct 30, 2016 5:53 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242197&tstart=25"}{"Answers": {"usr-1": ["Hi all, I am looking for a way to prevent certain users being added to access logs. so far i have accessed s3 maybe 4 times myself and instead of getting 4 files with the information contained within it i have found that the internal amazon account (presumably the one that delivers logs and handles other things) has made around 1200 files of varying sizes (on average one every 150 seconds) that just contain utterly useless logs, i am interested when someone else accesses the bucket not when amazon does something - generally i don't believe i should have to pay for the software itself to log itself accessing itself. How do i go about removing certain accounts from being logged within the access logs? if i can't i imagine i will be sacking access logs off for that bucket!"], "usr-2": ["I'm sorry to hear that you are getting access log records for object accesses you're not interested in logging. S3 Access Logging is either enabled or disabled for a given bucket, and when enabled, it records all accesses. You may use a prefix within a bucket or a separate bucket to distinguish your access logs from other objects. Then, using S3 Lifecycle policies, you can expire (delete) access logs and/or transition them to the lower cost Standard - Infrequent Access storage class as discussed here: http://docs.aws.amazon.com/AmazonS3/latest/dev/deleting-log-files-lifecycle.html ."]}, "Question": "Hi all, I am looking for a way to prevent certain users being added to access logs. so far i have accessed s3 maybe 4 times myself and instead of getting 4 files with the information contained within it i have found that the internal amazon account (presumably the one that delivers logs and handles other things) has made around 1200 files of varying sizes (on average one every 150 seconds) that just contain utterly useless logs, i am interested when someone else accesses the bucket not when amazon does something - generally i don't believe i should have to pay for the software itself to log itself accessing itself. How do i go about removing certain accounts from being logged within the access logs? if i can't i imagine i will be sacking access logs off for that bucket!", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242239&tstart=25"}{"Answers": {"usr-1": ["Hello, we are trying to use squid in front of s3. Is there a way we can configure the sdk to use this proxy for all requests? Or will we have to just use an http client and sign the requests ourselves?"], "usr-2": ["I'll use the presigned url"]}, "Question": "Hello, we are trying to use squid in front of s3. Is there a way we can configure the sdk to use this proxy for all requests? Or will we have to just use an http client and sign the requests ourselves?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=242751&tstart=25"}{"Answers": {"usr-1": ["I am trying to generated a signed S3 URL from within a Lambda function, and I can't get it to work. I have this code in Lambda: var aws = require('aws-sdk'); var s3 = new aws.S3({signatureVersion: 'v4'}); var bucket = 'test-bucket'; var params = {Bucket: bucket, Key: \"test-file\"}; s3.getSignedUrl('getObject', params, function(err, url) { console.log(\"Signed URL: \" + url); } That returns a URL that looks like: https://test-bucket.s3.amazonaws.com/test-file?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=<key>%2F20161116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20161116T150249Z&X-Amz-Expires=1000&X-Amz-Security-Token=FQoDYXdzEPD%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDAP7Q6dx5kOs53eXMyLuASsKNCdHGHJKGhjfGUFgufgH36Fhv05MYR5S%2B%2BHustV81iYG4c21yCJZnV2Z1tBFrqn26hzPfZGXllMS2SiMituXGmCw7sscA8F855aVyMbcDKWpjn4%2FsVzfZQ76Gdzk6dKctemxyt9r5WYMk2IgQtgFeoyiQk%2BliWFkIqCqs3M55fqEj5HMQk4kyPwdXznfo0fXzmtafHrUFSHHo6sgfCgi18i8XYfPF3ibj7vu%2FyyKCYnc4ABzyFPC%2B%2B3gylA3%2B0pS7cUPqRZrmOOu19zmj7mTKOvQOVkWDRjWfVHeic%2BlwPWVomnvsoDoIo%2FNmxwQU%3D&X-Amz-Signature=e53abc5b096hjkg7679tyfft78rt6yifkft7b6ab7f68479c3be&X-Amz-SignedHeaders=host I stick that URL in a web browser and I just get an \"Access Denied\" message. I can successfully create a signed url for the same file using the Bucket Explorer application-- this generate a URL in the form: https://test-bucket.s3.amazonaws.com/test-file?AWSAccessKeyId=&ltAccess Key Removed&gt&Expires=1479396462&Signature=qv8tddF6hjkGHKGHKjghjkGHONgEM%3D I wanted to try to replicate this format to test my signature but is requires the expiration timestamp instead of a count..... So, what am I doing wrong in my request? Has anyone successfully used Lambda to generate signed URLs? Edited by: youarenotagadget on Nov 16, 2016 9:26 AM"], "usr-2": ["When in doubt, double check the permissions your Lambda has!"]}, "Question": "I am trying to generated a signed S3 URL from within a Lambda function, and I can't get it to work. I have this code in Lambda: var aws = require( 'aws-sdk' ); var s3 = new aws.S3( { signatureVersion: 'v4' } ); var bucket = 'test-bucket' ; var params = { Bucket: bucket, Key: 'test-file' } ; s3.getSignedUrl( 'getObject' , params, function(err, url) { console.log( 'Signed URL: ' + url); } That returns a URL that looks like: https://test-bucket.s3.amazonaws.com/test-file?X-Amz-Algorithm=AWS4-HMAC-SHA256andX-Amz-Credential= <key>\\%2F20161116\\%2Fus-east-1\\%2Fs3\\%2Faws4_requestandX-Amz-Date=20161116T150249ZandX-Amz-Expires=1000andX-Amz-Security-Token=FQoDYXdzEPD\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2F\\%2FwEaDAP7Q6dx5kOs53eXMyLuASsKNCdHGHJKGhjfGUFgufgH36Fhv05MYR5S\\%2B\\%2BHustV81iYG4c21yCJZnV2Z1tBFrqn26hzPfZGXllMS2SiMituXGmCw7sscA8F855aVyMbcDKWpjn4\\%2FsVzfZQ76Gdzk6dKctemxyt9r5WYMk2IgQtgFeoyiQk\\%2BliWFkIqCqs3M55fqEj5HMQk4kyPwdXznfo0fXzmtafHrUFSHHo6sgfCgi18i8XYfPF3ibj7vu\\%2FyyKCYnc4ABzyFPC\\%2B\\%2B3gylA3\\%2B0pS7cUPqRZrmOOu19zmj7mTKOvQOVkWDRjWfVHeic\\%2BlwPWVomnvsoDoIo\\%2FNmxwQU\\%3DandX-Amz-Signature=e53abc5b096hjkg7679tyfft78rt6yifkft7b6ab7f68479c3beandX-Amz-SignedHeaders=host I stick that URL in a web browser and I just get an 'Access Denied' message. I can successfully create a signed url for the same file using the Bucket Explorer application-- this generate a URL in the form: https://test-bucket.s3.amazonaws.com/test-file?AWSAccessKeyId=andltAccess Key RemovedandgtandExpires=1479396462andSignature=qv8tddF6hjkGHKGHKjghjkGHONgEM\\%3D I wanted to try to replicate this format to test my signature but is requires the expiration timestamp instead of a count..... So, what am I doing wrong in my request? Has anyone successfully used Lambda to generate signed URLs? Edited by: youarenotagadget on Nov 16, 2016 9:26 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243212&tstart=25"}{"Answers": {"usr-1": ["Hi I have two objects at the same prefix in a bucket, both are visible from the cli but only one shows up on my s3 console. They both have the same owner and object acl's, were both uploaded using command \"aws deploy push\" from cli. The object not showing in s3 console is test.zip please advise. Kind regards, Ethan"], "usr-2": ["test.zip was originally uploaded using \"aws deploy push\" but with no object name specified. That caused strange behavior in the console and attached object ACL. I subsequently modified the object to be same as the correctly configured object but when that didn't help I asked this question, but by the time I finished typing the question the changes had taken effect and resolved the issue. I forgot that immediate read-after-write is only available for PUTS of new objects with eventually consistency being in effect thereafter for the object."]}, "Question": "Hi I have two objects at the same prefix in a bucket, both are visible from the cli but only one shows up on my s3 console. They both have the same owner and object acl's, were both uploaded using command 'aws deploy push' from cli. The object not showing in s3 console is test.zip please advise. Kind regards, Ethan", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243843&tstart=25"}{"Answers": {"usr-1": ["I've set up my static S3 website, using a custom domain from Google Domains. In the Google Domains DNS settings, I am redirecting example.com to www.example.com, and I have this CNAME record in place: Name: \"www\" Type: CNAME TTL: 5m Data: example.com.s3-website-us-east-1.amazonaws.com. Thus, I successfully load my website when I navigate to either www.example.com or example.com (which just redirects). BUT - the domain is still served from \"http://example.com.s3-website-us-east-1.amazonaws.com\"! I've followed the walkthroughs closely thus far. I assume there must be a way to serve my website from the \"example.com\" domain without switching over to Route 53, right? Do I not have my CNAME configured correctly? Anyone? Bueller?"], "usr-2": ["This will happen if you for example redirect to the full S3 static website address instead of your own domain. That may not be the case here. If you are willing to share the domain name (PM is fine), I would be happy to take a quick look."], "usr-3": ["Thank you - I'll PM. But do you mean redirect in the DNS from www.example.com to my S3 bucket (e.g., example.com.s3-website-us-east-1.amazonaws.com)?"], "usr-4": ["I'm following up on your PM here. The redirection from example.com to www handled by Google works fine. However, the www.example.com S3 bucket is then redirecting to the full example.com.s3-website... static website address where the actual content is currently located. You may want to 1.) disable the redirection in the www S3 bucket static website settings, and 2.) move the content from the example.com bucket to the www bucket."], "usr-5": ["This worked! Thank you."]}, "Question": "I've set up my static S3 website, using a custom domain from Google Domains. In the Google Domains DNS settings, I am redirecting example.com to www.example.com, and I have this CNAME record in place: Name: 'www' Type: CNAME TTL: 5m Data: example.com.s3-website-us-east-1.amazonaws.com. Thus, I successfully load my website when I navigate to either www.example.com or example.com (which just redirects). BUT - the domain is still served from 'http://example.com.s3-website-us-east-1.amazonaws.com'! I've followed the walkthroughs closely thus far. I assume there must be a way to serve my website from the 'example.com' domain without switching over to Route 53, right? Do I not have my CNAME configured correctly? Anyone? Bueller?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244074&tstart=25"}{"Answers": {"usr-1": ["I have the following Java code: public boolean writeToS3(String regionName, String key, byte[] contentAsBytes) { if (environment.equals(\"stage\") || environment.equals(\"prods\")) { AmazonS3 s3client = new AmazonS3Client(new InstanceProfileCredentialsProvider()); Region region = Region.getRegion(Regions.fromName(regionName)); s3client.setRegion(region); ByteArrayInputStream contentsAsStream = new ByteArrayInputStream(contentAsBytes); ObjectMetadata md = new ObjectMetadata(); md.setContentLength(contentAsBytes.length); PutObjectRequest putRequest = new PutObjectRequest(bucketName, key, contentsAsStream, md); int times = FlightEventBackupServiceImpl.maxGetTimes; do { try { s3client.putObject(putRequest); // error from this line } catch (AmazonClientException ace) { if (times == 1) { LOGGER.error(\"failed to write to S2\", ace); return false; } } } while (--times > 0); } return true; } It worked fine for several weeks. Starting yesterday or today it stopped working and I got the following error: 11 Dec 2016 21:40:06,141 ERROR: com.gogoair.flighteventsaving.business.S3ServiceImpl - failed to write to S2 com.amazonaws.AmazonClientException: Data read has a different length than the expected: dataLength=0; expectedLength=1631; includeSkipped=false; in.getClass()=class com.amazonaws.internal.ReleasableInputStream; markedS upported=true; marked=0; resetSinceLastMarked=false; markCount=1; resetCount=0 at com.amazonaws.util.LengthCheckInputStream.checkLength(LengthCheckInputStream.java:150) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:110) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream.read(MD5DigestCalculatingInputStream.java:98) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:151) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at org.apache.http.entity.InputStreamEntity.writeTo(InputStreamEntity.java:142) at com.amazonaws.http.RepeatableInputStreamRequestEntity.writeTo(RepeatableInputStreamRequestEntity.java:153) at org.apache.http.entity.HttpEntityWrapper.writeTo(HttpEntityWrapper.java:96) at org.apache.http.impl.client.EntityEnclosingRequestWrapper$EntityWrapper.writeTo(EntityEnclosingRequestWrapper.java:112) at org.apache.http.impl.entity.EntitySerializer.serialize(EntitySerializer.java:117) at org.apache.http.impl.AbstractHttpClientConnection.sendRequestEntity(AbstractHttpClientConnection.java:265) at org.apache.http.impl.conn.ManagedClientConnectionImpl.sendRequestEntity(ManagedClientConnectionImpl.java:203) at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doSendRequest(SdkHttpRequestExecutor.java:63) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:685) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:487) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55) at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:843) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:597) at com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:363) at com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:329) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:308) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3659) at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1422) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:65) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:80) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.process(FlightEventBackupServiceImpl.java:134) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.backup(FlightEventBackupServiceImpl.java:95) at sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:108) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:745) Of course, each time the expectedLenth is different, but dataLength is always 0. Did AWS changed something on S3 recently? Thanks a lot! Daniel Edited by: danli33 on Dec 11, 2016 1:56 PM =========== By the way, the same code still works on SOX environment, but not work in stage. Edited by: danli33 on Dec 12, 2016 7:18 AM Edited by: danli33 on Dec 12, 2016 8:32 AM Edited by: danli33 on Dec 12, 2016 9:42 AM My bad. I should have add \"return true;\" after uploading the data."]}, "Question": "I have the following Java code: public boolean writeToS3(String regionName, String key, byte[] contentAsBytes) { if (environment.equals('stage') || environment.equals('prods')) { AmazonS3 s3client = new AmazonS3Client(new InstanceProfileCredentialsProvider()); Region region = Region.getRegion(Regions.fromName(regionName)); s3client.setRegion(region); ByteArrayInputStream contentsAsStream = new ByteArrayInputStream(contentAsBytes); ObjectMetadata md = new ObjectMetadata(); md.setContentLength(contentAsBytes.length); PutObjectRequest putRequest = new PutObjectRequest(bucketName, key, contentsAsStream, md); int times = FlightEventBackupServiceImpl.maxGetTimes; do { try { s3client.putObject(putRequest); // error from this line } catch (AmazonClientException ace) { if (times == 1) { LOGGER.error('failed to write to S2', ace); return false; } } } while (--times > 0); } return true; } It worked fine for several weeks. Starting yesterday or today it stopped working and I got the following error: 11 Dec 2016 21:40:06,141 ERROR: com.gogoair.flighteventsaving.business.S3ServiceImpl - failed to write to S2 com.amazonaws.AmazonClientException: Data read has a different length than the expected: dataLength=0; expectedLength=1631 ; includeSkipped=false; in.getClass()=class com.amazonaws.internal.ReleasableInputStream; markedS upported=true; marked=0; resetSinceLastMarked=false; markCount=1; resetCount=0 at com.amazonaws.util.LengthCheckInputStream.checkLength(LengthCheckInputStream.java:150) at com.amazonaws.util.LengthCheckInputStream.read(LengthCheckInputStream.java:110) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream.read(MD5DigestCalculatingInputStream.java:98) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at com.amazonaws.event.ProgressInputStream.read(ProgressInputStream.java:151) at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:72) at org.apache.http.entity.InputStreamEntity.writeTo(InputStreamEntity.java:142) at com.amazonaws.http.RepeatableInputStreamRequestEntity.writeTo(RepeatableInputStreamRequestEntity.java:153) at org.apache.http.entity.HttpEntityWrapper.writeTo(HttpEntityWrapper.java:96) at org.apache.http.impl.client.EntityEnclosingRequestWrapper$EntityWrapper.writeTo(EntityEnclosingRequestWrapper.java:112) at org.apache.http.impl.entity.EntitySerializer.serialize(EntitySerializer.java:117) at org.apache.http.impl.AbstractHttpClientConnection.sendRequestEntity(AbstractHttpClientConnection.java:265) at org.apache.http.impl.conn.ManagedClientConnectionImpl.sendRequestEntity(ManagedClientConnectionImpl.java:203) at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:237) at com.amazonaws.http.protocol.SdkHttpRequestExecutor.doSendRequest(SdkHttpRequestExecutor.java:63) at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:122) at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:685) at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:487) at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55) at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:843) at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:597) at com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:363) at com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:329) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:308) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3659) at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1422) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:65) at com.gogoair.flighteventsaving.business.S3ServiceImpl.writeToS3(S3ServiceImpl.java:80) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.process(FlightEventBackupServiceImpl.java:134) at com.gogoair.flighteventsaving.business.FlightEventBackupServiceImpl.backup(FlightEventBackupServiceImpl.java:95) at sun.reflect.GeneratedMethodAccessor115.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:302) at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:108) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.lang.Thread.run(Thread.java:745) Of course, each time the expectedLenth is different, but dataLength is always 0. Did AWS changed something on S3 recently? Thanks a lot! Daniel Edited by: danli33 on Dec 11, 2016 1:56 PM =========== By the way, the same code still works on SOX environment, but not work in stage. Edited by: danli33 on Dec 12, 2016 7:18 AM Edited by: danli33 on Dec 12, 2016 8:32 AM Edited by: danli33 on Dec 12, 2016 9:42 AM My bad. I should have add 'return true;' after uploading the data.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244814&tstart=25"}{"Answers": {"usr-1": ["Hi all, I opted in to the new s3 management console. Great and simple design, bold and clear, much better than the old one. However, when trying to add a new metric filter, I encountered the following: 1. A filter tag is added (I added an s3 object tag) and the filter is created, however when editing the newly added filter, the tag isn't there. Also, the relevant cloudwatch metric displays wrong values (to be exact, it shows all values, the filter doesn't work at all). I want to clarify that tag names with dashes cannot be added. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics-filter.html 2. When editing bucket metric options, 'Request metrics' and 'Data transfer metrics' are both selected or not and the drop downs cannot be pinpointed. Seems like a javascript issue. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html -> Step 7 3. slashes (/) cannot be used at prefix, for example adding the prefix \"/Bucket01/BigData/SparkCluster\" fails with message ''Prefix search does not support the character '/'.\". Does that mean that only root prefixes can be used or the javascript validator doesn't work as expected? Prefix without slash can be added, but that limits the whole filter concept significantly. Also, the s3 faq states that slashes can be included (https://aws.amazon.com/s3/faqs/ -> storage management -> S3 CloudWatch Metrics -> Can I align storage metrics to my applications or business organizations?) Tried with latest firefox (50.0.2) and chrome (55.0.2883.75 (64-bit)) at macos sierra (10.12.1 (16B2555)) Regards, Thanos Edited by: thanos on Dec 2, 2016 11:20 AM"], "usr-2": ["Update Filter including tags cannot be added! The response is 'Error. We encountered an internal error. Please try again.' Procedure described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics-filter.html, the only difference is that value 'music' is a tag and not a prefix. Tried on 2 separate aws accounts. Regards, Thanos"], "usr-3": ["Hi Thanos, Thank you for reporting these issues. thanos wrote: 1. A filter tag is added (I added an s3 object tag) and the filter is created, however when editing the newly added filter, the tag isn't there. Also, the relevant cloudwatch metric displays wrong values (to be exact, it shows all values, the filter doesn't work at all). What may have happened here is that you didn't completely finish adding the tag to the filter by hitting Enter after entering the tag value. The console has been updated to prevent this from happening; can you please try adding a tag-based filter again? I want to clarify that tag names with dashes cannot be added. Hyphens in tag names are supported; you can see the supported characters for tags in the developer guide: http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html. It should be possible to set up a metrics filter for any valid tag. The console currently does not allow some characters that it should, but we're working on a fix for that; in the meantime, as a workaround, you can use the AWS CLI to configure metrics: http://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-metrics-configuration.html. 2. When editing bucket metric options, 'Request metrics' and 'Data transfer metrics' are both selected or not and the drop downs cannot be pinpointed. Seems like a javascript issue. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html -> Step 7 The S3 API currently only supports enabling or disabling them together. I agree that the screenshot in the documentation is misleading; I've requested that it be updated. 3. slashes (/) cannot be used at prefix, for example adding the prefix \"/Bucket01/BigData/SparkCluster\" fails with message ''Prefix search does not support the character '/'.\". The issue that prevented using slashes in prefixes has been resolved; can you please try this again? Please note however that your keys probably do not start with a slash, so in your example, if the bucket name is \"Bucket01\", the prefix value that you will want to enter is \"BigData/SparkCluster\". We encountered an internal error. Please try again. This message can appear when configuring a tag filter where the value is an empty string. We are working on a fix for this issue and hope to have it deployed soon. Thanks, Miles"], "usr-4": ["Hi Miles, Thank you for your great reply. Everything now works as described. The javascript validators work as expected, tags and prefixes can be added. It was a bit obscure that tags should be in the form '<tag key> | <tag value>', neither was it documented somewhere, the new javascript helper made that crystal clear. Regarding point 2, I hope that bucket metric pinpointing would be available soon. Regards, Thanos"], "usr-5": ["milesaws wrote: This message can appear when configuring a tag filter where the value is an empty string. We are working on a fix for this issue and hope to have it deployed soon. Metric filters that include tags with empty values are now fully supported. -Miles"]}, "Question": "Hi all, I opted in to the new s3 management console. Great and simple design, bold and clear, much better than the old one. However, when trying to add a new metric filter, I encountered the following: 1. A filter tag is added (I added an s3 object tag) and the filter is created, however when editing the newly added filter, the tag isn't there. Also, the relevant cloudwatch metric displays wrong values (to be exact, it shows all values, the filter doesn't work at all). I want to clarify that tag names with dashes cannot be added. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics-filter.html 2. When editing bucket metric options, 'Request metrics' and 'Data transfer metrics' are both selected or not and the drop downs cannot be pinpointed. Seems like a javascript issue. The steps followed described here: http://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html -> Step 7 3. slashes (/) cannot be used at prefix, for example adding the prefix '/Bucket01/BigData/SparkCluster' fails with message ''Prefix search does not support the character '/'.'. Does that mean that only root prefixes can be used or the javascript validator doesn't work as expected? Prefix without slash can be added, but that limits the whole filter concept significantly. Also, the s3 faq states that slashes can be included ( https://aws.amazon.com/s3/faqs/ -> storage management -> S3 CloudWatch Metrics -> Can I align storage metrics to my applications or business organizations?) Tried with latest firefox (50.0.2) and chrome (55.0.2883.75 (64-bit)) at macos sierra (10.12.1 (16B2555)) Regards, Thanos Edited by: thanos on Dec 2, 2016 11:20 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244159&tstart=25"}{"Answers": {"usr-1": ["Good news: I changed my S3 static website redirection rules this morning and it took effect immediately! Bad news: I made a mistake, corrected it right away, but I can't get the correction to take effect! Its been an hour already and no change. Is there any way to force the new rules into effect?"], "usr-2": ["Tech support advised me to invalidate my CloudFront cache, and I did, and it worked!"]}, "Question": "Good news: I changed my S3 static website redirection rules this morning and it took effect immediately! Bad news: I made a mistake, corrected it right away, but I can't get the correction to take effect! Its been an hour already and no change. Is there any way to force the new rules into effect?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246051&tstart=25"}{"Answers": {"usr-1": ["I can't view the S3 bucket as root user or IAM user with AdministratorAccess. I'm getting the following error from AWS management console or using AWS CLI. From Management Cosole: Sorry! You do not have permissions to view this bucket Using AWS CLI: An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I can't access the bucket even using root account or any IAM account with AWS Managed Policy (AdministratorAccess) Please help resolving this. Edited by: rbandi on Dec 12, 2016 5:54 PM"], "usr-2": ["The following documentation link shows - as the root user, you can still access the bucket by modifying the bucket policy to allow root user access. But I can't access the bucket policy. When I click on properties, I get an error saying \" Sorry! You do not have permissions to view this bucket\". http://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_iam-s3.html Can someone from Amazon please help?"], "usr-3": ["Answering my own question. I was able to resolve this issue with new s3 console. With new s3 console, I can modify the bucket policy with root account. I couldn't do the same with the old s3 console."]}, "Question": "I can't view the S3 bucket as root user or IAM user with AdministratorAccess. I'm getting the following error from AWS management console or using AWS CLI. From Management Cosole : Sorry! You do not have permissions to view this bucket Using AWS CLI : An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied I can't access the bucket even using root account or any IAM account with AWS Managed Policy (AdministratorAccess) Please help resolving this. Edited by: rbandi on Dec 12, 2016 5:54 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=244872&tstart=25"}{"Answers": {"usr-1": ["To whom it may concern, I have an S3 bucket that I am trying to delete, but I cannot because it doesn't have an associated region. When logged into the console, when I click on the bucket I see: Error retrieving bucket region Bucket: bmp-order-api-dev-serverlessdeploymentbucket-1m75vmy8aoo3p Please see the attached screenshot. Can you please delete this bucket? It's causing a number of AWS toolkits to fail horribly ;-/ Thank you! ~ Matthew"], "usr-2": ["As of today the Bucket has been destroyed without me doing anything. Hopefully, that was intentional"]}, "Question": "To whom it may concern, I have an S3 bucket that I am trying to delete, but I cannot because it doesn't have an associated region. When logged into the console, when I click on the bucket I see: Error retrieving bucket region Bucket: bmp-order-api-dev-serverlessdeploymentbucket-1m75vmy8aoo3p Please see the attached screenshot. Can you please delete this bucket? It's causing a number of AWS toolkits to fail horribly ;-/ Thank you! ~ Matthew", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246201&tstart=25"}{"Answers": {"usr-1": ["I am trying to enable logging on an S3 bucket and the \"Save\" button is missing. I have tried on multiple browsers (firefox / safari) and multiple AWS accounts. The save button is not there on any of the s3 buckets that I have tried. Repro steps: 1. login to aws console via firefox/safari 2. navigate to S3 3. select an S3 bucket - this can be a bucket with logging enabled or not, same results 4. select properties 5. select logging The enable checkbox is there, the bucket text box is there, and the prefix textbox is also there. Anybody else see this issue or know how to resolve?"], "usr-2": ["Thank you for bringing this to our attention. We've identified the issue and are working on a fix. --Carl"], "usr-3": ["The issue should be resolved. Thanks again for reporting the issue, and apologies for any inconvenience. --Carl"]}, "Question": "I am trying to enable logging on an S3 bucket and the 'Save' button is missing. I have tried on multiple browsers (firefox / safari) and multiple AWS accounts. The save button is not there on any of the s3 buckets that I have tried. Repro steps: 1. login to aws console via firefox/safari 2. navigate to S3 3. select an S3 bucket - this can be a bucket with logging enabled or not, same results 4. select properties 5. select logging The enable checkbox is there, the bucket text box is there, and the prefix textbox is also there. Anybody else see this issue or know how to resolve?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=243962&tstart=25"}{"Answers": {"usr-1": ["I have just now created an Amazon S3 Account and created a unique BUCKET in storage management. Then using a Synology NAS I attempted to set up a New Hyper Backup Task > Data Backup > Amazon S3 Service, and get an \"authentication error\" when entering my Access & Secret Keys. In reading Amazon's support docs I am understanding that my Access Key is simply my Amazon User Account (Email) in the form of User@Domain.com and the Secret Key is simply my password. Am I missing something here? Is there a time delay in when the account is created until it is replicated and usable on the S3 server? Are there any specific PORTS needing to be opened in my firewall for allowing the authentication to complete successfully? I am currently using: Synology DS412+ Current DSM version: DSM 6.0.2-8451 DSM date: 2016/08/17 With Hyper Backup Version: 2.0.1-0384 Any help would be GREATLY appreciated! Thanks! Michael"]}, "Question": "I have just now created an Amazon S3 Account and created a unique BUCKET in storage management. Then using a Synology NAS I attempted to set up a New Hyper Backup Task > Data Backup > Amazon S3 Service, and get an 'authentication error' when entering my Access and Secret Keys. In reading Amazon's support docs I am understanding that my Access Key is simply my Amazon User Account (Email) in the form of User@Domain.com and the Secret Key is simply my password. Am I missing something here? Is there a time delay in when the account is created until it is replicated and usable on the S3 server? Are there any specific PORTS needing to be opened in my firewall for allowing the authentication to complete successfully? I am currently using: Synology DS412+ Current DSM version: DSM 6.0.2-8451 DSM date: 2016/08/17 With Hyper Backup Version: 2.0.1-0384 Any help would be GREATLY appreciated! Thanks! Michael", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246396&tstart=25"}{"Answers": {"usr-1": ["Has anyone else had any issues with the new S3 Inventory function in the last couple days? I have several daily inventory jobs setup and the last ones fired on Jan 8. I haven't touched the inventory jobs, bucket permissions, or anything related in about a month."], "usr-2": ["Thank you for your comment. The issue with the S3 Inventory list was fixed. If you have further questions, we would like to hear more details on your issue. You can provide us more information by opening a case https://aws.amazon.com/contact-us/ and titling it, \"Object Catalog, Jan 9\" so we may reference it to this post."]}, "Question": "Has anyone else had any issues with the new S3 Inventory function in the last couple days? I have several daily inventory jobs setup and the last ones fired on Jan 8. I haven't touched the inventory jobs, bucket permissions, or anything related in about a month.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=246864&tstart=25"}{"Answers": {"usr-1": ["Hi, We have a publicly accessible bucket with the following policy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AllowPublicRead\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::xxxxxxxx/*\" } ] } Until now, the images in the bucket was accessible via the browser. But today we get the following error \"Authorization header is invalid -- one and only one ' ' (space) required\" The weird thing is if I open the same link in Chrome Incognito Mode, image loads perfectly fine. Any idea why? Thank you"], "usr-2": ["Never mind, one of my chrome plugin was additing Authorisation header to my requests"]}, "Question": "Hi, We have a publicly accessible bucket with the following policy { 'Version': '2008-10-17', 'Statement': [ { 'Sid': 'AllowPublicRead', 'Effect': 'Allow', 'Principal': { 'AWS': '-' }, 'Action': 's3:GetObject', 'Resource': 'arn:aws:s3:::xxxxxxxx/-' } ] } Until now, the images in the bucket was accessible via the browser. But today we get the following error 'Authorization header is invalid -- one and only one ' ' (space) required' The weird thing is if I open the same link in Chrome Incognito Mode, image loads perfectly fine. Any idea why? Thank you", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247444&tstart=25"}{"Answers": {"usr-1": ["I am having problems uploading a 4.7 Gb file to my S3 bucket using the AWS CLI (\"aws s3 cp myfile s3://mybucket --region us-east-1\") from a Windows 10 workstation. It is persistently failing with either of these two errors: \"ConnectionError: ('Connection aborted.', error(10053, 'An established connection was aborted by the software in your host machine')) \"ConnectionError: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host')) The amount of time it takes before failing varies wildly from run to run. I have enabled debugging (\"--debug\") and I see a scary amount of the above throughout the duration of the command, as well as many other negative-sounding messages, such as: botocore.awsrequest - DEBUG - Waiting for 100 Continue response. botocore.awsrequest - DEBUG - No response seen from server, continuing to send the response body. botocore.endpoint - DEBUG - ConnectionError received when sending HTTP request. Traceback (most recent call last): File \"botocore\\endpoint.pyc\", line 204, in _get_response File \"botocore\\vendored\\requests\\sessions.pyc\", line 573, in send File \"botocore\\vendored\\requests\\adapters.pyc\", line 415, in send botocore.vendored.requests.packages.urllib3.connectionpool - INFO - Resetting dropped connection: mybucket.s3.amazonaws.com botocore.hooks - DEBUG - Event needs-retry.s3.UploadPart: calling handler <botocore.retryHandler object at 0x0330B190> botocore.retryHandler - DEBUG - retry needed, retryable exception caught: ('Connection aborted.', error(10053, 'An established connection was aborted by the software in your host machine')) Traceback (most recent call last): File \"botocore.retryHandler.pyc\", line 269, in _should_retry File \"botocore.retryHandler.pyc\", line 317, in __call__ File \"botocore.retryHandler.pyc\", line 223, in __call__ File \"botocore.retryHandler.pyc\", line 359, in _check_caught_exception Last month I was (eventually) able to upload a 3.9 Gb file after about half a dozen tries, but this time it is just not working. I have tried decreasing the config parameter \"max_concurrent_requests\" to \"1\", but it didn't help (I didn't think it would, but it was worth a try). I have tried decreasing and increasing the config parameter \"multipart_chunksize\" to \"5MB\" and \"16MB\" respectively, but it didn't help either. According to Windows' Task Manager my system is using 64% of its memory (19.4 MB for the AWS CLI), and virtually 0% of its Network (0.7 Mbps for the AWS CLI). I work from my home office, so I am using a residential high-speed internet service, but I don't have trouble moving files anywhere else. They may take some time, but they do get there. So I am more inclined to suspect either the backend (S3) or the application (AWS CLI). I've been working with AWS for only a month, so I'm willing to start from the possibility that I'm doing something wrong. Am I doing something wrong? Thanks in advance."], "usr-2": ["The silence is deafening. Please folks, somebody must have some suggestions... Thanks."], "usr-3": ["I have now broken it up into parts myself using \"hjsplit\" (used the default 1,400 Kb part size), initiated a multi-part upload myself using the \"s3api create-multipart-upload\" CLI command, and uploaded each of the 3,488 parts individually using the \"s3api upload-part\" CLI command. As far as I can tell, all of the parts were uploaded successfully. Now when I try running the \"s3api complete-multipart-upload\" CLI command, it simply returns: u'Error' Not helpful or informative at all. When I add \"--debug\" to the \"s3api complete-multipart-upload\" CLI command, I see in the output: <Error><Code>EntityTooSmall</Code><Message>Your proposed upload is smaller than the minimum allowed size</Message><ProposedSize>1433600</ProposedSize><MinSizeAllowed>5242880</MinSizeAllowed><PartNumber>1</PartNumber><ETag>637c6c3d8116f29e59e7f26aebd77fa9</ETag><RequestId>6A0136BD25C350CE</RequestId><HostId>jlaYrmtCAom6FcxUcQ/26MC174X3NHWe2nwKCQDVjQXhvFp4HqjbvO59IuwJTH3rewUHOvyE4Aw=</HostId></Error> My interpretation of that message is that I now have to redo the upload of the individual parts with part size of 6 Mb each. But before I do waste even more of my time (and money) doing that, I would just like to confirm that my interpretation of that error is correct. Can somebody please confirm that for me? Thanks. And in case anyone in AWS Development is reading this (which I highly doubt), it would be really nice of you to tell me that the file part size is too small when I am doing the upload of the part... not after I've spent a whole day (and burning through my data limit) doing uploads that won't work. This continues to be a truly horrible experience."], "usr-4": ["Got it to work by re-splitting my file into chunks larger than the minimum. Thanks for all the help."]}, "Question": "I am having problems uploading a 4.7 Gb file to my S3 bucket using the AWS CLI ('aws s3 cp myfile s3://mybucket --region us-east-1') from a Windows 10 workstation. It is persistently failing with either of these two errors: 'ConnectionError: ('Connection aborted.', error(10053, 'An established connection was aborted by the software in your host machine')) 'ConnectionError: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host')) The amount of time it takes before failing varies wildly from run to run. I have enabled debugging ('--debug') and I see a scary amount of the above throughout the duration of the command, as well as many other negative-sounding messages, such as: botocore.awsrequest - DEBUG - Waiting for 100 Continue response. botocore.awsrequest - DEBUG - No response seen from server, continuing to send the response body. botocore.endpoint - DEBUG - ConnectionError received when sending HTTP request. Traceback (most recent call last): File 'botocore\\endpoint.pyc' , line 204, in _get_response File 'botocore\\vendored\\requests\\sessions.pyc' , line 573, in send File 'botocore\\vendored\\requests\\adapters.pyc' , line 415, in send botocore.vendored.requests.packages.urllib3.connectionpool - INFO - Resetting dropped connection: mybucket.s3.amazonaws.com botocore.hooks - DEBUG - Event needs-retry.s3.UploadPart: calling handler <botocore.retryHandler object at 0x0330B190> botocore.retryHandler - DEBUG - retry needed, retryable exception caught: ( 'Connection aborted.' , error(10053, 'An established connection was aborted by the software in your host machine' )) Traceback (most recent call last): File 'botocore.retryHandler.pyc' , line 269, in _should_retry File 'botocore.retryHandler.pyc' , line 317, in __call__ File 'botocore.retryHandler.pyc' , line 223, in __call__ File 'botocore.retryHandler.pyc' , line 359, in _check_caught_exception Last month I was (eventually) able to upload a 3.9 Gb file after about half a dozen tries, but this time it is just not working. I have tried decreasing the config parameter 'max_concurrent_requests' to '1', but it didn't help (I didn't think it would, but it was worth a try). I have tried decreasing and increasing the config parameter 'multipart_chunksize' to '5MB' and '16MB' respectively, but it didn't help either. According to Windows' Task Manager my system is using 64\\% of its memory (19.4 MB for the AWS CLI), and virtually 0\\% of its Network (0.7 Mbps for the AWS CLI). I work from my home office, so I am using a residential high-speed internet service, but I don't have trouble moving files anywhere else. They may take some time, but they do get there. So I am more inclined to suspect either the backend (S3) or the application (AWS CLI). I've been working with AWS for only a month, so I'm willing to start from the possibility that I'm doing something wrong. Am I doing something wrong? Thanks in advance.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247636&tstart=25"}{"Answers": {"usr-1": ["Hello, There is a bucket that was running ok, that now has no region assigned. I need to delete it, but it is impossible. It was created in EU Germany region. Thanks in advance for your answer."], "usr-2": ["It has disappeared. Thanks."], "usr-3": ["Thank you for your comment. We are interested in finding more details about your case. When you open your case please add as much information as possible which relates to this issue. We would like to collect this information we can improve your experience. Please include description, bucket, calls and related and details how this occurred and we will be happy to investigate further. You can provide us more information by opening a case https://aws.amazon.com/contact-us/ and titling it, \"bucket region missing 1/27/2017\" so we may reference it to this post."]}, "Question": "Hello, There is a bucket that was running ok, that now has no region assigned. I need to delete it, but it is impossible. It was created in EU Germany region. Thanks in advance for your answer.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=247898&tstart=25"}{"Answers": {"usr-1": ["Hi, I'm testing an App together with MySQL on a free t2.Micro instance that comes with EBS. However, it ran out of space for the root directory, where it shows 100% as per below, and my App and MySQL went down. Filesystem Size Used Avail Use% Mounted on udev 492M 8.0K 492M 1% /dev tmpfs 100M 348K 99M 1% /run /dev/xvda1 7.8G 7.7G 0 100% / none 4.0K 0 4.0K 0% /sys/fs/cgroup none 5.0M 0 5.0M 0% /run/lock none 497M 0 497M 0% /run/shm none 100M 0 100M 0% /run/user Are there known issues on MySQL sitting on an instance supported by EBS or if we are running MySQL DB, it should sit in an instance with fixed storage space. Some urgent advice is much appreciated. Thanks."], "usr-2": ["Hello There are no issues running MySQL on an EBS backed EC2 instance. Please ensure that the EBS volume is large enough. For a managed MySQL instance, please try Amazon RDS for MySQL (https://aws.amazon.com/rds/mysql/). Regards Conrad"], "usr-3": ["Replies no longer required. Forgotten this is a free test instance, so EBS is capped at 8GB, unlike my other paid instances."], "usr-4": ["Hi, thanks for your confirmation on no known issues of MySQL running on EBS. Just realised the free instance comes with a cap at 8GB, unlike my paid instances. Thanks."]}, "Question": "Hi, I'm testing an App together with MySQL on a free t2.Micro instance that comes with EBS. However, it ran out of space for the root directory, where it shows 100\\% as per below, and my App and MySQL went down. Filesystem Size Used Avail Use\\% Mounted on udev 492M 8.0K 492M 1\\% /dev tmpfs 100M 348K 99M 1\\% /run /dev/xvda1 7.8G 7.7G 0 100\\% / none 4.0K 0 4.0K 0\\% /sys/fs/cgroup none 5.0M 0 5.0M 0\\% /run/lock none 497M 0 497M 0\\% /run/shm none 100M 0 100M 0\\% /run/user Are there known issues on MySQL sitting on an instance supported by EBS or if we are running MySQL DB, it should sit in an instance with fixed storage space. Some urgent advice is much appreciated. Thanks.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=236826&tstart=50"}{"Answers": {"usr-1": ["In S3, I have an index.html, a CSS file, and an images folder with 3 jpg in it. I cut and pasted the jpgs into the S3 folder. They show there. I set a display policy per the instructions to display to everyone. The page displays, but the images do not. I get the alt text and the tooltips, but no images. On the page, when I right click the Open Image in New Tab, I get an Access Denied message in XML. The page is - http://s3.amazonaws.com/pittsfordcommunitycenter/index.html Images display in my development PC. I've tried a lot of things in AWS, including what the documentation says. What do I need to change?"], "usr-2": ["Hello MikeL, I am unable to reproduce this issue. Except for the favicon.ico I am able to load all objects. If you are still seeing 403 Access Denied for images I would suggest clearing browser cache and trying again. Regards, Dilip S"], "usr-3": ["The problem is now solved. I thought I made an error in AWS somewhere, but the error was in the image tags in my HTML. I changed ../image/ to ./image/ and everything works on my dev PC and in AWS. Thanks for looking at the situation."]}, "Question": "In S3, I have an index.html, a CSS file, and an images folder with 3 jpg in it. I cut and pasted the jpgs into the S3 folder. They show there. I set a display policy per the instructions to display to everyone. The page displays, but the images do not. I get the alt text and the tooltips, but no images. On the page, when I right click the Open Image in New Tab, I get an Access Denied message in XML. The page is - http://s3.amazonaws.com/pittsfordcommunitycenter/index.html Images display in my development PC. I've tried a lot of things in AWS, including what the documentation says. What do I need to change?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237507&tstart=50"}{"Answers": {"usr-1": ["We have about 200 failures and rising with the error below (a few examples). I have a bunch of examples if you need them. The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: B7741E80FCB73606) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 53513F36E00991E1) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 310EBAB6F587FFD0) We have also seen issues with files being deleted/moved/missing as we can't access them anymore and get the same error above."]}, "Question": "We have about 200 failures and rising with the error below (a few examples). I have a bunch of examples if you need them. The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: B7741E80FCB73606) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 53513F36E00991E1) The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 310EBAB6F587FFD0) We have also seen issues with files being deleted/moved/missing as we can't access them anymore and get the same error above.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237678&tstart=50"}{"Answers": {"usr-1": ["Hi! I've uploaded a file with custom metadata (x-amz-meta-blah), and the metadata are not visible in the S3 console/bucket browser in the metadata dropdown. If I GET the item I see the metadata fine. Is this a regression/bug? The documentation seems to imply the S3 console should show all the custom metadata. Thanks!"], "usr-2": ["Looks like the metadata is visible today. Bug fixed? Who knows."], "usr-3": ["Looks like the problem was fixed sometime in the night."]}, "Question": "Hi! I've uploaded a file with custom metadata (x-amz-meta-blah), and the metadata are not visible in the S3 console/bucket browser in the metadata dropdown. If I GET the item I see the metadata fine. Is this a regression/bug? The documentation seems to imply the S3 console should show all the custom metadata. Thanks!", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237745&tstart=50"}{"Answers": {"usr-1": ["I just set up two buckets, with bucket1 logging into bucket2/logs (configured in S3 Management Console). Then I download a file from bucket1 successfully using a presigned url - and with an error when I manually modify the url. Strangely enough bot GETs are not in the logs. What's in the logs are the PUT requests where aws-internal writes a log entry. What could be the reason for this behavior?"], "usr-2": ["It just took some time until the download logs arrived. Funny that logs about writing logs appeared significantly earlier."]}, "Question": "I just set up two buckets, with bucket1 logging into bucket2/logs (configured in S3 Management Console). Then I download a file from bucket1 successfully using a presigned url - and with an error when I manually modify the url. Strangely enough bot GETs are not in the logs. What's in the logs are the PUT requests where aws-internal writes a log entry. What could be the reason for this behavior?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237740&tstart=50"}{"Answers": {"usr-1": ["Trying to write a basic 101 script to just copy nightly log files for specific domains to specific buckets. When it works, simply touch a file, and delete them (the touch is so a watchdog can tell that it completes). But my basic test is finishing saying DONE but it's not copying. #/bin/bash year=`date +%Y`; \u00a0 if /usr/local/bin/aws s3 cp /mnt/freenas1/backup_temp/ s3://admin.logs.misc/domain1/$year/ --exclude \"*\" --include \"*domain1*\" then echo done fi I run that and it comes back with DONE, but I don't see the copy output, and the files never get to reach S3. There are multiple files like 2016.08.19.domain1.tar.gz, but I have others like domain2, etc. so I want pretty much * domain1 * to go (sorry but the * next to the words makes them bold, but Im sure it's understood) Thanks all"]}, "Question": "Trying to write a basic 101 script to just copy nightly log files for specific domains to specific buckets. When it works, simply touch a file, and delete them (the touch is so a watchdog can tell that it completes). But my basic test is finishing saying DONE but it's not copying. #/bin/bash year=`date +\\%Y`; \u00a0 if /usr/local/bin/aws s3 cp /mnt/freenas1/backup_temp/ s3: //admin.logs.misc/domain1/$year/ --exclude '-' --include '-domain1-' then echo done fi I run that and it comes back with DONE, but I don't see the copy output, and the files never get to reach S3. There are multiple files like 2016.08.19.domain1.tar.gz, but I have others like domain2, etc. so I want pretty much - domain1 - to go (sorry but the - next to the words makes them bold, but Im sure it's understood) Thanks all", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237892&tstart=50"}{"Answers": {"usr-1": ["We are performing GET bucket location call with V4 signature generated using \"us-east-1\" region in the following URL format: https://s3.amazonaws.com/my-bucket-name?location Sometimes, it returns the error in following format: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Error> <Code>AuthorizationHeaderMalformed</Code> <Message>The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'us-west-1'</Message> <Region>us-west-1</Region> <RequestId>A1A1</RequestId> <HostId>B1B1</HostId> </Error> 1. Is the region 'us-west-1' mentioned in \"Region\" tag the correct location of the bucket? 2. Or is it asking us to make GET bucket location call again with new V4 signature generated using this region (us-west-1 in current example)? 3. Can we make multiple GET bucket regions calls simultaneously for various buckets in various regions? Edited by: Hitesh on Aug 24, 2016 11:39 PM: Added the question of multiple calls simultaneously Edited by: Hitesh on Sep 8, 2016 2:47 AM: It was a multithreading issue in the code. No issue from S3."], "usr-2": ["It was a bug in our code for multi-threading. Nothing related to s3."]}, "Question": "We are performing GET bucket location call with V4 signature generated using 'us-east-1' region in the following URL format: https://s3.amazonaws.com/my-bucket-name?location Sometimes, it returns the error in following format: <?xml version= '1.0' encoding= 'UTF-8' ?> <Error> <Code>AuthorizationHeaderMalformed</Code> <Message>The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'us-west-1' </Message> <Region>us-west-1</Region> <RequestId>A1A1</RequestId> <HostId>B1B1</HostId> </Error> 1. Is the region 'us-west-1' mentioned in 'Region' tag the correct location of the bucket? 2. Or is it asking us to make GET bucket location call again with new V4 signature generated using this region (us-west-1 in current example)? 3. Can we make multiple GET bucket regions calls simultaneously for various buckets in various regions? Edited by: Hitesh on Aug 24, 2016 11:39 PM: Added the question of multiple calls simultaneously Edited by: Hitesh on Sep 8, 2016 2:47 AM: It was a multithreading issue in the code. No issue from S3.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237879&tstart=50"}{"Answers": {"usr-1": ["Hi, I am always getting \"523 unknown I/O\" error on specific directories and this has been happening from 5 Aug and before 5 Aug things were fine. Edited by: azfar123 on Aug 24, 2016 5:10 AM"], "usr-2": ["fixed,related with idmap service."]}, "Question": "Hi, I am always getting '523 unknown I/O' error on specific directories and this has been happening from 5 Aug and before 5 Aug things were fine. Edited by: azfar123 on Aug 24, 2016 5:10 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237961&tstart=50"}{"Answers": {"usr-1": ["We have a fiber internet connexion that allows us to upload and download up to 100 Mb/s. Usually, when we download files from our S3 account, a 800 Mb file would take a few seconds to download. But in the past days, download speeds are really low. I created a new bucket and upload a blank file to test : the upload speed was 21543 KB/s, but the download speed is 1,7 MB/s. I did a download speed on speedtest.net and the the results are 931,29 Mbps for download and 928,63 Mbps for upload. Does any one know why the download speeds have decreased ? You can try my test file : https://s3.amazonaws.com/test2016-08-29/TestDoanloadSpeed.dmg Screen Capture 1 Screen Capture 2 Screen Capture 3"], "usr-2": ["I don't know what you did Mark, but download speeds are back to normal. Thanks !"]}, "Question": "We have a fiber internet connexion that allows us to upload and download up to 100 Mb/s. Usually, when we download files from our S3 account, a 800 Mb file would take a few seconds to download. But in the past days, download speeds are really low. I created a new bucket and upload a blank file to test : the upload speed was 21543 KB/s, but the download speed is 1,7 MB/s. I did a download speed on speedtest.net and the the results are 931,29 Mbps for download and 928,63 Mbps for upload. Does any one know why the download speeds have decreased ? You can try my test file : https://s3.amazonaws.com/test2016-08-29/TestDoanloadSpeed.dmg Screen Capture 1 Screen Capture 2 Screen Capture 3", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238257&tstart=50"}{"Answers": {"usr-1": ["Hi All, I have a weird problem, we store SQL Server database backups in S3 and every once in a while we download them to perform restores but when I try this i get the following errors in SQL Server 2008 R2: \u201cThe media family on device X is incorrectly formed\u201d Which according to my brief online research normally points to corruption in the backup. Now this is where it gets really strange. When i download the backups via a browser like Firefox the backups restore perfectly fine but when I use a client like S3 Browser (www.s3browser.com) or Cloudberry Explorer the restore fails with the above error message. When i compare the file size i noticed it differs very slightly when I download it via a browser compared to the above mentioned S3 clients. So e.g. for Firefox the file size is 6,555,466KB and when i use S3 client the size is 6,379,796KB. Why this discrepancy? What is going on here??? Why does using these clients seem to corrupt the SQL backups when i download them? Please help! Edited by: Rabi Achrafi on Aug 19, 2016 7:26 AM"], "usr-2": ["Has anyone else experienced this or at least provide suggestions on what could be causing this? Any help would be greatly appreciated."], "usr-3": ["Hi Rabi, Let's start with what software do you use to back up SQL databases?"], "usr-4": ["SQL Server 2008 R2, we don't use any special third party backup software and no encryption/compression is used."], "usr-5": ["I think I now know what is causing this, so basically i realised the backup software which uploads the SQL backups to S3 actually compresses it at the same time thus altering the file state ever so slightly. I have now disabled this feature so restoring backups should now work. Thanks All."]}, "Question": "Hi All, I have a weird problem, we store SQL Server database backups in S3 and every once in a while we download them to perform restores but when I try this i get the following errors in SQL Server 2008 R2: 'The media family on device X is incorrectly formed' Which according to my brief online research normally points to corruption in the backup. Now this is where it gets really strange. When i download the backups via a browser like Firefox the backups restore perfectly fine but when I use a client like S3 Browser (www.s3browser.com) or Cloudberry Explorer the restore fails with the above error message. When i compare the file size i noticed it differs very slightly when I download it via a browser compared to the above mentioned S3 clients. So e.g. for Firefox the file size is 6,555,466KB and when i use S3 client the size is 6,379,796KB. Why this discrepancy? What is going on here??? Why does using these clients seem to corrupt the SQL backups when i download them? Please help! Edited by: Rabi Achrafi on Aug 19, 2016 7:26 AM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=237633&tstart=50"}{"Answers": {"usr-1": ["Our S3 bucket with our website is suddenly empty. We are certain that we didn't delete it. The CloudWatch metrics for the bucket indicate that there are 90 objects in the bucket totalling about 16M, basically little or no change for weeks. But we can't see any objects in the bucket in the Management Console or using the AWS CLI., and the pages are not being served via the static web hosting. What could have happened, and what do we do? Edited by: brianm on Aug 30, 2016 11:11 AM I figured out what happened. The problem was on our side."]}, "Question": "Our S3 bucket with our website is suddenly empty. We are certain that we didn't delete it. The CloudWatch metrics for the bucket indicate that there are 90 objects in the bucket totalling about 16M, basically little or no change for weeks. But we can't see any objects in the bucket in the Management Console or using the AWS CLI., and the pages are not being served via the static web hosting. What could have happened, and what do we do? Edited by: brianm on Aug 30, 2016 11:11 AM I figured out what happened. The problem was on our side.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238310&tstart=50"}{"Answers": {"usr-1": ["It's my first attempt with AWSS3. I'm looking at the aws-sdk-ios-samples on GitHub and I'm trying to port the S3TransferUtility-Sample / SecondViewController.start(sender : UIButton) function into Swift3. So far I have this: public func downloadCollection(_ name: String) { Logger.profile(self, function: #function) \u00a0 let expression = AWSS3TransferUtilityDownloadExpression() expression.progressBlock = {(task, progress) in DispatchQueue.main.async { if let download = self.downloadList[task.key] { download.progress=Float(progress.fractionCompleted) NotificationCenter.default.post(name: Notification.Name(kNoteProgress), object: nil, userInfo: [\"path\" : task.key, \"progress\" : download.progress]) } } } self.awss3CompletionHandler = { (task, location, data, error) -> Void in DispatchQueue.main.async { let download = self.downloadList[task.key] if ((error) != nil) { Logger.add(\"ERROR downloading AWS file \\(error)\") NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [\"path\": task.key, \"error\" : error]) } else if (download!.progress != 1.0) { Logger.add(\"ERROR downloading AWS file - did not receive whole file, percent recieved \\(download!.progress * 100) percent\") NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [\"path\": task.key,\"error\" : error]) } self.downloadList[task.key] = nil NotificationCenter.default.post(name: Notification.Name(kNoteDownloadFinished), object: nil, userInfo: [\"path\" : task.key]) } } var xferUtility : AWSS3TransferUtility = AWSS3TransferUtility() xferUtility.downloadData(fromBucket: URLDownloader.AWS_BUCKET_NAME, key: \"\\(name).iOS.zip\", expression: expression, completionHander: awss3CompletionHandler).continue { (task) -> Any? in if let error = task.error { Logger.add(\"ERROR downloading AWS file \\(error.localizedDescription)\") } if let exception = task.exception { Logger.add(\"ERROR downloading AWS file \\(exception.description)\") } return nil } } It's the xferUtility.downloadData that is causing the error. It says Ambiguous use of 'continue' when it tries to evaluate the downloadData call or, rather, the continue attached to its result. The downloadData is this open func downloadData(fromBucket bucket: String, key: String, expression: AWSS3TransferUtilityDownloadExpression?, completionHander completionHandler: AWSS3.AWSS3TransferUtilityDownloadCompletionHandlerBlock? = nil) -> AWSTask<AWSS3TransferUtilityDownloadTask> which returns a AWSTask<AWSS3TransferUtilityDownloadTask>. The AWSTask is where we find the continue... open func `continue`(_ block: AWSCore.AWSContinuationBlock) -> AWSTask<AnyObject> There are several `continue` functions but this appears to be the correct one based on the body of the closure in the sample app. Anyone know how to translate this so Swift3 will like it? I'm using Xcode 8.0 beta 6 (8S201h). TIA Edited by: Dozer on Aug 31, 2016 2:17 PM (better formatting)"], "usr-2": ["I think you now need to add an open parenthesis after continue and before the {: completionHander: awss3CompletionHandler).continue ( { (task) -> Any? in and then close it after the } in your next-to-last line. Edited by: jasonmusser on Sep 15, 2016 1:15 PM"], "usr-3": ["@jasonmusser Thanks, that got me past the compiler errors."]}, "Question": "It's my first attempt with AWSS3. I'm looking at the aws-sdk-ios-samples on GitHub and I'm trying to port the S3TransferUtility-Sample / SecondViewController.start(sender : UIButton) function into Swift3. So far I have this: public func downloadCollection(_ name: String) { Logger.profile(self, function: #function) \u00a0 let expression = AWSS3TransferUtilityDownloadExpression() expression.progressBlock = { (task, progress) in DispatchQueue.main.async { if let download = self.downloadList[task.key] { download.progress=Float(progress.fractionCompleted) NotificationCenter.default.post(name: Notification.Name(kNoteProgress), object: nil, userInfo: [ 'path' : task.key, 'progress' : download.progress]) } } } self.awss3CompletionHandler = { (task, location, data, error) -> Void in DispatchQueue.main.async { let download = self.downloadList[task.key] if ((error) != nil) { Logger.add( 'ERROR downloading AWS file \\(error)' ) NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [ 'path' : task.key, 'error' : error]) } else if (download!.progress != 1.0) { Logger.add( 'ERROR downloading AWS file - did not receive whole file, percent recieved \\(download!.progress - 100) percent' ) NotificationCenter.default.post(name: Notification.Name(kNoteDownloadError), object: nil, userInfo: [ 'path' : task.key, 'error' : error]) } self.downloadList[task.key] = nil NotificationCenter.default.post(name: Notification.Name(kNoteDownloadFinished), object: nil, userInfo: [ 'path' : task.key]) } } var xferUtility : AWSS3TransferUtility = AWSS3TransferUtility() xferUtility.downloadData(fromBucket: URLDownloader.AWS_BUCKET_NAME, key: '\\(name).iOS.zip' , expression: expression, completionHander: awss3CompletionHandler). continue { (task) -> Any? in if let error = task.error { Logger.add( 'ERROR downloading AWS file \\(error.localizedDescription)' ) } if let exception = task.exception { Logger.add( 'ERROR downloading AWS file \\(exception.description)' ) } return nil } } It's the xferUtility.downloadData that is causing the error. It says Ambiguous use of 'continue' when it tries to evaluate the downloadData call or, rather, the continue attached to its result. The downloadData is this open func downloadData(fromBucket bucket: String, key: String, expression: AWSS3TransferUtilityDownloadExpression?, completionHander completionHandler: AWSS3.AWSS3TransferUtilityDownloadCompletionHandlerBlock? = nil) -> AWSTask<AWSS3TransferUtilityDownloadTask> which returns a AWSTask<AWSS3TransferUtilityDownloadTask>. The AWSTask is where we find the continue... open func `continue`(_ block: AWSCore.AWSContinuationBlock) -> AWSTask<AnyObject> There are several `continue` functions but this appears to be the correct one based on the body of the closure in the sample app. Anyone know how to translate this so Swift3 will like it? I'm using Xcode 8.0 beta 6 (8S201h). TIA Edited by: Dozer on Aug 31, 2016 2:17 PM (better formatting)", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238481&tstart=50"}{"Answers": {"usr-1": ["Three locations need updated to properly show how to work with the current PHP v3 SDK and are currently INCORRECT as they stand, resulting in much confusion as this appears to be undocumented anywhere. They are still showing the old v2 sdk method and can be updated to be accurate with one very minor change. Can someone please get this updated in these locations? URL 1: http://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html URL 2: http://docs.amazonaws.cn/en_us/AmazonS3/latest/dev/LLuploadFilePHP.html These pages indicate completing the multipart upload as follows: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'Parts' => $parts, )); This is only valid for the old v2 SDK. It needs fixed to be the following for the v3 SDK: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'MultipartUpload' => array( 'Parts' => $parts, ), )); URL 3: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/migration.html This migration guide could easily be updated to reflect the simple change of wrapping the \"Parts\" parameter inside a new array \"MultipartUpload\" as shown above. This would be huge help to anyone migrating. Thanks! Edited by: dustout on Sep 6, 2016 8:35 PM Edited by: dustout on Sep 6, 2016 8:35 PM"], "usr-2": ["Hello, Thank you for providing this information. I have passed this on to the necessary teams. Regards, Tim"]}, "Question": "Three locations need updated to properly show how to work with the current PHP v3 SDK and are currently INCORRECT as they stand, resulting in much confusion as this appears to be undocumented anywhere. They are still showing the old v2 sdk method and can be updated to be accurate with one very minor change. Can someone please get this updated in these locations? URL 1: http://docs.aws.amazon.com/AmazonS3/latest/dev/LLuploadFilePHP.html URL 2: http://docs.amazonaws.cn/en_us/AmazonS3/latest/dev/LLuploadFilePHP.html These pages indicate completing the multipart upload as follows: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'Parts' => $parts, )); This is only valid for the old v2 SDK. It needs fixed to be the following for the v3 SDK: $result = $s3->completeMultipartUpload(array( 'Bucket' => $bucket, 'Key' => $key, 'UploadId' => $uploadId, 'MultipartUpload' => array( 'Parts' => $parts, ), )); URL 3: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/migration.html This migration guide could easily be updated to reflect the simple change of wrapping the 'Parts' parameter inside a new array 'MultipartUpload' as shown above. This would be huge help to anyone migrating. Thanks! Edited by: dustout on Sep 6, 2016 8:35 PM Edited by: dustout on Sep 6, 2016 8:35 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238818&tstart=50"}{"Answers": {"usr-1": ["I have a website in which I have linked files for sale that are in my S3 account. However I find that clients can only download them if I make the files public, in other words I grant everyone permission, which I think is risky. Is there a simple way to arrange for clients to download non-public files? The clients are people who come to my website, pay money and leave their email address."], "usr-2": ["Hi Jeffrey, The best is to look at the \u00abAuthenticating REST Requests\u00bb here - http://s3.amazonaws.com/doc/s3-developer-guide/RESTAuthentication.html You will need to work with IAM policies for your user, this product might be helpful in this case - (IAM policy manager for buckets / objects). Edited by: evgeny on Sep 8, 2016 5:06 AM"], "usr-3": ["Thank you Evgeny. The problem is that article is too technical for me. I need a simple solution. I could ask a different question: What is the risk if I make saleable files public? Can a person who has paid for and downloaded one file then download all the others without paying for them?"], "usr-4": ["Yes, you can create Pre-Signed URLs which gives the person the ability to download that file (and only that file) until the URL expires (you can set how long you want the URL to be valid for). To do this in PHP, you will want something like this: https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-url.html Or this: http://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html"]}, "Question": "I have a website in which I have linked files for sale that are in my S3 account. However I find that clients can only download them if I make the files public, in other words I grant everyone permission, which I think is risky. Is there a simple way to arrange for clients to download non-public files? The clients are people who come to my website, pay money and leave their email address.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238904&tstart=50"}{"Answers": {"usr-1": ["I'm currently seeing a huge delay (approximately 20 hours) on objects added to S3 triggering Lambdas attached to the ObjectCreated event. As far as I can tell, no configuration has been changed, and the same Lambdas were being triggered within a few seconds only a couple of days ago. Is there some sort of rate-limiting / throttling that we've inadvertently run afoul of? Or is something quite badly broken on the S3 end? Or should we just not be expecting stuff to always run instantly (I've always been led to believe the events are supposed to be instant so I doubt it's that) Happy to provide any more information necessary if it'll help track things down. Everything in question is being run in us-east-1"], "usr-2": ["Providing my own answer here following a conversation with AWS Support, in case anyone else runs into the same issue. It turns out that since our account has lately been running pretty heavy on Lambda usage, we were running into our concurrent execution limit - this appears to be global across the account and not per-function. So we had a few really hot functions clogging up the limit, and some of our quieter functions were being permanently throttled because they could never allocate a worker. As far as I can see there's no way to tell whether you're hitting or running near the concurrency limit, so I guess the only way to tell is to keep an eye on the CloudWatch logs for invocation count, duration, and throttles. A bit tricky if you have many, many lambdas, but there might be one or two that're particularly hungry for concurrency."]}, "Question": "I'm currently seeing a huge delay (approximately 20 hours) on objects added to S3 triggering Lambdas attached to the ObjectCreated event. As far as I can tell, no configuration has been changed, and the same Lambdas were being triggered within a few seconds only a couple of days ago. Is there some sort of rate-limiting / throttling that we've inadvertently run afoul of? Or is something quite badly broken on the S3 end? Or should we just not be expecting stuff to always run instantly (I've always been led to believe the events are supposed to be instant so I doubt it's that) Happy to provide any more information necessary if it'll help track things down. Everything in question is being run in us-east-1", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239491&tstart=50"}{"Answers": {"usr-1": ["Hi, I download my own media files from my own S3 bucket using my own libCurl C++ application. If I mark a file as public and download it with a simple libCurl GET request (no additional headers), the downloaded file works perfectly fine. However, if I mark the exact same file as private and add headers to the GET request (Host, x-amz-date, x-amz-content-sha256 and the Authorization Header), I cannot open the downloaded file. For mp4 files: ffmpeg tells me that the moov atom of the file cannot be found. For png files: Windows tells me the \"file is not a valid bitmap file, or its format is not supported\" The filesizes of a working and a 'broken' file differ slightly. The broken file is about 350 bytes bigger... My question is: Why does adding headers to the GET request alter the downloaded file? And what can I do to fix this? Thanks in advance!"], "usr-2": ["Using a Hex Editor I was able to find out that the HTTP response is written into the received media file at the beginning. (Again: only if I include additional headers in the HTTP request. Otherwise, the received file is fine.) How can I prevent that?"], "usr-3": ["I found the solution myself. Just had to remove this line from my code: curl_easy_setopt(curl, CURLOPT_HEADER, true); Initially, I thought this line would allow me to add additional headers to my HTTP request... Instead, it includes the HTTP response in the output, which was my problem."]}, "Question": "Hi, I download my own media files from my own S3 bucket using my own libCurl C++ application. If I mark a file as public and download it with a simple libCurl GET request (no additional headers), the downloaded file works perfectly fine. However, if I mark the exact same file as private and add headers to the GET request (Host, x-amz-date, x-amz-content-sha256 and the Authorization Header), I cannot open the downloaded file. For mp4 files: ffmpeg tells me that the moov atom of the file cannot be found. For png files: Windows tells me the 'file is not a valid bitmap file, or its format is not supported' The filesizes of a working and a 'broken' file differ slightly. The broken file is about 350 bytes bigger... My question is: Why does adding headers to the GET request alter the downloaded file? And what can I do to fix this? Thanks in advance!", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238909&tstart=50"}{"Answers": {"usr-1": ["Can a person who has paid for and downloaded one file then browse and download all the others without paying for them when all files are made public inS3?"], "usr-2": ["If they can access the bucket, yes. Why wouldn't you send them a pre-authenticated URL to the file? You can then restrict the time they are able to use the URL and thus download the file, also the URL is for that file only."], "usr-3": ["What is a pre-authenticated URL. Do I have to do this manually or can it be done automatically when they buy from my website?"], "usr-4": ["You can do this via the API. The PHP documentation is here (by virtue that it was the first 'useful' link that came up when I typed \"presigned s3 url\" into Google. https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-presigned-url.html Worth mentioning that this would mean removing \"Public\" permissions from your S3 Objects... Edited by: mal on Sep 10, 2016 2:16 PM"]}, "Question": "Can a person who has paid for and downloaded one file then browse and download all the others without paying for them when all files are made public inS3?", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=238993&tstart=50"}{"Answers": {"usr-1": ["I'm attempting to create an S3 POST with SSE-C keys generated from KMS. Despite my best efforts, the request fails with \"The calculated MD5 hash of the key did not match the hash that was provided.\" I have included the proper headings according to the S3 documentation. I generate the x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 parameters with the following: var kmsParams = {KeyId: 'alias/myKey', KeySpec: 'AES_256'}; kms.generateDataKey(kmsParams, function(err, data) { if(err) { console.log(err) } else { var key = new Buffer(data.Plaintext).toString('Base64'), hash = crypto.createHash('md5').update(key).digest('base64'); }); What am I doing wrong here? I realize that the customer encryption key should be 32 bytes (256 bits) long. However, generateDataKey seems to return a plaintext string that is 44 bytes long, even if I explicitly specify the NumberOfBytes parameter as '32'. Can someone explain this discrepancy for me? Even if the key was not correctly formatted, I imagine I would receive an error message to that effect. I've tried to generate the MD5 hash in a number of ways, but it continues to fail with the aforementioned error message. If I execute a standard PUT request, it works fine: var kmsParams = {KeyId: 'alias/myKey', KeySpec: 'AES_256'}; kms.generateDataKey(kmsParams, function(err, data) { if(err) { console.log(err) } else { var s3Params = { Bucket: s3Bucket, Key: filename, Body: myVar, SSECustomerAlgorithm: 'AES256', SSECustomerKey: data.Plaintext }; s3.putObject(s3Params, function(err, data) { if(err) { console.log(err) } else { console.log(data) }; }); });"], "usr-2": ["Resolved! The hash value should be: hash = crypto.createHash('md5').update(data.Plaintext).digest('Base64'); Only data.Plaintext needs to be hashed as Base64. If you modify it in any manner (e.g. Base64'd, toString), it'll return the error."]}, "Question": "I'm attempting to create an S3 POST with SSE-C keys generated from KMS. Despite my best efforts, the request fails with 'The calculated MD5 hash of the key did not match the hash that was provided.' I have included the proper headings according to the S3 documentation . I generate the x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 parameters with the following: var kmsParams = { KeyId: 'alias/myKey' , KeySpec: 'AES_256' } ; kms.generateDataKey(kmsParams, function(err, data) { if (err) { console.log(err) } else { var key = new Buffer(data.Plaintext).toString( 'Base64' ), hash = crypto.createHash( 'md5' ).update(key).digest( 'base64' ); } ); What am I doing wrong here? I realize that the customer encryption key should be 32 bytes (256 bits) long. However, generateDataKey seems to return a plaintext string that is 44 bytes long, even if I explicitly specify the NumberOfBytes parameter as '32'. Can someone explain this discrepancy for me? Even if the key was not correctly formatted, I imagine I would receive an error message to that effect. I've tried to generate the MD5 hash in a number of ways, but it continues to fail with the aforementioned error message. If I execute a standard PUT request, it works fine: var kmsParams = { KeyId: 'alias/myKey' , KeySpec: 'AES_256' } ; kms.generateDataKey(kmsParams, function(err, data) { if (err) { console.log(err) } else { var s3Params = { Bucket: s3Bucket, Key: filename, Body: myVar, SSECustomerAlgorithm: 'AES256' , SSECustomerKey: data.Plaintext } ; s3.putObject(s3Params, function(err, data) { if (err) { console.log(err) } else { console.log(data) } ; } ); } );", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239563&tstart=50"}{"Answers": {"usr-1": ["What are the S3 and Glacier requirements prior to moving data with Lifecycle policies into Glacier ? I need to upload Archival data via S3 so I keep the directory structure (I do know that I could upload data straight to Glacier but that would not be feasable for me) Data moves from my site to S3 buckets (via snowball and mirroring from onsite NAS to AWS S3 bucket . After I verify all data has been properly transferred, I create a Lifecycle policy so the whole bucket goes immediately (one day after synching) into Glacier for an indefinite time. My Questions: For GLACIER: It would seem logical to me to open a vault in Glacier but it is not mentioned in any documents - so is a vault needed ? For S3: Do I need to specify the type of storage of the bucket data prior to enabling the lifecycle management rule? No box is checked but the default when nothing is checked is Standard S3. Thank you Edited by: daradmin on Sep 19, 2016 12:25 PM Edited by: daradmin on Sep 19, 2016 12:26 PM"], "usr-2": ["daradmin wrote: For GLACIER: It would seem logical to me to open a vault in Glacier but it is not mentioned in any documents - so is a vault needed ? No vault is needed. Files archived to Glacier through the S3 integration are managed internally by S3. You interact with Glacier solely through S3. The files will not be visible or otherwise available through Glacier directly. For S3: Do I need to specify the type of storage of the bucket data prior to enabling the lifecycle management rule? No box is checked but the default when nothing is checked is Standard S3. I'm not entirely sure what option you are referring to, but in general you don't need to do anything special beforehand."]}, "Question": "What are the S3 and Glacier requirements prior to moving data with Lifecycle policies into Glacier ? I need to upload Archival data via S3 so I keep the directory structure (I do know that I could upload data straight to Glacier but that would not be feasable for me) Data moves from my site to S3 buckets (via snowball and mirroring from onsite NAS to AWS S3 bucket . After I verify all data has been properly transferred, I create a Lifecycle policy so the whole bucket goes immediately (one day after synching) into Glacier for an indefinite time. My Questions: For GLACIER: It would seem logical to me to open a vault in Glacier but it is not mentioned in any documents - so is a vault needed ? For S3: Do I need to specify the type of storage of the bucket data prior to enabling the lifecycle management rule? No box is checked but the default when nothing is checked is Standard S3. Thank you Edited by: daradmin on Sep 19, 2016 12:25 PM Edited by: daradmin on Sep 19, 2016 12:26 PM", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239549&tstart=50"}{"Answers": {"usr-1": ["I transferred my domain name from another company to amazon aws route 53 since I was already using s3 buckets / route 53 for my site. I already set the www to redirect when I originally set it up and everything was working fine until the transfer. I noticed the ns was different for my redirect in route 53 than what was assigned and I changed it but that did not fix it. Please help plaportal.org is working but www.plaportal.org is not redirecting to it."], "usr-2": ["You seem to be missing a record for www. That goes into the same hosted zone as your plaportal.org A alias record. That single hosted zone will cover plaportal.org and any subdomains, such as www, that you want to add."]}, "Question": "I transferred my domain name from another company to amazon aws route 53 since I was already using s3 buckets / route 53 for my site. I already set the www to redirect when I originally set it up and everything was working fine until the transfer. I noticed the ns was different for my redirect in route 53 than what was assigned and I changed it but that did not fix it. Please help plaportal.org is working but www.plaportal.org is not redirecting to it.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239619&tstart=50"}{"Answers": {"usr-1": ["I was trying to move a file from a folder in S3 to root location of bucket using CLI. Below is the command I tried from EC2 instance. aws s3 mv s3://my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0 But I am seeing below exception in console. move failed: s3://my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf to s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied \u00a0 Parameter validation failed: Invalid type for parameter UploadId, value: None, type: <type 'NoneType'>, valid types: <type 'basestring'> I am logged in as an IAM user, EC2 instance which ran command from has the role assigned and S3 bucket policy defines the necessary role. Also, from command line, I ran dzdo su before running AWS CLI. These files I try to move from a folder to root location, are imported using Snowball import and are SSE enabled. Should I add any other parameters or I miss something? Any inputs are highly appreciated."], "usr-2": ["I had to add --sse in my move command, as I had enabled Server Side encryption in my bucket. Then, it worked fine. aws s3 mv s3://my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf --sse"]}, "Question": "I was trying to move a file from a folder in S3 to root location of bucket using CLI. Below is the command I tried from EC2 instance. aws s3 mv s3: //my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf \u00a0 But I am seeing below exception in console. move failed: s3: //my-bucket-name/batch_group_25/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf to s3://my-bucket-name/a3e6da44-2060-4dce-805c-ac635aa9cb2e.pdf An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied \u00a0 Parameter validation failed: Invalid type for parameter UploadId, value: None, type: <type 'NoneType' >, valid types: <type 'basestring' > I am logged in as an IAM user, EC2 instance which ran command from has the role assigned and S3 bucket policy defines the necessary role. Also, from command line, I ran dzdo su before running AWS CLI. These files I try to move from a folder to root location, are imported using Snowball import and are SSE enabled. Should I add any other parameters or I miss something? Any inputs are highly appreciated.", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=240674&tstart=50"}{"Answers": {"usr-1": ["Hello, I'm using several instances of EB and what is troubling me is that I have some S3 usage (when I checked my free tier usage ). The Elastic Beanstalk instances are part of EC2, and becuase EC2 uses EBS (as seen here https://eu-central-1.console.aws.amazon.com/ec2/v2/home?region=eu-central-1#Volumes:sort=desc:createTime), its (free tier) storage is advertised as: \u2022 30 GB of Amazon Elastic Block Storage in any combination of General Purpose (SSD) or Magnetic, plus 2 million I/Os (with EBS Magnetic) and 1 GB of snapshot storage So, if EB (as part of EC2) uses EBS, why was S3 added/used (especially without prior consent to service)? Why would I need S3 when I already have 30 GB on EBS (free)? This discrepancy (i.e. S3 not being required for EC2 instances) is also aparent from the SIMPLE MONTHLY CALCULATOR -- the EC2 (with its EBS) is distinct and does not auto-include S3 (which is a separate service and is not added to EC2). Example calculation: http://s3.amazonaws.com/calculator/index.html#key=calc-02ACBD08-3223-4866-8B4E-566FF0EEBA48&r=FRA Looking forward to your explanation"], "usr-2": ["Hi, I'm very sorry to hear that you were charged for your S3 usage when you created Elastic Beanstalk environments. I've gone through the steps to create my own environments and I also wasn't able to see any warnings about free-tier usage or any mentions of S3 buckets until after the environment was created. I've taken it up with the Beanstalk team to let them know they should update the console to reflect the mandatory S3 usage. Best Regards, Omar S."], "usr-3": ["Our Beanstalk team have reached back. We are working on including a warning for s3 usage on our beanstalk console. Thanks for bringing this to our attention and we apologize for any inconvenience caused. One thing to note is that the charge was covered by the s3 free tier [1] assigned to you upon sign up to AWS https://aws.amazon.com/s3/pricing/ [1] Regards, Kay"], "usr-4": ["Thank you Omar and Kay for acknowledging this issue and quickly adding into the pipline to be fixed. @Kay: regarding the usage being covered with S3 free tier, there is another issue I have uncovered. Please see https://forums.aws.amazon.com/thread.jspa?threadID=239823 for details. Sincerely Yours, Robert"]}, "Question": "Hello, I'm using several instances of EB and what is troubling me is that I have some S3 usage (when I checked my free tier usage ). The Elastic Beanstalk instances are part of EC2, and becuase EC2 uses EBS (as seen here https://eu-central-1.console.aws.amazon.com/ec2/v2/home?region=eu-central-1#Volumes:sort=desc:createTime ), its (free tier) storage is advertised as: - 30 GB of Amazon Elastic Block Storage in any combination of General Purpose (SSD) or Magnetic, plus 2 million I/Os (with EBS Magnetic) and 1 GB of snapshot storage So, if EB (as part of EC2) uses EBS, why was S3 added/used (especially without prior consent to service)? Why would I need S3 when I already have 30 GB on EBS (free)? This discrepancy (i.e. S3 not being required for EC2 instances) is also aparent from the SIMPLE MONTHLY CALCULATOR -- the EC2 (with its EBS) is distinct and does not auto-include S3 (which is a separate service and is not added to EC2). Example calculation: http://s3.amazonaws.com/calculator/index.html#key=calc-02ACBD08-3223-4866-8B4E-566FF0EEBA48andr=FRA Looking forward to your explanation", "Title": "", "awsTag": "EC2", "crawled": true, "dateScraped": "2017-04-05-17-42", "sourceUri": "https://forums.aws.amazon.com/thread.jspa?threadID=239773&tstart=50"}