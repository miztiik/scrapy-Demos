{
    "Answers": {
        "usr-1": [
            "Welcome to the forums, Wayne! :)", 
            "It's not entirely clear to me what you're doing...", 
            "Are you writing each 300-byte line to its own record in DynamoDB, for subsequent processing and annotation (\"enrichment\")?  Do you require single-line granularity, here, or could you batch some lines up?  What would happen if everything flowed through your system in groups of 10 lines, for example?  Could it work to leave the source data in S3 and distribute work as pointers into the S3 file--like \"process lines 101-150 in file XYZ\"?  (I think whether this makes any sense at all will depend on some of the questions below.)", 
            "How does the \"enrichment\" processing get triggered--DynamoDB streams, SQS, or something else?  How many and what kind of workers do you have doing this enrichment--EC2's, Lambdas, or something else?  Is there one worker that handles each line, or do several workers enrich different aspects of the same line in parallel?  Is DynamoDB the \"journal\" for all this processing, as it happens?  What happens when these workers fail/die?  What would happen if you re-\"enriched\" some of the lines, either in whole or in part--would there be any data loss or corruption because of those REST / DB actions?  What responsiveness do you require for this processing?", 
            "Where does the data go when you've processed it--does it get exported somewhere or queried from DynamoDB?", 
            "Well, these are some of the things I've thought about, anyway. :)", 
            "Thanks for answering my questions, Wayne.  I'm trying to work out a mental model of your data and processing flow, and I think it seems something like this--so please correct me if I'm wrong:", 
            "Data comes in in 10,000-line files uploaded to S3.", 
            "An SQS message tracks the work to be done to process each new file.", 
            "A worker EC2 (or eventually maybe a Lambda) grabs the SQS message, downloads the file, and starts processing the whole 10,000-line file at once.", 
            "As the worker processes the data, it writes each of the 10,000 annotated lines to DynamoDB as a separate item.", 
            "Presumably, if a worker dies, the SQS message will time out and another worker will pick that message up and carry on where it left off (because the partial work was already saved to DynamoDB).", 
            "Once the worker has processed and written to DynamoDB all 10,000 lines, it maybe (?) posts to an SNS topic which fans out to SQS queues for moving the data to EMR and ElasticSearch.", 
            "Once the processing for the whole file is complete, and the baton passed on, the SQS message to \"enrich\" the lines in the file is deleted.", 
            "To move the data to ES and EMR, each line is presumably (?) read from DynamoDB.", 
            "Once the data has been passed along, DynamoDB is out of the picture and it no longer needs to store the data.  Presumably, you then delete the data from DynamoDB in some way.", 
            "If my above assessment is accurate, then I can think of a couple of things you could consider (each with tradeoffs, of course!) to reduce the burden on DynamoDB.  However, I have a feeling that my model of what you're doing is still not quite right, so I'd like you to confirm or correct, please.  Thanks! :)"
        ], 
        "usr-2": [
            "Mattias,", 
            "Thank you for replying and thank you for raising that I missed one or two critical points. So hope the below answers your questions", 
            "Q) It's not entirely clear to me what you're doing...", 
            "A) We have network equipment capturing usage records think of this as your internet bandwidth traffic", 
            "Q) Are you writing each 300-byte line to its own record in DynamoDB, for subsequent processing and annotation (\"enrichment\")?", 
            "A) Yes. The records need to be enriched with information specific to the customer e.g. username, mac address, location etc which is coming from different sources", 
            "Q) Do you require single-line granularity, here, or could you batch some lines up? What would happen if everything flowed through your system in groups of 10 lines, for example? Could it work to leave the source data in S3 and distribute work as pointers into the S3 file--like \"process lines 101-150 in file XYZ\"? (I think whether this makes any sense at all will depend on some of the questions below.)", 
            "A) This would be ok as long as further downstream processes could reconstruct the records", 
            "Q) How does the \"enrichment\" processing get triggered--DynamoDB streams, SQS, or something else? How many and what kind of workers do you have doing this enrichment--EC2's, Lambdas, or something else? Is there one worker that handles each line, or do several workers enrich different aspects of the same line in parallel? Is DynamoDB the \"journal\" for all this processing, as it happens? What happens when these workers fail/die? What would happen if you re-\"enriched\" some of the lines, either in whole or in part--would there be any data loss or corruption because of those REST / DB actions? What responsiveness do you require for this processing?", 
            "A) Currently, we have files being pushed to an S3 bucket. Events are enabled on this to trigger SQS message currently for a PUT event. SQS guarantees us at least delivery once and we have EC2 workers listening to that queue. We have the timeout set to 5 minutes allowing the workers to pick up the event from SQS, download the file from S3, parse the file and format it and then load into dynamo. Because of the use of SQS and the acknowledgement if a server dies then the message is automatically added back to the queue and will be processed. We have a unique key that should prevent the duplication of events.", 
            "We are looking to moving this piece into a lambda based architecture and I recently went through Ryan's Lambda Architecture Course and this will be one of our enhancements. ", 
            "Q) Where does the data go when you've processed it--does it get exported somewhere or queried from DynamoDB?", 
            "A) Once data is enriched then its going to be pumped into Elasticsearch (for Kibana Dashboards etc) and then EMR for Spark Jobs etc for monthly billing activities etc.", 
            "Thanks for answering and I hope this gives context.", 
            "Wayne"
        ]
    }, 
    "Question": [
        "Dear Cloud Gurus,", 
        "I am looking for some tips on saving costs for DynamoDB.", 
        "Our use case is that we've got 250MM+ records being received in 10k line files. Currently, our design is put to S3, SQS message on put event and then have listeners that process the file (EC2) and dump into DynamoDB. Some may say what, why are you not using EMR etc.?", 
        "Our response, is we need to do further enrichments of this data using other files (much smaller volume), RESTful API's, DB sources etc. Big data solutions are not ideal handling this. ", 
        "Our cost on DynamoDB is currently around 5k by using 6k read & 6k write setup. As part of cost savings we're going to implement some changes to handle dynamic scaling using the CLI when we detect our load is slow by integration with cloud watch data. That way we're not constantly consuming 6k (which is our peak TPS).", 
        "On top of that, we're looking into Reserved DynamoDB capacity which will help lower.", 
        "Additionally, we're going to make changes to ensure our reads and writes are properly distributed across the shards to ensure we're getting the expected ratio of reads & writes and not being cut back on data being weighed to one shard for example.", 
        "Our message size is very small, approx 200-300 bytes per line.", 
        "Additionally to add context in case if someone asks why we're not use Kinesis to DynamoDB is because the files are coming from an appliance and vendor doesn't support third party agents.", 
        "I look forward to responses."
    ], 
    "Tags": [
        "best-practice"
    ], 
    "Title": "cost-savings-on-dynamodb", 
    "awsTag": "sa-pro-best-practice", 
    "crawled": "True", 
    "dateScraped": "2016-11-04-16-24-17", 
    "pageLoadWaitTime": "15", 
    "pgCrawled": "1", 
    "sourceUrl": "https://acloud.guru/course/aws-certified-solutions-architect-professional/discuss/-KMGSp0wDLgKTKCDCsWz/cost-savings-on-dynamodb"
}